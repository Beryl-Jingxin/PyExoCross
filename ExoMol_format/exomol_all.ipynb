{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_def():\n",
    "    list_files = []\n",
    "\n",
    "    urlpage_mol = 'https://www.exomol.com/data/molecules/'\n",
    "    r_mol = requests.get(urlpage_mol)\n",
    "    if r_mol.status_code==200:\n",
    "        page_mol = urllib.request.urlopen(urlpage_mol)\n",
    "    soup_mol = BeautifulSoup(page_mol, 'html.parser')\n",
    "    list_group_mol = soup_mol.find_all('div', attrs={'class': 'list-group'})\n",
    "    list_molecule = []\n",
    "    for i_mol in range(len(list(list_group_mol))):\n",
    "        n_mol = len(list(list(list_group_mol)[i_mol]))\n",
    "        list_group_item_mol = list(list(list_group_mol)[i_mol])[1:n_mol:2]\n",
    "        str_group_mol = str(list_group_item_mol).split('\"')\n",
    "        list_molecule += str_group_mol[3:len(str_group_mol):4]\n",
    "\n",
    "    for index_mol in tqdm(range(len(list_molecule))):    \n",
    "        urlpage_iso = urlpage_mol + list_molecule[index_mol]\n",
    "        print(list_molecule[index_mol])\n",
    "        r_iso = requests.get(urlpage_iso)\n",
    "        if r_iso.status_code==200:\n",
    "            page_iso = urllib.request.urlopen(urlpage_iso)\n",
    "        soup_iso = BeautifulSoup(page_iso, 'html.parser')\n",
    "        list_group_iso = soup_iso.find_all('div', attrs={'class': 'list-group'})\n",
    "        list_iso = []\n",
    "        for i_iso in range(len(list(list_group_iso))):\n",
    "            n_iso = len(list(list(list_group_iso)[i_iso]))\n",
    "            list_group_item_iso = list(list(list_group_iso)[i_iso])[1:n_iso:2]\n",
    "            str_group_iso = str(list_group_item_iso).split('\"')\n",
    "            list_iso += str_group_iso[3:len(str_group_iso):4]\n",
    "\n",
    "        if (len(list_iso) != 0):\n",
    "            for index_iso in range(len(list_iso)):\n",
    "                urlpage_dataset = urlpage_iso + '/' + list_iso[index_iso]\n",
    "                r_dataset = requests.get(urlpage_dataset)\n",
    "                if r_dataset.status_code==200:\n",
    "                    page_dataset = urllib.request.urlopen(urlpage_dataset)\n",
    "                soup_dataset = BeautifulSoup(page_dataset, 'html.parser')\n",
    "                list_group_dataset = soup_dataset.find_all('div', attrs={'class': 'list-group'})\n",
    "                list_datasets = []\n",
    "                for i_dataset in range(len(list(list_group_dataset))):\n",
    "                    n_dataset = len(list(list(list_group_dataset)[i_dataset]))\n",
    "                    list_group_item_dataset = list(list(list_group_dataset)[i_dataset])[1:n_dataset:2]\n",
    "                    str_group_dataset = str(list_group_item_dataset).split('\"')\n",
    "                    list_datasets += str_group_dataset[3:len(str_group_dataset):4]\n",
    "                list_dataset = list_datasets\n",
    "                a = 'list-group-item link-list-group-item recommended'\n",
    "                b = 'list-group-item link-list-group-item'\n",
    "                c = 'pull-right'\n",
    "                if a in list_dataset:\n",
    "                    list_dataset.remove('list-group-item link-list-group-item recommended')\n",
    "                if b in list_dataset:\n",
    "                    list_dataset.remove('list-group-item link-list-group-item')\n",
    "                if c in list_dataset:\n",
    "                    list_dataset.remove('pull-right')\n",
    "\n",
    "                if (len(list_dataset) != 0):\n",
    "                    for index_dataset in range(len(list_dataset)):\n",
    "                        urlpage_file = urlpage_dataset + '/' + list_dataset[index_dataset]\n",
    "                        r_file = requests.get(urlpage_file)\n",
    "                        if r_file.status_code==200:\n",
    "                            page_file = urllib.request.urlopen(urlpage_file)\n",
    "                        soup_file = BeautifulSoup(page_file, 'html.parser')\n",
    "                        list_group_file = soup_file.find_all('div', attrs={'class': 'list-group'})\n",
    "\n",
    "                        if (len(list_group_file) != 0):\n",
    "                            for i_file in range(len(list(list_group_file))):\n",
    "                                n_file = len(list(list(list_group_file)[i_file]))\n",
    "                                list_group_item_file = list(list(list_group_file)[i_file])[1:n_file:2]\n",
    "                                str_group_file = str(list_group_item_file).replace('/db','https://www.exomol.com/db').split('\"')\n",
    "                                list_files += str_group_file[3:len(str_group_file):4]\n",
    "\n",
    "    list_file = list_files[0:len(list_files):2]\n",
    "    new_def_url = pd.DataFrame()\n",
    "    new_def_url['new'] = list_file\n",
    "    return new_def_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_old_def():\n",
    "    exomol_all_url = 'http://www.exomol.com/db/exomol.all'\n",
    "    content = requests.get(exomol_all_url).text.replace('#','')\n",
    "    exomol_col_name = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6']\n",
    "    exomol_all = pd.read_csv(StringIO(content), sep='\\\\s+', names=exomol_col_name, header=None)\n",
    "    \n",
    "    first = exomol_all['c1']\n",
    "    third = exomol_all['c3']\n",
    "    row = len(first)\n",
    "    iso_slug = pd.DataFrame()\n",
    "    iso_formula = pd.DataFrame()\n",
    "    isotopologue = pd.DataFrame()\n",
    "    molecule_single = pd.DataFrame()\n",
    "    num_isotopologues = pd.DataFrame()\n",
    "\n",
    "    for i in range(row):\n",
    "\n",
    "        _iso_slug = exomol_all[first.isin(['Iso-slug'])]['c0'].values\n",
    "        _iso_formula = exomol_all[first.isin(['IsoFormula'])]['c0'].values\n",
    "        _isotopologue = exomol_all[first.isin(['Isotopologue'])]['c0'].values\n",
    "        _molecule_single = exomol_all[first.isin(['Molecule'])]['c0'].values\n",
    "        _num_isotopologues = exomol_all[third.isin(['isotopologues'])]['c0'].values\n",
    "\n",
    "    iso_slug = iso_slug.append(pd.DataFrame(_iso_slug))\n",
    "    iso_formula = iso_formula.append(pd.DataFrame(_iso_formula))\n",
    "    isotopologue = isotopologue.append(pd.DataFrame(_isotopologue))\n",
    "    molecule_single = molecule_single.append(pd.DataFrame(_molecule_single))\n",
    "    num_isotopologues = num_isotopologues.append(pd.DataFrame(_num_isotopologues))\n",
    "    \n",
    "    molecule_repeated = pd.DataFrame()\n",
    "    molecule_num = len(molecule_single)\n",
    "\n",
    "    for j in range(molecule_num):    \n",
    "        molecule_repeated = molecule_repeated.append(pd.DataFrame((molecule_single.values[j] + ' ')\n",
    "                                                                  * int(num_isotopologues.values[j])))\n",
    "\n",
    "    molecule_str = (str(molecule_repeated.values).replace(\"[['\",\" \")\n",
    "                    .replace(\"']\\n ['\",\" \").replace(\"']]\",\" \").replace(\"+\",\"_p\"))\n",
    "    molecule = pd.read_csv(StringIO(molecule_str), sep='\\s+', header=None)\n",
    "    \n",
    "\n",
    "    old_def_url = pd.DataFrame()\n",
    "    old_def_num = len(iso_slug)\n",
    "    for i in range(old_def_num):\n",
    "        old_def_url = old_def_url.append('https://www.exomol.com/db/' + molecule[i] + '/'\n",
    "                                         + iso_slug.values[i] + '/'+ isotopologue.values[i] + '/'\n",
    "                                         + iso_slug.values[i] + '__' + isotopologue.values[i] + '.def')\n",
    "    old_def_url.columns = ['old']\n",
    "    return old_def_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_add_def(new_def_url, old_def_url):\n",
    "    add_def_url = new_def_url[~(new_def_url['new'].isin(old_def_url['old']))].reset_index(drop=True)\n",
    "    add_def_url.columns = ['add']\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.max_columns', 1000)\n",
    "    pd.set_option('display.max_colwidth', 1000)\n",
    "    return add_def_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_master_file():\n",
    "    folder_name = '../data/'\n",
    "    master_filename = 'exomol.all'\n",
    "    if os.path.exists(folder_name):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "    filename = os.path.join(folder_name, master_filename)\n",
    "\n",
    "    exomol_all_url = 'http://www.exomol.com/db/exomol.all'\n",
    "    r = requests.get(exomol_all_url) \n",
    "    with open(filename, 'wb') as code:\n",
    "        code.write(r.content)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_invalid_urlpage(add_def_url):\n",
    "    # Invalid URL page\n",
    "    urlpages_list = []\n",
    "    for i in tqdm(range(len(list(add_def_url['add'])))):\n",
    "        urlpage = add_def_url['add'][i]\n",
    "        r_add = requests.get(urlpage)\n",
    "        if r_add.status_code!=200:\n",
    "            urlpages_list.append(urlpage)\n",
    "    urlpage = pd.DataFrame(urlpages_list)\n",
    "    print('There are ', len(urlpage),' invalid url pages of def files.')\n",
    "    return urlpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_def_url = get_new_def()\n",
    "old_def_url = get_old_def()\n",
    "add_def_url = get_add_def(new_def_url, old_def_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('New version of master file ExoMol.all has ', int(len(new_def_url)), ' def files.')\n",
    "new_def_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Old version of master file ExoMol.all has ', len(old_def_url), ' def files.')\n",
    "old_def_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Master file ExoMol.all has ', len(add_def_url), ' new def files.')\n",
    "add_def_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_master_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlpage = get_invalid_urlpage(add_def_url)\n",
    "urlpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exomol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "722cf535e3d158a42a418157a233af576121476252bfbc7c5af47c8831a1fc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
