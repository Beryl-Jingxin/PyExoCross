{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import urllib3\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from tqdm import notebook\n",
    "urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths\n",
    "\n",
    "\n",
    "<table><tr><td bgcolor=skyblue><font size=24> Could be changed ! </font></td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "def_path = '/home/jingxin/data/test/def'\n",
    "url_path = '/home/jingxin/data/pyexocross/url'\n",
    "molecule = ['MgH','NO']\n",
    "iso_slug = ['24Mg-1H','14N-16O']\n",
    "iso_formula = ['(24Mg)(1H)','(14N)(16O)']\n",
    "dataset = ['XAB','XABC']\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Def File\n",
    "\n",
    "Get all URLs of def files. The number of def files should be the same as the number of isotopologue datasets. The URLs contains the names of molecules, iso-slugs and isotopologue datasets. We save their corresponding isoFormula names as another column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_deffile(molecule, iso_slug, dataset):\n",
    "    def_url = []\n",
    "    def_num = len(iso_slug)\n",
    "    for i in notebook.tqdm(range(def_num)):\n",
    "        url = ('http://www.exomol.com/db/' + molecule[i] + '/'\n",
    "               + iso_slug[i] + '/'+ dataset[i] + '/'\n",
    "               + iso_slug[i] + '__' + dataset[i] + '.def')\n",
    "        def_url.append(url)\n",
    "    return(def_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download def files and save them into ./data/def/ folder. Save the names of these def files with all information we got before, that is to say, save as 'molecule_isoFormula_iso-slug_isotopologue.def'. It will be more convenient for processing data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_deffile(def_path):\n",
    "    failed_list = [] \n",
    "    def_url = read_deffile(molecule, iso_slug, dataset)\n",
    "    for link in notebook.tqdm(def_url):\n",
    "        def_filename = link.split('/')[-1]\n",
    "        print(\"Downloading file: %s\" % def_filename)\n",
    "        print(link)\n",
    " \n",
    "        # Make folders for save doanloaded files.\n",
    "        if os.path.exists(def_path):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(def_path, exist_ok=True)\n",
    "        filename = os.path.join(def_path, def_filename)\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(link, stream=True, verify=False)\n",
    "        except Exception:\n",
    "            failed_list.append(def_filename)\n",
    "            print(' download failed. Go to download next one\\n')\n",
    "              \n",
    "        # For compute the progess.\n",
    "        total_size = int(r.headers['Content-Length'])\n",
    "        temp_size = 0    \n",
    "   \n",
    "        # Download started.\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                if chunk:\n",
    "                    temp_size += len(chunk)\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "                    done = int(50 * temp_size / total_size)\n",
    "                    sys.stdout.write(\"\\r[%s%s] %d%%\" % ('â–ˆ' * done, ' ' * (50 - done),\n",
    "                                                        100 * temp_size / total_size))\n",
    "                    sys.stdout.flush()\n",
    "        print(\" Downloaded!\\n\")\n",
    "    print(\"All def files downloaded!\\n\")    \n",
    "    print(\"The files which are failed to download: \\n\")\n",
    "    print(failed_list) # Record which file is failed to download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Download Links with API\n",
    "\n",
    "Get the API URLs of those uncertainty molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api(def_path):\n",
    "    molecule_str = []\n",
    "    iso_str = []\n",
    "    api_url = []\n",
    "    for i in range(len(molecule)):\n",
    "        molecule_str.append(molecule[i].replace('_p','+').split('__')[0].replace('+','_p'))\n",
    "        iso_str.append(iso_slug[i].replace('_p','+'))\n",
    "\n",
    "        api_url.append('http://exomol.com/api/?molecule=*&datatype=linelist'.replace('*',molecule_str[i]))\n",
    "    return(api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = []\n",
    "api_url = get_api(def_path)\n",
    "for i in range(len(molecule)):\n",
    "    response = requests.get(api_url[i])\n",
    "    if(response.status_code != 200):\n",
    "        print('ExoMol API Error' + str(response.status_code))\n",
    "\n",
    "    # If the obtained status code is 200, it is correct.\n",
    "    else:\n",
    "        content = response.text            # Get the relevant content.\n",
    "        json_dict = json.loads(content)    # Convert json into dictionary.\n",
    "\n",
    "        # Extract files information from dictionary and convert them into list\n",
    "        _iso_formula = iso_formula[i]\n",
    "        _dataset = dataset[i]\n",
    "        json_list = json_dict[_iso_formula]['linelist'][_dataset]['files']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the download links of states.bz2 files and trans.bz2 files from API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_url(def_path):\n",
    "    \"\"\"Get the download url from API.\"\"\"\n",
    "    file_url = []\n",
    "    api_url = get_api(def_path)\n",
    "    for i in range(len(molecule)):\n",
    "        response = requests.get(api_url[i])\n",
    "        if(response.status_code != 200):\n",
    "            print('ExoMol API Error' + str(response.status_code))\n",
    "\n",
    "        # If the obtained status code is 200, it is correct.\n",
    "        else:\n",
    "            content = response.text            # Get the relevant content.\n",
    "            json_dict = json.loads(content)    # Convert json into dictionary.\n",
    "\n",
    "            # Extract files information from dictionary and convert them into list\n",
    "            _iso_formula = iso_formula[i]\n",
    "            _dataset = dataset[i]\n",
    "            json_list = json_dict[_iso_formula]['linelist'][_dataset]['files']\n",
    "            url_show = []\n",
    "            for j in range(len(json_list)):\n",
    "                link = json_list[j].get('url')\n",
    "                try:\n",
    "                    if((link.endswith('states.bz2') or link.endswith('trans.bz2'))):\n",
    "\n",
    "                        file_url.append(\"http://www.\" + link)\n",
    "                        url_show.append(\"http://www.\" + link)\n",
    "                except KeyError:\n",
    "                    print('Keyerror, keep going!')\n",
    "        print('\\nThe number of downloading files for', _iso_formula, _dataset, ': ', len(url_show))\n",
    "        print(\"Download links:\")                    \n",
    "        for k in url_show:\n",
    "            print(k)\n",
    "    return (file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download States and Trans Files\n",
    "\n",
    "We write all the download URLs into a text file, name it as api__urls.txt. \n",
    "\n",
    "In Linux, we use command \n",
    "\n",
    "```\n",
    "wget -d -r -i /.../save_path/.../api__urls.txt\n",
    "```\n",
    "\n",
    "Download states.bz2 files and trans.bz2 files with download links. Save these files into correspoding folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(url_path):\n",
    "    url_filename = url_path + '/api__urls.txt'\n",
    "\n",
    "    if os.path.exists(url_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(url_path, exist_ok=True)\n",
    "\n",
    "    target_link = get_target_url(def_path)\n",
    "    with open(url_filename, 'w') as file:\n",
    "        file.write('\\n'.join(target_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a94f25381c94d4eb633093cd3c3d20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca96615e48fa41aea453449d551ceca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: 24Mg-1H__XAB.def\n",
      "http://www.exomol.com/db/MgH/24Mg-1H/XAB/24Mg-1H__XAB.def\n",
      "[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% Downloaded!\n",
      "\n",
      "Downloading file: 14N-16O__XABC.def\n",
      "http://www.exomol.com/db/NO/14N-16O/XABC/14N-16O__XABC.def\n",
      "[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% Downloaded!\n",
      "\n",
      "All def files downloaded!\n",
      "\n",
      "The files which are failed to download: \n",
      "\n",
      "[]\n",
      "\n",
      "The number of downloading files for (24Mg)(1H) XAB :  2\n",
      "Download links:\n",
      "http://www.exomol.com/db/MgH/24Mg-1H/XAB/24Mg-1H__XAB.states.bz2\n",
      "http://www.exomol.com/db/MgH/24Mg-1H/XAB/24Mg-1H__XAB.trans.bz2\n",
      "\n",
      "The number of downloading files for (14N)(16O) XABC :  2\n",
      "Download links:\n",
      "http://www.exomol.com/db/NO/14N-16O/XABC/14N-16O__XABC.trans.bz2\n",
      "http://www.exomol.com/db/NO/14N-16O/XABC/14N-16O__XABC.states.bz2\n"
     ]
    }
   ],
   "source": [
    "download_deffile(def_path)\n",
    "download_files(url_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jingxin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2654e51ae32fcaf6e4752957f7b65ce9230b83b2685f0437e52701cfc52e3fff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
