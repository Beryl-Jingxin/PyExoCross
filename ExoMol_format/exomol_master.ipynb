{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Master File\n",
    "\n",
    "Read the master file 'ExoMol All'. The URL is http://www.exomol.com/db/exomol.all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exomol_all_url = 'http://www.exomol.com/db/exomol.all'\n",
    "\n",
    "content = requests.get(exomol_all_url).text.replace('#','')\n",
    "exomol_col_name = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6']\n",
    "exomol_all = pd.read_csv(StringIO(content), sep='\\\\s+', names=exomol_col_name, header=None)\n",
    "\n",
    "exomol_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all molecules, their iso-slugs, isoFormula and isotopologue dataset names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = exomol_all['c1']\n",
    "fourth = exomol_all['c4']\n",
    "row = len(first)\n",
    "\n",
    "iso_slug = pd.DataFrame()\n",
    "iso_formula = pd.DataFrame()\n",
    "iso_dataset = pd.DataFrame()\n",
    "molecule_unduplicated = pd.DataFrame()\n",
    "num_iso_in_mol = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(row)):\n",
    "        \n",
    "    _iso_slug = exomol_all[first.isin(['Iso-slug'])]['c0'].values\n",
    "    _iso_formula = exomol_all[first.isin(['IsoFormula'])]['c0'].values\n",
    "    _iso_dataset = exomol_all[first.isin(['Isotopologue'])]['c0'].values\n",
    "    _molecule_unduplicated = exomol_all[first.isin(['Molecule'])]['c0'].values\n",
    "    _num_iso_in_mol = exomol_all[fourth.isin(['considered'])]['c0'].values\n",
    "\n",
    "iso_slug = iso_slug.append(pd.DataFrame(_iso_slug))\n",
    "iso_formula = iso_formula.append(pd.DataFrame(_iso_formula))\n",
    "iso_dataset = iso_dataset.append(pd.DataFrame(_iso_dataset))\n",
    "molecule_unduplicated = molecule_unduplicated.append(pd.DataFrame(_molecule_unduplicated))\n",
    "num_iso_in_mol = num_iso_in_mol.append(pd.DataFrame(_num_iso_in_mol))\n",
    "isotopologue_unduplicated = iso_slug.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "iso_datset_unduplicated = iso_dataset.drop_duplicates(subset=None, keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_slug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All molecule and their corresponding isotopologue dataset numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_unduplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iso_in_mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isotopologue_unduplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_datset_unduplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the length of molecules list to be the same as the length of iso-slugs and isotopologue dataset names for following loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_repeated = pd.DataFrame()\n",
    "molecule_num = len(molecule_unduplicated)\n",
    "\n",
    "for j in range(molecule_num):    \n",
    "    molecule_repeated = molecule_repeated.append(pd.DataFrame((molecule_unduplicated.values[j] + ' ')\n",
    "                                                              * int(num_iso_in_mol.values[j])))\n",
    "\n",
    "molecule_str = (str(molecule_repeated.values).replace(\"[['\",\" \")\n",
    "                .replace(\"']\\n ['\",\" \").replace(\"']]\",\" \").replace(\"+\",\"_p\"))\n",
    "molecule = pd.read_csv(StringIO(molecule_str), sep='\\s+', header=None)\n",
    "molecule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Def File\n",
    "\n",
    "Get all URLs of def files. The number of def files should be the same as the number of isotopologue datasets. The URLs contains the names of molecules, iso-slugs and isotopologue datasets. We save their corresponding isoFormula names as another column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_url = pd.DataFrame()\n",
    "\n",
    "def_num = len(iso_slug)\n",
    "\n",
    "for i in tqdm(range(def_num)):\n",
    "    def_url = def_url.append('http://www.exomol.com/db/' + molecule[i] + '/'\n",
    "                             + iso_slug.values[i] + '/'+ iso_dataset.values[i] + '/'\n",
    "                             + iso_slug.values[i] + '__' + iso_dataset.values[i] + '.def')\n",
    "\n",
    "    \n",
    "def_url['IsoFormula'] = iso_formula\n",
    "def_url.columns = ['def url','IsoFormula']\n",
    "\n",
    "def_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def_url.values[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download def files and save them into ./data/def/ folder. Save the names of these def files with all information we got before, that is to say, save as 'molecule_isoFormula_iso-slug_isotopologue.def'. It will be more convenient for processing data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_deffile(file_url):\n",
    "    \n",
    "    failed_list = [] \n",
    "    for _link in tqdm(file_url['def url'].values):\n",
    "        \n",
    "        link = _link\n",
    "        iso_slug = link.split('/')[-4]\n",
    "        iso_formula = (str(file_url[file_url['def url'].isin([link])]['IsoFormula'].values)\n",
    "                       .replace(\"['\",\"\").replace(\"']\",\"\"))\n",
    "        inital_def_name = link.split('/')[-1]\n",
    "        new_def_filename = iso_slug + '_' + iso_formula + '_' + inital_def_name\n",
    "        print(\"Downloading file: %s\" % new_def_filename)\n",
    "        print(link)\n",
    " \n",
    "        # Make folders for save doanloaded files.\n",
    "        folder_name = './data/def/'\n",
    "        if os.path.exists(folder_name):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(folder_name, exist_ok=True)\n",
    "        filename = os.path.join(folder_name, new_def_filename)\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(link, stream=True, verify=False)\n",
    "        except Exception:\n",
    "            failed_list.append(new_def_filename)\n",
    "            print(' download failed. Go to download next one\\n')\n",
    "              \n",
    "        # For compute the progess.\n",
    "        total_size = int(r.headers['Content-Length'])\n",
    "        temp_size = 0    \n",
    "   \n",
    "        # Download started.\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                if chunk:\n",
    "                    temp_size += len(chunk)\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "                    done = int(50 * temp_size / total_size)\n",
    "                    sys.stdout.write(\"\\r[%s%s] %d%%\" % ('â–ˆ' * done, ' ' * (50 - done),\n",
    "                                                        100 * temp_size / total_size))\n",
    "                    sys.stdout.flush()\n",
    "        \n",
    "        print(\" Downloaded!\\n\")\n",
    "\n",
    "    print(\"All def files downloaded!\\n\")    \n",
    "    print(\"The files which are failed to download: \\n\")\n",
    "    print(failed_list) # Record which file is failed to download.\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_deffile(def_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the def files whose the uncertainty availability row shows YES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/def\"\n",
    "\n",
    "def_col_name = ['c0', '#', 'c1', 'c2', 'c3', 'c4', 'c5']\n",
    "\n",
    "for(dirpath,dirnames,files)in os.walk(path):\n",
    "    tot = 0\n",
    "    count = 0\n",
    "    unc_def_filename = []\n",
    "    for filename in files:\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "        tot += 1\n",
    "        def_df =  pd.read_csv(filepath,sep='\\s+', names=def_col_name, header=None)\n",
    "        c1 = def_df['c1']\n",
    "        if def_df[c1.isin(['Uncertainty'])]['c0'].values == '1':\n",
    "            unc_def_filename.append(filename)\n",
    "            count += 1            \n",
    "        else:\n",
    "            no_unc_def_filename = []\n",
    "        \n",
    "    print('There are ', tot, ' def files.\\n')\n",
    "    print('The uncertainty availability shows YES in the following ', count, ' def files:\\n', unc_def_filename)\n",
    "    print('\\nThe uncertainty availability does not exit or shows NO in other ', tot - count, 'def files.')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unc_def_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Download Links with API\n",
    "\n",
    "Get the API URLs of those uncertainty molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_str = []\n",
    "iso_formula_str = []\n",
    "iso_dataset_str = []\n",
    "iso = []\n",
    "api_url = []\n",
    "target_link = []\n",
    "\n",
    "for i in range(len(unc_def_filename)):\n",
    "    molecule_str.append(unc_def_filename[i].replace('_p','+').split('_')[0].replace('+','_p'))\n",
    "    iso_formula_str.append(unc_def_filename[i].replace('_p','+').split('_')[1])\n",
    "    iso_dataset_str.append(unc_def_filename[i].split('_')[-1].split('.')[0])\n",
    "    \n",
    "    _iso = (iso_formula_str[i], iso_dataset_str[i])\n",
    "    iso.append(_iso)\n",
    "    \n",
    "    api_url.append('http://exomol.com/api/?molecule=*&datatype=linelist'.replace('*',molecule_str[i]))\n",
    "    \n",
    "print(iso)\n",
    "api_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the download links of states.bz2 files and trans.bz2 files from API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_url():\n",
    "    \n",
    "    \"\"\"\n",
    "    Get the download url from API.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    file_url = []\n",
    "    for i in range(len(iso)):\n",
    "        \n",
    "        response = requests.get(api_url[i])\n",
    "\n",
    "        if(response.status_code != 200):\n",
    "            print('ExoMol API Error' + str(response.status_code))\n",
    "\n",
    "        # If the obtained status code is 200, it is correct.\n",
    "        else:\n",
    "            content = response.text            # Get the relevant content.\n",
    "            json_dict = json.loads(content)    # Convert json into dictionary.\n",
    "\n",
    "            # Extract files information from dictionary and convert them into list\n",
    "            iso_slug = iso[i][0]\n",
    "            iso_dataset = iso[i][1]\n",
    "            json_list = json_dict[iso_slug]['linelist'][iso_dataset]['files']\n",
    "\n",
    "            print('The number of downloading files for', iso_slug, iso_dataset, ': ', len(json_list))\n",
    "            print(\"Download links:\")\n",
    "            \n",
    "            url_show = []\n",
    "            for j in range(len(json_list)):\n",
    "                link = json_list[j].get('url')\n",
    "                try:\n",
    "                    if((link.endswith('states.bz2') or link.endswith('trans.bz2'))):\n",
    "                        file_url.append(\"http://www.\" + link)\n",
    "                        url_show.append(\"http://www.\" + link)\n",
    "                except KeyError:\n",
    "                    print('Keyerror, keep going!')\n",
    "                    \n",
    "    \n",
    "        for k in url_show:\n",
    "            print(k)\n",
    "\n",
    "    return file_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_link = get_target_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download States and Trans Files\n",
    "\n",
    "We write all the download URLs into a text file, name it as $api__urls.txt$. \n",
    "\n",
    "In Linux, we use command \n",
    "\n",
    "```\n",
    "wget -d -r -i /.../save_path/.../api__urls.txt\n",
    "```\n",
    "\n",
    "Download states.bz2 files and trans.bz2 files with download links. Save these files into correspoding folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_path = './data/url'\n",
    "url_filename = url_path + '/api__urls.txt'\n",
    "\n",
    "if os.path.exists(url_path):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(url_path, exist_ok=True)\n",
    "\n",
    "with open(url_filename, 'w') as file:\n",
    "    file.write('\\n'.join(target_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
