{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU:  256\n"
     ]
    }
   ],
   "source": [
    "# encoding: utf-8\n",
    "import os\n",
    "import bz2\n",
    "import glob\n",
    "import time\n",
    "import requests\n",
    "import argparse\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numexpr as ne\n",
    "import dask.array as da\n",
    "import astropy.units as au\n",
    "import dask.dataframe as dd\n",
    "import astropy.constants as ac\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "from itertools import chain\n",
    "from tqdm.notebook import tqdm\n",
    "from pandarallel import pandarallel\n",
    "from indexed_bzip2 import IndexedBzip2File\n",
    "from matplotlib.collections import LineCollection\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import Process, Pool, freeze_support\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "from scipy.special import voigt_profile, wofz, erf, roots_hermite\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", np.ComplexWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "\n",
    "import multiprocessing as mp\n",
    "freeze_support()\n",
    "num_cpus = mp.cpu_count()\n",
    "print('Number of CPU: ', num_cpus)\n",
    "\n",
    "# import vaex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.9.18 64bit [GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]"
        },
        {
         "module": "IPython",
         "version": "8.12.3"
        },
        {
         "module": "OS",
         "version": "Linux 5.14.0 362.8.1.el9_3.x86_64 x86_64 with glibc2.34"
        },
        {
         "module": "pandas",
         "version": "2.0.3"
        },
        {
         "module": "numpy",
         "version": "1.22.3"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.9.18 64bit [GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]</td></tr><tr><td>IPython</td><td>8.12.3</td></tr><tr><td>OS</td><td>Linux 5.14.0 362.8.1.el9\\_3.x86\\_64 x86\\_64 with glibc2.34</td></tr><tr><td>pandas</td><td>2.0.3</td></tr><tr><td>numpy</td><td>1.22.3</td></tr><tr><td colspan='2'>Wed Apr 24 17:51:27 2024 BST</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.9.18 64bit [GCC 11.4.1 20230605 (Red Hat 11.4.1-2)] \\\\ \\hline\n",
       "IPython & 8.12.3 \\\\ \\hline\n",
       "OS & Linux 5.14.0 362.8.1.el9\\_3.x86\\_64 x86\\_64 with glibc2.34 \\\\ \\hline\n",
       "pandas & 2.0.3 \\\\ \\hline\n",
       "numpy & 1.22.3 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Wed Apr 24 17:51:27 2024 BST} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.9.18 64bit [GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]\n",
       "IPython 8.12.3\n",
       "OS Linux 5.14.0 362.8.1.el9_3.x86_64 x86_64 with glibc2.34\n",
       "pandas 2.0.3\n",
       "numpy 1.22.3\n",
       "Wed Apr 24 17:51:27 2024 BST"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext version_information\n",
    "%version_information pandas, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2024-04-24T17:51:27.188392+01:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.18\n",
      "IPython version      : 8.12.3\n",
      "\n",
      "Compiler    : GCC 11.4.1 20230605 (Red Hat 11.4.1-2)\n",
      "OS          : Linux\n",
      "Release     : 5.14.0-362.8.1.el9_3.x86_64\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 256\n",
      "Architecture: 64bit\n",
      "\n",
      "Watermark: 2.4.3\n",
      "\n",
      "astropy   : 6.0.0\n",
      "numpy     : 1.22.3\n",
      "dask      : 2024.1.0\n",
      "requests  : 2.31.0\n",
      "numexpr   : 2.8.8\n",
      "argparse  : 1.1\n",
      "matplotlib: 3.8.2\n",
      "pandas    : 2.0.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --watermark\n",
    "%watermark --iversions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Input File Path\n",
    "\n",
    "\n",
    "<table><tr><td bgcolor=skyblue><font size=24> Could be changed ! </font></td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "inp_filepath = '/home/jingxin/PyExoCross/input/CH4_exomol.inp'\n",
    "#########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:    \n",
    "    def start(self):\n",
    "        self.start_CPU = time.process_time()\n",
    "        self.start_sys = time.time()\n",
    "        return self\n",
    "\n",
    "    def end(self, *args):\n",
    "        self.end_CPU = time.process_time()\n",
    "        self.end_sys = time.time()\n",
    "        self.interval_CPU = self.end_CPU - self.start_CPU\n",
    "        self.interval_sys = self.end_sys - self.start_sys\n",
    "        print('{:25s} : {}'.format('Running time on CPU', self.interval_CPU), 's')\n",
    "        print('{:25s} : {}'.format('Running time on system', self.interval_sys), 's')\n",
    "        \n",
    "    def cal(self, *args):\n",
    "        self.end_CPU = time.process_time()\n",
    "        self.end_sys = time.time()\n",
    "        self.interval_CPU = self.end_CPU - self.start_CPU\n",
    "        self.interval_sys = self.end_sys - self.start_sys\n",
    "        return(self.interval_CPU, self.interval_sys)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Information from Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputWarning(UserWarning):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inp_para(inp_filepath):\n",
    "    # Find the maximum column for all the rows.\n",
    "    with open(inp_filepath, 'r') as temp_f:\n",
    "        col_count = max([len([x for x in l.split(\" \") if x.strip()]) for l in temp_f.readlines()])\n",
    "    # Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1).\n",
    "    column_names = [i for i in range(col_count)] \n",
    "    inp_df = pd.read_csv(inp_filepath, sep='\\\\s+', header = None, names=column_names, usecols=column_names)\n",
    "    col0 = inp_df[0]\n",
    "    \n",
    "    # Database\n",
    "    database = inp_df[col0.isin(['Database'])][1].values[0].upper().replace('EXOMOL','ExoMol')\n",
    "    \n",
    "    # Basic information\n",
    "    molecule = inp_df[col0.isin(['Molecule'])][1].values[0]\n",
    "    isotopologue = inp_df[col0.isin(['Isotopologue'])][1].values[0]\n",
    "    dataset = inp_df[col0.isin(['Dataset'])][1].values[0]\n",
    "    mol_iso_id = int(inp_df[col0.isin(['MolIsoID'])][1].iloc[0])\n",
    "    \n",
    "    # File path\n",
    "    read_path = inp_df[col0.isin(['ReadPath'])][1].values[0]\n",
    "    save_path = inp_df[col0.isin(['SavePath'])][1].values[0]\n",
    "    if os.path.exists(save_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "    # Functions \n",
    "    Conversion = int(inp_df[col0.isin(['Conversion'])][1].iloc[0])\n",
    "    PartitionFunctions = int(inp_df[col0.isin(['PartitionFunctions'])][1].iloc[0])\n",
    "    CoolingFunctions = int(inp_df[col0.isin(['CoolingFunctions'])][1].iloc[0])\n",
    "    Lifetimes = int(inp_df[col0.isin(['Lifetimes'])][1].iloc[0])\n",
    "    OscillatorStrengths = int(inp_df[col0.isin(['OscillatorStrengths'])][1].iloc[0])\n",
    "    SpecificHeats = int(inp_df[col0.isin(['SpecificHeats'])][1].iloc[0])\n",
    "    StickSpectra = int(inp_df[col0.isin(['StickSpectra'])][1].iloc[0])\n",
    "    CrossSections = int(inp_df[col0.isin(['CrossSections'])][1].iloc[0])\n",
    "    \n",
    "    # Cores and chunks\n",
    "    ncputrans = int(inp_df[col0.isin(['NCPUtrans'])][1].iloc[0])\n",
    "    ncpufiles = int(inp_df[col0.isin(['NCPUfiles'])][1].iloc[0])\n",
    "    chunk_size = int(inp_df[col0.isin(['ChunkSize'])][1].iloc[0])\n",
    "    \n",
    "    # Quantum numbers\n",
    "    NeedQNs = Conversion + StickSpectra + CrossSections\n",
    "    if NeedQNs != 0:\n",
    "        QNslabel_list = list(inp_df[col0.isin(['QNslabel'])].iloc[0])[1:]\n",
    "        QNsformat_list = list(inp_df[col0.isin(['QNsformat'])].iloc[0])[1:]\n",
    "        QNslabel_list = [x for x in QNslabel_list if x == x]\n",
    "        QNsformat_list = [x for x in QNsformat_list if x == x]\n",
    "    else:\n",
    "        QNslabel_list = []\n",
    "        QNsformat_list = []  \n",
    "    \n",
    "    # Convert from one format to another\n",
    "    if Conversion != 0:\n",
    "        ConversionFormat = int(inp_df[col0.isin(['ConversionFormat'])][1].iloc[0])\n",
    "        ConversionMinFreq = float(inp_df[col0.isin(['ConversionFrequncyRange'])][1].iloc[0])\n",
    "        ConversionMaxFreq = float(inp_df[col0.isin(['ConversionFrequncyRange'])][2].iloc[0])\n",
    "        GlobalQNLabel_list = list(inp_df[col0.isin(['GlobalQNLabel'])].iloc[0].dropna())[1:]\n",
    "        GlobalQNFormat_list = list(inp_df[col0.isin(['GlobalQNFormat'])].iloc[0].dropna())[1:]\n",
    "        LocalQNLabel_list = list(inp_df[col0.isin(['LocalQNLabel'])].iloc[0].dropna())[1:]\n",
    "        LocalQNFormat_list = list(inp_df[col0.isin(['LocalQNFormat'])].iloc[0].dropna())[1:]\n",
    "        # Uncertainty filter\n",
    "        ConversionUncYN = inp_df[col0.isin(['ConvUncFilter(Y/N)'])][1].values[0].upper()[0]\n",
    "        if ConversionUncYN == 'Y':\n",
    "            ConversionUnc = float(inp_df[col0.isin(['ConvUncFilter(Y/N)'])][2].iloc[0])\n",
    "        elif ConversionUncYN == 'N':\n",
    "            ConversionUnc = 'None'\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct uncertainty filter choice 'Y' or 'N' into the input file.\")  \n",
    "        # Threshold filter\n",
    "        ConversionThresholdYN = inp_df[col0.isin(['ConvThreshold(Y/N)'])][1].values[0].upper()[0]\n",
    "        if ConversionThresholdYN == 'Y':\n",
    "            ConversionThreshold = float(inp_df[col0.isin(['ConvThreshold(Y/N)'])][2].iloc[0])\n",
    "        elif ConversionThresholdYN == 'N':\n",
    "            ConversionThreshold = 'None'\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct threshold choice 'Y' or 'N' into the input file.\")       \n",
    "    else:\n",
    "        ConversionFormat = 0\n",
    "        ConversionMinFreq = 0\n",
    "        ConversionMaxFreq = 1e20\n",
    "        GlobalQNLabel_list = []\n",
    "        GlobalQNFormat_list = []\n",
    "        LocalQNLabel_list = []\n",
    "        LocalQNFormat_list = []\n",
    "        ConversionUnc = 'None'\n",
    "        ConversionThreshold = 'None'\n",
    "        \n",
    "    # Calculate partition, cooling functions or specific heats \n",
    "    if PartitionFunctions + CoolingFunctions + SpecificHeats != 0:\n",
    "        Ntemp = int(inp_df[col0.isin(['Ntemp'])][1].iloc[0])    # The number of temperature steps\n",
    "        Tmax = int(inp_df[col0.isin(['Tmax'])][1].iloc[0])      # Maximal temperature in K (minimal T = 1 K )\n",
    "    else:\n",
    "        Ntemp = 0\n",
    "        Tmax = 0  \n",
    "     \n",
    "    # Calculate lifetimes \n",
    "    if Lifetimes != 0:\n",
    "        CompressYN = inp_df[col0.isin(['Compress(Y/N)'])][1].values[0].upper()[0]\n",
    "    else:\n",
    "        CompressYN = 'N'\n",
    "        \n",
    "    # Calculate oscillator strengths\n",
    "    if OscillatorStrengths != 0:\n",
    "        gfORf = inp_df[col0.isin(['gf/f'])][1].values[0].upper()\n",
    "        PlotOscillatorStrengthYN = inp_df[col0.isin(['PlotOscillatorStrength(Y/N)'])][1].values[0].upper() \n",
    "        if PlotOscillatorStrengthYN == 'Y':\n",
    "            _limitYaxisOS = inp_df[col0.isin(['Y-axisLimitOscillatorStrength'])][1].values[0]\n",
    "            if _limitYaxisOS == '#':\n",
    "                limitYaxisOS = 1e-30\n",
    "            elif pd.isnull(_limitYaxisOS) == True:\n",
    "                limitYaxisOS = 1e-30\n",
    "            else:\n",
    "                limitYaxisOS = float(_limitYaxisOS)\n",
    "        else:\n",
    "            limitYaxisOS = 0\n",
    "    else:\n",
    "        gfORf = 'GF'\n",
    "        PlotOscillatorStrengthYN = 'None'\n",
    "        limitYaxisOS = 0\n",
    "    \n",
    "    # Calculate stick spectra or cross sections \n",
    "    if StickSpectra + CrossSections != 0:\n",
    "        T = int(inp_df[col0.isin(['Temperature'])][1].iloc[0])\n",
    "        min_wn = float(inp_df[col0.isin(['Range'])][1].iloc[0])\n",
    "        max_wn = float(inp_df[col0.isin(['Range'])][2].iloc[0])\n",
    "        abs_emi = inp_df[col0.isin(['Absorption/Emission'])][1].values[0].upper()[0].replace('A','Ab').replace('E','Em')\n",
    "        # Uncertainty filter\n",
    "        UncFilterYN = inp_df[col0.isin(['UncFilter(Y/N)'])][1].values[0].upper()[0]\n",
    "        if UncFilterYN == 'Y':\n",
    "            UncFilter = float(inp_df[col0.isin(['UncFilter(Y/N)'])][2].iloc[0])\n",
    "        elif UncFilterYN == 'N':\n",
    "            UncFilter = 'None'\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct uncertainty filter choice 'Y' or 'N' into the input file.\")  \n",
    "        # Threshold filter\n",
    "        thresholdYN = inp_df[col0.isin(['Threshold(Y/N)'])][1].values[0].upper()[0]\n",
    "        if thresholdYN == 'Y':\n",
    "            threshold = float(inp_df[col0.isin(['Threshold(Y/N)'])][2].iloc[0])\n",
    "        elif thresholdYN == 'N':\n",
    "            threshold = 'None'\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct threshold choice 'Y' or 'N' into the input file.\") \n",
    "        # Quantum number filter\n",
    "        QNsFilterYN = inp_df[col0.isin(['QNsFilter(Y/N)'])][1].values[0].upper()[0]\n",
    "        if QNsFilterYN == 'Y':\n",
    "            QNsFilter = list(inp_df[col0.isin(['QNsFilter(Y/N)'])].iloc[0].dropna())[2:]\n",
    "            QNs_label = []\n",
    "            QNs_value = []\n",
    "            for i in range(len(QNsFilter)):\n",
    "                QNs_label.append(QNsFilter[i].split('[')[0])\n",
    "                QNs_value.append(QNsFilter[i].split('[')[1].split(']')[0].split(';'))\n",
    "            QNs_format = [QNsformat_list[j] for j in [QNslabel_list.index(i) for i in QNs_label]]\n",
    "        elif QNsFilterYN == 'N':\n",
    "            QNsFilter = []\n",
    "            QNs_label = []\n",
    "            QNs_value = []\n",
    "            QNs_format = []\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct quantum number filter choice 'Y' or 'N' into the input file.\")\n",
    "    else:\n",
    "        T = 0\n",
    "        min_wn = 0\n",
    "        max_wn = 1e20\n",
    "        abs_emi = 'None'\n",
    "        UncFilter = 'None'\n",
    "        threshold = 'None'\n",
    "        QNsFilter = []\n",
    "        QNs_label = []\n",
    "        QNs_value = []  \n",
    "        QNs_format = []\n",
    "        \n",
    "    # Stick spectra\n",
    "    if StickSpectra != 0:\n",
    "        PlotStickSpectraYN = inp_df[col0.isin(['PlotStickSpectra(Y/N)'])][1].values[0].upper()[0]\n",
    "        if PlotStickSpectraYN == 'Y':\n",
    "            _limitYaxisStickSpectra = inp_df[col0.isin(['Y-axisLimitStickSpectra'])][1].values[0]\n",
    "            if _limitYaxisStickSpectra == '#':\n",
    "                limitYaxisStickSpectra = 1e-30\n",
    "            elif pd.isnull(_limitYaxisStickSpectra) == True:\n",
    "                limitYaxisStickSpectra = 1e-30\n",
    "            else:\n",
    "                limitYaxisStickSpectra = float(_limitYaxisStickSpectra)\n",
    "        else:\n",
    "            limitYaxisStickSpectra = 0\n",
    "    else:\n",
    "        PlotStickSpectraYN = 'None'\n",
    "        limitYaxisStickSpectra = 0\n",
    "\n",
    "    # Cross sections\n",
    "    if CrossSections != 0:\n",
    "        NpointsORBinSize = inp_df[col0.isin(['Npoints/BinSize'])][1].values[0].upper()\n",
    "        if 'POI' in NpointsORBinSize:\n",
    "            N_point = int(inp_df[col0.isin(['Npoints/BinSize'])][2].iloc[0])\n",
    "            bin_size = float((max_wn - min_wn)/(N_point-1))\n",
    "        elif 'BIN' in NpointsORBinSize or 'SIZ' in NpointsORBinSize:\n",
    "            bin_size = float(inp_df[col0.isin(['Npoints/BinSize'])][2].iloc[0])\n",
    "            N_point = int((max_wn - min_wn)/bin_size+1)\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct grid choice 'Npoints' or 'BinSize' into the input file.\")\n",
    "        # Predissociative cross sections\n",
    "        predissocYN = inp_df[col0.isin(['PredissocXsec(Y/N)'])][1].values[0].upper()[0]\n",
    "        if predissocYN != 'Y' and predissocYN != 'N':\n",
    "            raise ImportError(\"Please type the correct predissociative choice 'Y' or 'N' into the input file.\")        \n",
    "        # Cutoff\n",
    "        cutoffYN = inp_df[col0.isin(['Cutoff(Y/N)'])][1].values[0].upper()[0]\n",
    "        if cutoffYN == 'Y':\n",
    "            cutoff = float(inp_df[col0.isin(['Cutoff(Y/N)'])][2].iloc[0])\n",
    "        elif cutoffYN == 'N':\n",
    "            cutoff = 'None'\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct cutoff choice 'Y' or 'N' into the input file.\")\n",
    "        # Other parameters\n",
    "        P = float(inp_df[col0.isin(['Pressure'])][1].iloc[0])\n",
    "        broadeners = list(inp_df[col0.isin(['Broadeners'])].iloc[0])[1:]\n",
    "        broadeners = [i for i in broadeners if i is not np.nan]\n",
    "        ratios = np.array(list(inp_df[col0.isin(['Ratios'])].iloc[0])[1:], dtype=float)\n",
    "        ratios = ratios[~np.isnan(ratios)]\n",
    "        wn_grid = np.linspace(min_wn, max_wn, N_point)\n",
    "        wn_wl = inp_df[col0.isin(['Wavenumber(wn)/wavelength(wl)'])][1].values[0].upper()\n",
    "        profile = inp_df[col0.isin(['Profile'])][1].values[0].upper().replace('PRO','')\n",
    "        # Doppler HWHM\n",
    "        DopplerHWHMYN = inp_df[col0.isin(['DopplerHWHM(Y/N)'])][1].values[0].upper()[0]        \n",
    "        if 'DOP' in profile: \n",
    "            alpha_HWHM = 'None'\n",
    "        elif 'GAU' in profile:\n",
    "            if DopplerHWHMYN == 'Y':\n",
    "                alpha_HWHM = float(inp_df[col0.isin(['DopplerHWHM(Y/N)'])][2].iloc[0])\n",
    "            else:\n",
    "                raise ImportError(\"Gaussian line profile requires a HWHM. \" \n",
    "                                  + \"Please choose 'Y' and give a value for Doppler HWHM in the input file. \" \n",
    "                                  + \"Otherwise, please choose Doppler line profile \" \n",
    "                                  + \"(with calculated temperature-dependent Doppler HWHM).\")\n",
    "        elif 'VOI' in profile:\n",
    "            if DopplerHWHMYN == 'Y':\n",
    "                alpha_HWHM = float(inp_df[col0.isin(['DopplerHWHM(Y/N)'])][2].iloc[0])\n",
    "            elif DopplerHWHMYN == 'N':\n",
    "                alpha_HWHM = 'None'\n",
    "            else:\n",
    "                raise ImportError(\"Please type the correct Doppler HWHM choice 'Y' or 'N' into the input file.\")\n",
    "        else:\n",
    "            alpha_HWHM = 'None'\n",
    "        # Lorentzian HWHM \n",
    "        LorentzianHWHMYN = inp_df[col0.isin(['LorentzianHWHM(Y/N)'])][1].values[0].upper()[0]  \n",
    "        if LorentzianHWHMYN == 'Y':\n",
    "            gamma_HWHM = float(inp_df[col0.isin(['LorentzianHWHM(Y/N)'])][2].iloc[0])\n",
    "        elif LorentzianHWHMYN == 'N':\n",
    "            gamma_HWHM = 'None'\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct Lorentzian HWHM choice 'Y' or 'N' into the input file.\")\n",
    "        # Plot \n",
    "        PlotCrossSectionYN = inp_df[col0.isin(['PlotCrossSection(Y/N)'])][1].values[0].upper()[0]  \n",
    "        if PlotCrossSectionYN == 'Y':\n",
    "            _limitYaxisXsec = inp_df[col0.isin(['Y-axisLimitXsec'])][1].values[0]\n",
    "            if _limitYaxisXsec == '#':\n",
    "                limitYaxisXsec = 1e-30\n",
    "            elif pd.isnull(_limitYaxisXsec) == True:\n",
    "                limitYaxisXsec = 1e-30\n",
    "            else:\n",
    "                limitYaxisXsec = float(_limitYaxisXsec)\n",
    "        else:\n",
    "            limitYaxisXsec = 0\n",
    "    else:\n",
    "        bin_size = 'None'\n",
    "        N_point = 'None'\n",
    "        predissocYN = 'N'\n",
    "        cutoff = 'None'         \n",
    "        alpha_HWHM = 'None'        \n",
    "        gamma_HWHM = 'None'\n",
    "        broadeners = []\n",
    "        ratios = np.array([])\n",
    "        P = 0\n",
    "        wn_grid = np.linspace(0,1,1)\n",
    "        profile = 'None'\n",
    "        wn_wl = 'None'\n",
    "        PlotCrossSectionYN = 'None'\n",
    "        limitYaxisXsec = 0\n",
    "\n",
    "    # Molecule and isotopologue ID, abundance, mass uncertainty, lifetime and g-factor           \n",
    "    molecule_id = int(mol_iso_id/10)\n",
    "    isotopologue_id = mol_iso_id - molecule_id * 10\n",
    "    if database == 'ExoMol':\n",
    "        # Read ExoMol definition file (.def) to get the mass.\n",
    "        deffile_path = (read_path+'/'+molecule+'/'+isotopologue+'/'+dataset+'/'+isotopologue+'__'+dataset+'.def')\n",
    "        def_df = pd.read_csv(deffile_path,sep='\\\\s+',usecols=[0,1,2,3,4],names=['0','1','2','3','4'],header=None)\n",
    "        abundance = 1\n",
    "        mass = float(def_df[def_df['4'].isin(['mass'])]['0'].values[0])     # ExoMol mass (Dalton)\n",
    "        if def_df.to_string().find('Uncertainty') != -1:\n",
    "            check_uncertainty = int(def_df[def_df['2'].isin(['Uncertainty'])]['0'].values[0])\n",
    "        else:\n",
    "            check_uncertainty = 0\n",
    "        if def_df.to_string().find('Predissociative') != -1:\n",
    "            check_predissoc = int(def_df[def_df['2'].isin(['Predissociative'])]['0'].values[0])\n",
    "        else:\n",
    "            check_predissoc = 0\n",
    "        check_lifetime = int(def_df[def_df['2'].isin(['Lifetime'])]['0'].values[0])\n",
    "        check_gfactor = int(def_df[def_df['3'].isin(['g-factor'])]['0'].values[0])\n",
    "    elif database == 'HITRAN':\n",
    "        isometa_url = 'https://hitran.org/docs/iso-meta/'\n",
    "        iso_meta_table = pd.read_html(isometa_url)[molecule_id - 1]\n",
    "        iso_meta_row = iso_meta_table[iso_meta_table['local ID'].isin([isotopologue_id])]\n",
    "        abundance = float(iso_meta_row['Abundance'][0].replace('\\xa0×\\xa010','E'))\n",
    "        mass = float(iso_meta_row['Molar Mass /g·mol-1'])                   # HITRAN molar mass (g/mol)\n",
    "        check_uncertainty = 0\n",
    "        check_predissoc = 0\n",
    "        check_lifetime = 0\n",
    "        check_gfactor = 0\n",
    "    else:\n",
    "        raise ImportError(\"Please add the name of the database 'ExoMol' or 'HITRAN' into the input file.\")\n",
    "\n",
    "    if predissocYN == 'Y' and check_predissoc == 0 and 'LOR' not in profile:\n",
    "        warnings.warn('Program will cost much time on calculating predissociative lifetimes before calculating the cross sections.\\n',InputWarning)\n",
    "            \n",
    "    return (database, molecule, isotopologue, dataset, read_path, save_path, \n",
    "            Conversion, PartitionFunctions, SpecificHeats, CoolingFunctions, Lifetimes, OscillatorStrengths, StickSpectra, CrossSections,\n",
    "            ncputrans, ncpufiles, chunk_size, ConversionFormat, ConversionMinFreq, ConversionMaxFreq, ConversionUnc, ConversionThreshold, \n",
    "            GlobalQNLabel_list, GlobalQNFormat_list, LocalQNLabel_list, LocalQNFormat_list,\n",
    "            Ntemp, Tmax, CompressYN, gfORf, broadeners, ratios, T, P, min_wn, max_wn, N_point, bin_size, wn_grid, \n",
    "            predissocYN, cutoff, threshold, UncFilter, QNslabel_list, QNsformat_list, QNs_label, QNs_value, QNs_format, QNsFilter, \n",
    "            alpha_HWHM, gamma_HWHM, abs_emi, profile, wn_wl, molecule_id, isotopologue_id, abundance, mass,\n",
    "            check_uncertainty, check_lifetime, check_gfactor, check_predissoc, PlotOscillatorStrengthYN, limitYaxisOS,\n",
    "            PlotStickSpectraYN, limitYaxisStickSpectra, PlotCrossSectionYN, limitYaxisXsec)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for calculating\n",
    "Tref = 296.0                        # Reference temperature is 296 K\n",
    "Pref = 1.0                          # Reference pressure is 1 bar\n",
    "N_A = ac.N_A.value                  # Avogadro number (1/mol)\n",
    "h = ac.h.to('erg s').value          # Planck's const (erg s)\n",
    "c = ac.c.to('cm/s').value           # Velocity of light (cm/s)\n",
    "kB = ac.k_B.to('erg/K').value       # Boltzmann's const (erg/K)\n",
    "R = ac.R.to('J / (K mol)').value    # Molar gas constant (J/(K mol))\n",
    "c2 = h * c / kB                     # Second radiation constant (cm K)\n",
    "\n",
    "(database, molecule, isotopologue, dataset, read_path, save_path, \n",
    " Conversion, PartitionFunctions, SpecificHeats, CoolingFunctions, Lifetimes, OscillatorStrengths, StickSpectra, CrossSections,\n",
    " ncputrans, ncpufiles, chunk_size, ConversionFormat, ConversionMinFreq, ConversionMaxFreq, ConversionUnc, ConversionThreshold, \n",
    " GlobalQNLabel_list, GlobalQNFormat_list, LocalQNLabel_list, LocalQNFormat_list,\n",
    " Ntemp, Tmax, CompressYN, gfORf, broadeners, ratios, T, P, min_wn, max_wn, N_point, bin_size, wn_grid, \n",
    " predissocYN, cutoff, threshold, UncFilter, QNslabel_list, QNsformat_list, QNs_label, QNs_value, QNs_format, QNsFilter, \n",
    " alpha_HWHM, gamma_HWHM, abs_emi, profile, wn_wl, molecule_id, isotopologue_id, abundance, mass, \n",
    " check_uncertainty, check_lifetime, check_gfactor, check_predissoc, PlotOscillatorStrengthYN, limitYaxisOS,\n",
    " PlotStickSpectraYN, limitYaxisStickSpectra, PlotCrossSectionYN, limitYaxisXsec) = inp_para(inp_filepath)\n",
    "\n",
    "# Constants\n",
    "c2InvTref = c2 / Tref                 # c2 / T_ref (cm)\n",
    "hc = h * c                            # erg cm\n",
    "PI = np.pi\n",
    "ln22 = np.log(2)*2\n",
    "sinPI = np.sin(np.pi)\n",
    "SqrtPI = np.sqrt(np.pi)\n",
    "Sqrtln2 = np.sqrt(np.log(2))\n",
    "OneminSqrtPIln2 = 1 - np.sqrt(np.pi * np.log(2))\n",
    "Negln2 = -np.log(2)\n",
    "PI4c = np.pi * 4 * c\n",
    "Inv8Pic = 1 / (8 * np.pi * c)         # 8 * pi * c (s/cm)\n",
    "Inv4Pi = 1 / (4 * np.pi)\n",
    "Inv2ln2 = 1 / (2 * np.log(2))\n",
    "InvSqrt2 = 1 / np.sqrt(2)\n",
    "InvSqrtPi= 1 / np.sqrt(np.pi)\n",
    "InvSprtln2 = 1 / np.sqrt(np.log(2))\n",
    "InvSqrt2Pi = 1 / np.sqrt(2 * np.pi)\n",
    "InvSqrt2ln2 = 1 / np.sqrt(2 * np.log(2))\n",
    "TwoSqrt2ln2 = 2 * np.sqrt(2 * np.log(2))\n",
    "Sqrtln2InvPi = np.sqrt(np.log(2) / np.pi)\n",
    "Sqrt2NAkBln2mInvc = np.sqrt(2 * N_A * kB * np.log(2) / mass) / c\n",
    "if bin_size != 'None':\n",
    "    binSize2 = bin_size * 2\n",
    "    binSizePI = bin_size * np.pi\n",
    "    binSizeHalf = bin_size / 2 \n",
    "    InvbinSizePIhalf = 1 / (bin_size * np.pi**0.5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert frequency, upper and lower energy and J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate frequency\n",
    "def cal_v(Ep, Epp):\n",
    "    v = ne.evaluate('Ep - Epp')\n",
    "    return(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate upper state energy with ExoMol database\n",
    "def cal_Ep(Epp, v):\n",
    "    Ep = ne.evaluate('Epp + v')\n",
    "    return(Ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate upper state energy with HITRAN database\n",
    "def cal_Ep_hitran(hitran_df):\n",
    "    Epp = hitran_df['Epp'].values\n",
    "    v = hitran_df['v'].values\n",
    "    Ep = cal_Ep(Epp, v)\n",
    "    Ep_df = pd.DataFrame(Ep,columns=['Ep'])\n",
    "    return(Ep_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate upper J\n",
    "def cal_Jp(Fp, Fpp, Jpp):\n",
    "    Jp = ne.evaluate('Fp + Fpp - Jpp')\n",
    "    return(Jp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F\n",
    "def cal_F(g):\n",
    "    F = ne.evaluate('(g - 1) * 0.5')\n",
    "    return(F)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Input Files\n",
    "\n",
    "Read the parameters of the linelist in ExoMol or HITRAN format text file. Return the dataframe of the data for the following calculations.\n",
    "\n",
    "## Read ExoMol Database Files\n",
    "\n",
    "### Decompress Large .trans.bz2 Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decompress Large .trans.bz2 Files\n",
    "def command_decompress(trans_filename):\n",
    "    # Directory where the decompressed .trans files will be saved\n",
    "    trans_dir = read_path+molecule+'/'+isotopologue+'/'+dataset+'/decompressed/'\n",
    "    if os.path.exists(trans_dir):\n",
    "        pass\n",
    "    else:\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(trans_dir, exist_ok=True)\n",
    "    trans_file = os.path.join(trans_dir, trans_filename.split('/')[-1].replace('.bz2', ''))\n",
    "    if os.path.exists(trans_file):\n",
    "        num = 0\n",
    "    else:\n",
    "        command = f'bunzip2 < {trans_filename} > {trans_file}'\n",
    "        print('Decompressing file:', trans_filename)\n",
    "        subprocess.run(command, shell=True)\n",
    "        num = 1\n",
    "    return(trans_file, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read States File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_states(read_path):\n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    print('Reading states ...')\n",
    "    states_df = pd.DataFrame()\n",
    "    states_filename = (read_path + molecule + '/' + isotopologue + '/' + dataset \n",
    "                       + '/' + isotopologue + '__' + dataset + '.states.bz2')\n",
    "    \n",
    "    chunks = pd.read_csv(states_filename, compression='bz2', sep='\\s+', header=None,\n",
    "                         chunksize=100_000, iterator=True, low_memory=False, dtype=object)\n",
    "    for chunk in chunks:\n",
    "        states_df = pd.concat([states_df, chunk])\n",
    "    if check_uncertainty == 1:\n",
    "        states_df = states_df.rename(columns={0:'id',1:'E',2:'g',3:'J',4:'unc'})\n",
    "        convert_dict = {'id':np.int32,'E':np.float64,'g':np.int32,'J':np.float16,'unc':np.float32}\n",
    "        states_df = states_df.astype(convert_dict)\n",
    "    else:      \n",
    "        states_df = states_df.rename(columns={0:'id',1:'E',2:'g',3:'J'})  \n",
    "        convert_dict = {'id':np.int32,'E':np.float64,'g':np.int32,'J':np.float16}\n",
    "        states_df = states_df.astype(convert_dict)\n",
    "    t.end()     \n",
    "    print('Finished reading states!\\n')       \n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')                \n",
    "    return(states_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get transitions File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from indexed_bzip2 import IndexedBzip2File\n",
    "# def read_trans(trans_filename):\n",
    "#     file = IndexedBzip2File(trans_filename, parallelization=num_cpus)\n",
    "#     lines = file.readlines()\n",
    "#     file.close()\n",
    "#     u, l, A = [], [], []\n",
    "#     for line in lines:\n",
    "#         part = line.split()\n",
    "#         u.append(int(part[0]))\n",
    "#         l.append(int(part[1]))\n",
    "#         A.append(float(part[2]))   \n",
    "#     trans_df = pd.DataFrame({'u':u, 'l':l, 'A':A})\n",
    "#     return trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transfiles(read_path):\n",
    "    # Get all the transitions files from the folder including the older version files which are named by vn(version number).\n",
    "    trans_filepaths_all = glob.glob(read_path + molecule + '/' + isotopologue + '/' + dataset + '/' + '*trans.bz2')\n",
    "    num_transfiles_all = len(trans_filepaths_all)    # The number of all transitions files including the older version files.\n",
    "    trans_filepaths = []    # The list of the lastest transitions files.\n",
    "    all_decompress_num = 0\n",
    "    decompress_num = 0\n",
    "    for i in range(num_transfiles_all):\n",
    "        split_version = trans_filepaths_all[i].split('__')[-1].split('.')[0].split('_')    # Split the filenames.\n",
    "        num = len(split_version)\n",
    "        # There are four format filenames.\n",
    "        # The lastest transitions files named in four formats:\n",
    "        # 1. Filenames are named with the name of isotopologue and dataset. \n",
    "        #    End with .trans.bz2.\n",
    "        #    e.g. 14N-16O__XABC.trans.bz2'\n",
    "        # 2. Filenames are named with the name of isotopologue and dataset. \n",
    "        #    Also have the range of wavenumbers xxxxx-yyyyy.\n",
    "        #    End with .trans.bz2.\n",
    "        #    e.g. 1H2-16O__POKAZATEL__00000-00100.trans.bz2\n",
    "        # 3. The older version transitions files are named with vn(version number) based on the first format of the lastest files.\n",
    "        #    e.g. 14N-16O__XABC_v2.trans.bz2\n",
    "        # 4. The older version transitions files are named with updated date (yyyymmdd).\n",
    "        #    e.g. 1H3_p__MiZATeP__20170330.trans.bz2\n",
    "        # After split the filenames:\n",
    "        # The first format filenames only leave the dataset name, e.g. XABC.\n",
    "        # The second format filenames only leave the range of the wavenumber, e.g. 00000-00100.\n",
    "        # The third format filenames leave two parts(dataset name and version number), e.g. XABC and v2.\n",
    "        # The fourth format filenames only leave the updated date, e.g. 20170330.\n",
    "        # This program only process the lastest data, so extract the filenames named by the first two format.\n",
    "        if num == 1:     \n",
    "            if split_version[0] == dataset:        \n",
    "                trans_filepaths.append(trans_filepaths_all[i])\n",
    "            elif len(split_version[0].split('-')) == 2:\n",
    "                file_size_bytes = os.path.getsize(trans_filepaths_all[i])\n",
    "                if file_size_bytes/1024**3 > 2:   \n",
    "                    (trans_filepath, num) = command_decompress(trans_filepaths_all[i])\n",
    "                    all_decompress_num += 1\n",
    "                    decompress_num += num\n",
    "                else:\n",
    "                    trans_filepath = trans_filepaths_all[i]\n",
    "                trans_filepaths.append(trans_filepath)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    print('Number of all transitions files \\t\\t:', num_transfiles_all)\n",
    "    print('Number of all decompressed transitions files \\t:', all_decompress_num)\n",
    "    print('Number of new decompressed transitions files \\t:', decompress_num)\n",
    "    return trans_filepaths    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Partition Function File From ExoMol Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read partition function with online webpage\n",
    "def read_exomolweb_pf(T):\n",
    "    pf_url = ('http://www.exomol.com/db/' + molecule + '/' + isotopologue + '/' + dataset \n",
    "              + '/' + isotopologue + '__' + dataset + '.pf')\n",
    "    pf_content = requests.get(pf_url).text\n",
    "    pf_col_name = ['T', 'Q']\n",
    "    pf_df = pd.read_csv(StringIO(pf_content), sep='\\\\s+', names=pf_col_name, header=None)\n",
    "    Q = pf_df['Q'][T-1]\n",
    "    return(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read partition function with local partition function file\n",
    "def read_exomol_pf(read_path, T):\n",
    "    pf_filename = (read_path + molecule + '/' + isotopologue + '/' + dataset \n",
    "                   + '/' + isotopologue + '__' + dataset + '.pf')\n",
    "    pf_col_name = ['T', 'Q']\n",
    "    pf_df = pd.read_csv(pf_filename, sep='\\\\s+', names=pf_col_name, header=None)\n",
    "    Q = pf_df['Q'][T-1]\n",
    "    return(Q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Broadening File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_broad(read_path):\n",
    "    print('Reading broadening file ...')\n",
    "    broad_df = pd.DataFrame()\n",
    "    broad_dfs = []\n",
    "    broad = []\n",
    "    ratio = []\n",
    "    for i in range(len(ratios)):\n",
    "        if ratios[i] != 0.0:\n",
    "            if broadeners[i].upper()[0:3] == 'DEF':\n",
    "                default_gamma_L = 0.07\n",
    "                default_n_air = 0.5\n",
    "                broad_df = pd.DataFrame([['code', default_gamma_L, default_n_air,'Jpp']])\n",
    "                broad_df = broad_df.rename(columns={0:'code', 1:'gamma_L', 2:'n_air', 3:'Jpp'})\n",
    "                broad_dfs.append(broad_df)\n",
    "            else:\n",
    "                broadener_name = str(broadeners[i])\n",
    "                pattern_broadener = read_path + molecule + '/**/*' + broadener_name + '.broad'\n",
    "                if glob.glob(pattern_broadener, recursive=True) != []:\n",
    "                    for fname_broadener in glob.glob(pattern_broadener, recursive=True):\n",
    "                        broad_df = pd.read_csv(fname_broadener, sep='\\s+', header=None, engine='python')\n",
    "                        broad_df = broad_df.rename(columns={0:'code', 1:'gamma_L', 2:'n_air', 3:'Jpp'})\n",
    "                        broad_dfs.append(broad_df)\n",
    "                else:\n",
    "                    raise ImportError('The ' + broadener_name + ' boradening file does not exist.') \n",
    "            broad.append(broadeners[i])\n",
    "            ratio.append(ratios[i])\n",
    "    nbroad = len(broad)\n",
    "    broad = list(i for i in broad if i==i)\n",
    "    ratio = list(i for i in ratio if i==i)\n",
    "    print('Broadeners \\t:', str(broad).replace('[','').replace(']','').replace(\"'\",''))\n",
    "    print('Ratios \\t\\t:', str(ratio).replace('[','').replace(']',''),'\\n')\n",
    "    return(broad, ratio, nbroad, broad_dfs)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum number filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QNfilter_linelist(linelist_df, QNs_value, QNs_label):\n",
    "    for i in range(len(QNs_label)):\n",
    "        if QNs_value[i] != ['']:\n",
    "            linelist_df[i] = linelist_df[[QNs_label[i]+\"'\",QNs_label[i]+'\"']].fillna('').agg(','.join, axis=1).astype(str).str.replace(' ', '')\n",
    "            uval = linelist_df[QNs_label[i]+\"'\"].astype(str).str.replace(' ', '').drop_duplicates().values\n",
    "            lval = linelist_df[QNs_label[i]+'\"'].astype(str).str.replace(' ', '').drop_duplicates().values\n",
    "            vallist = []\n",
    "            ulist = []\n",
    "            llist = []\n",
    "            for qnval in QNs_value[i]:\n",
    "                if '' not in qnval.split(','):\n",
    "                    vallist.append(qnval)\n",
    "                elif qnval.split(',')[0] == '':\n",
    "                    ulist.append([qnval.replace(\",\",val+\",\") for val in uval])\n",
    "                elif qnval.split(',')[1] == '':\n",
    "                    llist.append([qnval.replace(\",\",\",\"+val) for val in lval])\n",
    "            QNs_value[i] = vallist+list(chain(*ulist))+list(chain(*llist))\n",
    "            linelist_df = linelist_df[linelist_df[i].isin(QNs_value[i])].drop(columns=[i])   \n",
    "    # linelist_df = linelist_df.sort_values('v')  \n",
    "    return(linelist_df)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read HITRAN Database Files\n",
    "\n",
    "### Read HITRAN Linelist File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parfile(read_path):\n",
    "    if not os.path.exists(read_path):\n",
    "        raise ImportError('The input file ' + read_path + ' does not exist.')\n",
    "    # Initialise the iterator object.\n",
    "    read_par = pd.read_csv(read_path, chunksize=100_000, iterator=True, header=None, encoding='utf-8')\n",
    "    par_df = pd.DataFrame()\n",
    "    for chunk in read_par:\n",
    "        par_df = pd.concat([par_df, chunk])\n",
    "    return(par_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process HITRAN linelist data\n",
    "def read_hitran_parfile(read_path, parfile_df, minv, maxv, unclimit, Slimit):\n",
    "    '''\n",
    "    Read the parameters of the molecular absorption features\n",
    "    of HITRAN2020 format text file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    par_filepath : str\n",
    "        Input file path for reading.\n",
    "    Return\n",
    "    ------\n",
    "    hitran_df : DataFrame\n",
    "        The DataFrame of HITRAN data for the molecule.\n",
    "    '''    \n",
    "    par_filename = read_path.split('/')[-1]\n",
    "    if (len(str(parfile_df[0][0])) < 160):\n",
    "        raise ImportError('The file ' + par_filename + ' is not a HITRAN2020 format data file.')\n",
    "    #hitran_column_name = ['M','I','v','S','Acoeff','gamma_air','gamma_self',\n",
    "    #                     'Epp','n_air','delta_air','Vp','Vpp','Qp','Qpp',\n",
    "    #                     'Ierr','Iref','flag','gp','gpp']\n",
    "\n",
    "    hitran_df = pd.DataFrame()\n",
    "    hitran_df['M'] = pd.to_numeric(parfile_df[0].map(lambda x: x[0:2]), errors='coerce').astype('int64')                 # Molecule identification number\n",
    "    hitran_df['I'] = pd.to_numeric(parfile_df[0].map(lambda x: x[2:3]), errors='coerce').astype('int64')                 # Isotopologue number\n",
    "    hitran_df['v'] = pd.to_numeric(parfile_df[0].map(lambda x: x[3:15]), errors='coerce').astype('float64')              # Transition wavenumber (in cm^{-1})\n",
    "    hitran_df['S'] = pd.to_numeric(parfile_df[0].map(lambda x: x[15:25]), errors='coerce').astype('float64')             # Intensity (cm^{-1} / (molecule cm^{-2}))\n",
    "    hitran_df['A'] = pd.to_numeric(parfile_df[0].map(lambda x: x[25:35]), errors='coerce').astype('float64')             # The Einstein-A coefficient (s^{-1}) of a transition\n",
    "    hitran_df['gamma_air'] = pd.to_numeric(parfile_df[0].map(lambda x: x[35:40]), errors='coerce').astype('float64')     # Air-broadened half-width at half maximum (HWHM) coefficient (cm^{-1} atm^{-1})\n",
    "    hitran_df['gamma_self'] = pd.to_numeric(parfile_df[0].map(lambda x: x[40:45]), errors='coerce').astype('float64')    # Self-broadened half-width at half maximum (HWHM) coefficient (cm^{-1} atm^{-1})\n",
    "    hitran_df['Epp'] = pd.to_numeric(parfile_df[0].map(lambda x: x[45:55]), errors='coerce').astype('float64')           # Lower state energy (cm^{-1})\n",
    "    hitran_df['n_air'] = pd.to_numeric(parfile_df[0].map(lambda x: x[55:59]), errors='coerce').astype('float64')         # Temperature-dependent exponent for gamma_air\n",
    "    hitran_df['delta_air'] = pd.to_numeric(parfile_df[0].map(lambda x: x[59:67]), errors='coerce').astype('float64')     # Air pressure_include line shift (cm^{-1} atm^{-1})\n",
    "    hitran_df['Vp'] = parfile_df[0].map(lambda x: x[67:82])                                                              # Upper-state \"global\" quanta\n",
    "    hitran_df['Vpp'] = parfile_df[0].map(lambda x: x[82:97])                                                             # Lower-state \"global\" quanta\n",
    "    hitran_df['Qp'] = parfile_df[0].map(lambda x: x[97:112])                                                             # Upper-state \"local\" quanta\n",
    "    hitran_df['Qpp'] = parfile_df[0].map(lambda x: x[112:127])                                                           # Lower-state \"local\" quanta\n",
    "    #hitran_df['Unc'] = parfile_df[0].map(lambda x: x[127:128])                                                          # Uncertainty code, first integer in the error code\n",
    "    hitran_df['Unc'] = pd.to_numeric(parfile_df[0].map(lambda x: x[127:128]), errors='coerce').astype('int64')           # Uncertainty code, first integer in the error code\n",
    "    hitran_df['gp'] = pd.to_numeric(parfile_df[0].map(lambda x: x[146:153]), errors='coerce').astype('int64')            # Statistical weight of the upper state\n",
    "    hitran_df['gpp'] = pd.to_numeric(parfile_df[0].map(lambda x: x[153:160]), errors='coerce').astype('int64')           # Statistical weight of the lower state\n",
    "    \n",
    "    hitran_df = hitran_df[hitran_df['M'].isin([molecule_id])]\n",
    "    hitran_df = hitran_df[hitran_df['I'].isin([isotopologue_id])]\n",
    "    hitran_df = hitran_df[hitran_df['v'].between(minv, maxv)]\n",
    "    if unclimit != 'None':\n",
    "        hitran_df = hitran_df[hitran_df['Unc'] >= int(('%e' % unclimit)[-1])]\n",
    "    if Slimit != 'None':\n",
    "        hitran_df = hitran_df[hitran_df['S'] >= Slimit]\n",
    "    return(hitran_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Partition Function File From HITRANOnline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read partition function file from HITRANOnline\n",
    "def read_hitran_pf(T):\n",
    "    isometa_url = 'https://hitran.org/docs/iso-meta/'\n",
    "    iso_meta_table = pd.read_html(isometa_url)[molecule_id - 1]\n",
    "    iso_meta_row = iso_meta_table[iso_meta_table['local ID'].isin([isotopologue_id])]\n",
    "    #Q_ref = float(iso_meta_row.loc[0][6].replace('\\xa0×\\xa010','E'))\n",
    "    Q_url = 'https://hitran.org/data/Q/' + iso_meta_row.loc[0][7]\n",
    "    Q_content = requests.get(Q_url).text\n",
    "    Q_col_name = ['T', 'Q']\n",
    "    Q_df = pd.read_csv(StringIO(Q_content), sep='\\\\s+', names=Q_col_name, header=None)\n",
    "    Q = Q_df['Q'][T - 1]          \n",
    "    return(Q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process HITRAN quantum numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global quantum numbers\n",
    "def globalQNclasses(molecule,isotopologue):\n",
    "    globalQNclass1a = {'class':['CO','HF','HBr','HI','N2','NO+','NO_p','H2','CS'],\n",
    "                       'label': ['none','v1'],\n",
    "                       'format':['%13s','%2d']}\n",
    "    globalQNclass1b = {'class':['O2','NO','OH','ClO','SO'],\n",
    "                       'label':['none','X','Omega','none','v1'],\n",
    "                       'format':['%6s','%2s','%3s','%2s','%2d']}\n",
    "    globalQNclass2a = {'class':['CO2'],\n",
    "                       'label':['none','v1','v2','l2','v3','r'],\n",
    "                       'format':['%6s','%2d','%2d','%2d','%2d','%1d']}\n",
    "    globalQNclass2b = {'class':['N2O','OCS','HCN','CS2'],\n",
    "                       'label':['none','v1','v2','l2','v3'],\n",
    "                       'format':['%7s','%2d','%2d','%2d','%2d']}\n",
    "    globalQNclass3  = {'class':['H2O','O3','SO2','NO2','HOCl','H2S','HO2','HOBr'],\n",
    "                       'label':['none','v1','v2','v3'],\n",
    "                       'format':['%9s','%2d','%2d','%2d']}\n",
    "    globalQNclass4a = {'class':['15N-1H3','PH3','NF3'],\n",
    "                       'label':['none','v1','v2','v3','v4','S'],\n",
    "                       'format':['%5s','%2d','%2d','%2d','%2d','%2s']}\n",
    "    globalQNclass4b = {'class':['14N-1H3'],\n",
    "                       'label':['none','v1','v2','v3','v4','none','l3','l4','none','l','none','Gvib'],\n",
    "                       'format':['%1s','%1d','%1d','%1d','%1d','%1s','%1d','%1d','%1s','%1d','%1s','%4s']}\n",
    "    globalQNclass5a = {'class':['C2H2'],\n",
    "                       'label':['none','v1','v2','v3','v4','v5','l4','l5','+-','none','S'],\n",
    "                       'format':['%1s','%1d','%1d','%1d','%2d','%2d','%2d','%2d','%1s','%1s','%1s']}\n",
    "    globalQNclass5b = {'class':['C4H2'],\n",
    "                       'label':['none','v1','v2','v3','v4','v5','v6','v7','v8','v9','none','Sym','none','S'],\n",
    "                       'format':['%1s','%1d','%1d','%1d','%1d','%1d','%1d','%1d','%1d','%1d','%1s','%1s','%1s','%2s']}\n",
    "    globalQNclass5c = {'class':['HC3N'],\n",
    "                       'label':['none','v1','v2','v3','v4','v5','v6','v7','l5','l6','l7'],\n",
    "                       'format':['%2s','%1d','%1d','%1d','%1d','%1d','%1d','%1d','%2d','%2d','%2d']}\n",
    "    globalQNclass5d = {'class':['C2N2'],\n",
    "                       'label':['v1','v2','v3','v4','v5','l','+-','r','S'],\n",
    "                       'format':['%2d','%2d','%2d','%2d','%2d','%2d','%1s','%1d','%1s']}\n",
    "    globalQNclass6a = {'class':['H2CO','COF2','COCl2'],\n",
    "                       'label':['none','v1','v2','v3','v4','v5','v6'],\n",
    "                       'format':['%3s','%2d','%2d','%2d','%2d','%2d','%2d']}\n",
    "    globalQNclass6b = {'class':['H2O2'],\n",
    "                       'label':['none','v1','v2','v3','n','r','v5','v6'],\n",
    "                       'format':['%3s','%2d','%2d','%2d','%1d','%1d','%2d','%2d']}\n",
    "    globalQNclass7  = {'class':['SO3'],\n",
    "                       'label':['v1','v2','v3','l3','v4','l4','Gvib'],\n",
    "                       'format':['%2d','%2d','%2d','%2d','%2d','%2d','%3s']}\n",
    "    globalQNclass8  = {'class':['12C-1H4','13C-1H4','CF4','GeH4'],\n",
    "                       'label':['none','v1','v2','v3','v4','n','C'],\n",
    "                       'format':['%3s','%2d','%2d','%2d','%2d','%2s','%2s']}\n",
    "    globalQNclass9  = {'class':['12C-1H3-2H','13C-1H3-2H','HNO3','CH3Cl','C2H6','SF6','HCOOH','ClONO2','C2H4','CH3OH','CH3Br','CH3CN','CH3F','CH3I'],\n",
    "                       'label':['vibband'],\n",
    "                       'format':['%15s']}\n",
    "    globalQNclass = [globalQNclass1a,globalQNclass1b,globalQNclass2a,globalQNclass2b,globalQNclass3,\n",
    "                     globalQNclass4a,globalQNclass4b,globalQNclass5a,globalQNclass5b,globalQNclass5c,globalQNclass5d,\n",
    "                     globalQNclass6a,globalQNclass6b,globalQNclass7,globalQNclass8,globalQNclass9]\n",
    "    count = 0\n",
    "    for gQNclass in globalQNclass:\n",
    "        if molecule in gQNclass.get('class'):\n",
    "            GlobalQNLabels = gQNclass.get('label')\n",
    "            GlobalQNFormats = gQNclass.get('format')   \n",
    "        elif isotopologue in gQNclass.get('class'):\n",
    "            GlobalQNLabels = gQNclass.get('label')\n",
    "            GlobalQNFormats = gQNclass.get('format')\n",
    "        else:\n",
    "            count += 1\n",
    "    if count == 16:\n",
    "        GlobalQNLabels = GlobalQNLabel_list\n",
    "        GlobalQNFormats = GlobalQNFormat_list\n",
    "    return(GlobalQNLabels,GlobalQNFormats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local quantum numbers\n",
    "def localQNgroups(molecule,isotopologue):\n",
    "    localQNgroup1  = {'group':['H2O','O3','SO2','NO2','HNO3','H2CO','HOCl','H2O2','COF2','H2S','HCOOH','HO2','ClONO2','HOBr','C2H4','COCl2'],\n",
    "                      'ulabel': ['J','Ka','Kc','F','Sym'],\n",
    "                      'uformat':['%3d','%3d','%3d','%5s','%1s'],\n",
    "                      'llabel': ['J','Ka','Kc','F','Sym'],\n",
    "                      'lformat':['%3d','%3d','%3d','%5s','%1s']}\n",
    "    localQNgroup2a = {'group':['CO2','N2O','CO','HF','HCl','HBr','HI','OCS','N2','HCN','NO+','NO_p','HC3N','H2','CS','C2N2','CS2'],\n",
    "                      'ulabel':['m','none','F'],\n",
    "                      'uformat':['%1s','%9s','%5s'],\n",
    "                      'llabel': ['none','Br','J','Sym','F'],\n",
    "                      'lformat':['%5s','%1s','%3d','%1s','%5s']}\n",
    "    localQNgroup2b = {'group':['C4H2'],\n",
    "                      'ulabel':['l6','l7','l8','l9','none'],\n",
    "                      'uformat':['%2s','%2s','%2s','%2s','%7s'],\n",
    "                      'llabel': ['l6','l7','l8','l9','none','Br','J','Sym','none'],\n",
    "                      'lformat':['%2s','%2s','%2s','%2s','%1s','%1s','%3d','%1s','%1s']}\n",
    "    localQNgroup3  = {'group':['12C-1H4','13C-1H4','SF6','CF4','GeH4'],\n",
    "                      'ulabel':['none','J','C','alpha','F'],\n",
    "                      'uformat':['%2s','%3d','%2s','%3d','%5s'],\n",
    "                      'llabel': ['none','J','C','alpha','F'],\n",
    "                      'lformat':['%2s','%3d','%2s','%3d','%5s']}\n",
    "    localQNgroup4a = {'group':['12C-1H3-2H','13C-1H3-2H','15N-1H3','CH3Cl','PH3','CH3OH','CH3Br','CH3CN','CH3F','CH3I','NF3'],\n",
    "                      'ulabel':['J','K','l','C','Sym','F'],\n",
    "                      'uformat':['%3d','%3d','%2d','%2s','%1s','%4s'],\n",
    "                      'llabel': ['J','K','l','C','Sym','F'],\n",
    "                      'lformat':['%3d','%3d','%2d','%2s','%1s','%4s']}\n",
    "    localQNgroup4b = {'group':['14N-1H3'],\n",
    "                      'ulabel':['J','K','l','none','Grot','Gtot','none'],\n",
    "                      'uformat':['%2d','%3d','%2d','%1s','%3s','%3s','%1s'],\n",
    "                      'llabel': ['J','K','l','none','Grot','Gtot','none'],\n",
    "                      'lformat':['%2d','%3d','%2d','%1s','%3s','%3s','%1s']}\n",
    "    localQNgroup4c = {'group':['C2H6'],\n",
    "                      'ulabel':['J','K','l','Sym','F'],\n",
    "                      'uformat':['%3d','%3d','%2d','%3s','%4s'],\n",
    "                      'llabel': ['J','K','l','Sym','F'],\n",
    "                      'lformat':['%3d','%3d','%2d','%3s','%4s']}\n",
    "    localQNgroup5  = {'group':['SO3'],\n",
    "                      'ulabel':['none','J','K','none','Gtot','none'],\n",
    "                      'uformat':['%3s','%3d','%3d','%2s','%3s','%1s'],\n",
    "                      'llabel': ['none','J','K','none','Grot','none'],\n",
    "                      'lformat':['%3s','%3d','%3d','%2s','%3s','%1s']}\n",
    "    localQNgroup6  = {'group':['O2','SO'],\n",
    "                      'ulabel':['none','F'],\n",
    "                      'uformat':['%10s','%5s'],\n",
    "                      'llabel': ['none','Br1','N','Br2','J','F','M'],\n",
    "                      'lformat':['%1s','%1s','%3d','%1s','%3d','%5s','%1s']}\n",
    "    localQNgroup7a = {'group':['NO','ClO'],\n",
    "                      'ulabel':['none','none','F'],     # m, none, F\n",
    "                      'uformat':['%1s','%9s','%5s'],\n",
    "                      'llabel': ['none','Br','J','Sym','F'],\n",
    "                      'lformat':['%2s','%2s','%5.1f','%1s','%5s']}\n",
    "    localQNgroup7b = {'group':['OH'],\n",
    "                      'ulabel':['none','F'],\n",
    "                      'uformat':['%10s','%5s'],\n",
    "                      'llabel': ['none','Br','J','Sym','F'],\n",
    "                      'lformat':['%1s','%2s','%5.1f','%2s','%5s']}\n",
    "    localQNgroup = [localQNgroup1,localQNgroup2a,localQNgroup2b,localQNgroup3,\n",
    "                    localQNgroup4a,localQNgroup4b,localQNgroup4c,\n",
    "                    localQNgroup5,localQNgroup6,localQNgroup7a,localQNgroup7b]\n",
    "    count = 0\n",
    "    for lQNgroup in localQNgroup:\n",
    "        if molecule in lQNgroup.get('group'):\n",
    "            LocalQNupperLabels = lQNgroup.get('ulabel')\n",
    "            LocalQNupperFormats = lQNgroup.get('uformat')\n",
    "            LocalQNlowerLabels = lQNgroup.get('llabel')\n",
    "            LocalQNlowerFormats = lQNgroup.get('lformat')\n",
    "        elif isotopologue in lQNgroup.get('group'):\n",
    "            LocalQNupperLabels = lQNgroup.get('ulabel')\n",
    "            LocalQNupperFormats = lQNgroup.get('uformat')\n",
    "            LocalQNlowerLabels = lQNgroup.get('llabel')\n",
    "            LocalQNlowerFormats = lQNgroup.get('lformat')\n",
    "        else:\n",
    "            count += 1\n",
    "    if count == 11:\n",
    "        LocalQNupperLabels = LocalQNLabel_list\n",
    "        LocalQNupperFormats = LocalQNFormat_list\n",
    "        LocalQNlowerLabels = LocalQNLabel_list\n",
    "        LocalQNlowerFormats = LocalQNFormat_list     \n",
    "    return(LocalQNupperLabels, LocalQNlowerLabels, LocalQNupperFormats, LocalQNlowerFormats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_QN_hitran(hitran_df,GlobalQNLabels,LocalQNupperLabels,LocalQNlowerLabels,\n",
    "                       GlobalQNFormats,LocalQNupperFormats,LocalQNlowerFormats):\n",
    "    GlobalQNLabels,GlobalQNFormats = globalQNclasses(molecule,isotopologue)\n",
    "    LocalQNupperLabels, LocalQNlowerLabels, LocalQNupperFormats, LocalQNlowerFormats = localQNgroups(molecule,isotopologue)\n",
    "    GQN_format = [int(s) for s in str(GlobalQNFormats).replace('.1','') if s.isdigit()]\n",
    "    LQNu_format = [int(s) for s in str(LocalQNupperFormats).replace('.1','') if s.isdigit()]\n",
    "    LQNl_format = [int(s) for s in str(LocalQNlowerFormats).replace('.1','') if s.isdigit()]\n",
    "    hitran_df['Qp'] = [x.replace(' .5','0.5') for x in hitran_df['Qp'].values]\n",
    "    hitran_df['Qpp'] = [x.replace(' .5','0.5') for x in hitran_df['Qpp'].values]\n",
    "    hitran_df['Vp'] = [x.replace(' .5','0.5') for x in hitran_df['Vp'].values]\n",
    "    hitran_df['Vpp'] = [x.replace(' .5','0.5') for x in hitran_df['Vpp'].values]\n",
    "    # Global quantum numbers    \n",
    "    n_GQN = len(GlobalQNLabels)\n",
    "    GQNlsum = []\n",
    "    GQNrsum = []\n",
    "    for i in range(n_GQN):\n",
    "        GQNlsum.append(sum(GQN_format[:i]))\n",
    "        GQNrsum.append(sum(GQN_format[:i+1]))  \n",
    "    GQNu_df = pd.DataFrame(columns=GlobalQNLabels)  \n",
    "    GQNl_df = pd.DataFrame(columns=GlobalQNLabels)  \n",
    "    for i in range(n_GQN):\n",
    "        GQNu_df[GlobalQNLabels[i]] = hitran_df['Vp'].map(lambda x: x[GQNlsum[i]:GQNrsum[i]]) \n",
    "        GQNl_df[GlobalQNLabels[i]] = hitran_df['Vpp'].map(lambda x: x[GQNlsum[i]:GQNrsum[i]]) \n",
    "    if 'none' in GlobalQNLabels:\n",
    "        GQNu_df = GQNu_df.drop(columns=['none'])\n",
    "        GQNl_df = GQNl_df.drop(columns=['none'])\n",
    "\n",
    "    # Local quantum numbers    \n",
    "    n_LQNu = len(LocalQNupperLabels)\n",
    "    LQNulsum = []\n",
    "    LQNursum = []\n",
    "    for i in range(n_LQNu):\n",
    "        LQNulsum.append(sum(LQNu_format[:i]))\n",
    "        LQNursum.append(sum(LQNu_format[:i+1]))\n",
    "    n_LQNl = len(LocalQNlowerLabels)\n",
    "    LQNllsum = []\n",
    "    LQNlrsum = []\n",
    "    for i in range(n_LQNl):\n",
    "        LQNllsum.append(sum(LQNl_format[:i]))\n",
    "        LQNlrsum.append(sum(LQNl_format[:i+1]))\n",
    "    LQNu_df = pd.DataFrame(columns=LocalQNupperLabels)  \n",
    "    for i in range(n_LQNu):\n",
    "        LQNu_df[LocalQNupperLabels[i]] = hitran_df['Qp'].map(lambda x: x[LQNulsum[i]:LQNursum[i]]) \n",
    "    if 'none' in LocalQNupperLabels:\n",
    "        LQNu_df = LQNu_df.drop(columns=['none'])\n",
    "    LQNl_df = pd.DataFrame(columns=LocalQNlowerLabels)  \n",
    "    for i in range(n_LQNl):\n",
    "        LQNl_df[LocalQNlowerLabels[i]] = hitran_df['Qpp'].map(lambda x: x[LQNllsum[i]:LQNlrsum[i]]) \n",
    "    if 'none' in LocalQNlowerLabels:\n",
    "        LQNl_df = LQNl_df.drop(columns=['none'])\n",
    "        \n",
    "    if 'Br' in LocalQNupperLabels:\n",
    "        LQNu_df = LQNu_df.drop(columns=['Br'])    \n",
    "    if 'Br' in LocalQNlowerLabels:\n",
    "        LQNl_df['Br'] = LQNl_df['Br'].map(lambda x: x[1]).replace(['Q','P','R','O','S'],[0,-1,1,-2,2])\n",
    "        LQNl_df['J'] = pd.to_numeric(LQNl_df['J'])\n",
    "        LQNu_df['J'] = LQNl_df['Br'] + LQNl_df['J']\n",
    "        LQNl_df = LQNl_df.drop(columns=['Br'])\n",
    "\n",
    "    if 'F' not in LocalQNupperLabels:\n",
    "        LQNu_df['F'] = LQNu_df['J']\n",
    "        LQNl_df['F'] = LQNl_df['J']\n",
    "        \n",
    "    QNu_df = pd.concat([GQNu_df, LQNu_df], axis='columns')\n",
    "    QNl_df = pd.concat([GQNl_df, LQNl_df], axis='columns')\n",
    "    QNu_col = [i+\"'\" for i in list(QNu_df.columns)] \n",
    "    QNl_col = [i+'\"' for i in list(QNl_df.columns)] \n",
    "    return(QNu_df, QNl_df, QNu_col, QNl_col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitran_linelist_QN(hitran_df):\n",
    "    GlobalQNLabels,GlobalQNFormats = globalQNclasses(molecule,isotopologue)\n",
    "    LocalQNupperLabels, LocalQNlowerLabels, LocalQNupperFormats, LocalQNlowerFormats = localQNgroups(molecule,isotopologue)\n",
    "    QNu_df, QNl_df, QNu_col, QNl_col = separate_QN_hitran(hitran_df,GlobalQNLabels,LocalQNupperLabels,LocalQNlowerLabels,\n",
    "                                                          GlobalQNFormats,LocalQNupperFormats,LocalQNlowerFormats)\n",
    "    Ep_df = cal_Ep_hitran(hitran_df)\n",
    "    hitran_linelist_df = hitran_df[['v','S','A','gamma_air','gamma_self','Epp','n_air','delta_air','gp','gpp']]\n",
    "    hitran_linelist_df = pd.concat([hitran_linelist_df, Ep_df, QNu_df, QNl_df], axis='columns')\n",
    "    hitran_main_colname = ['v','S','A','gamma_air','gamma_self','E\"','n_air','delta_air',\"g'\",'g\"',\"E'\"]\n",
    "    QN_col = QNu_col + QNl_col\n",
    "    hitran_linelist_colname = hitran_main_colname + QN_col\n",
    "    hitran_linelist_df.columns = hitran_linelist_colname\n",
    "    # Do quantum number filter.\n",
    "    if QNsFilter !=[]:   \n",
    "        QNs_col = [i+\"'\" for i in QNs_label] + [i+'\"' for i in QNs_label]\n",
    "        if 'J' not in QNs_label:\n",
    "            hitran_linelist_df = hitran_linelist_df[hitran_main_colname + QNs_col + [\"J'\", 'J\"']]\n",
    "        else:\n",
    "            hitran_linelist_df = hitran_linelist_df[hitran_main_colname + QNs_col]\n",
    "        hitran_linelist_df = QNfilter_linelist(hitran_linelist_df, QNs_value, QNs_label)\n",
    "    else:\n",
    "        QNs_col = QN_col    \n",
    "    return(hitran_linelist_df, QNs_col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linelist_hitran(hitran_linelist_df):\n",
    "    '''\n",
    "    Read HITRAN .par file as the input file.\n",
    "    Return the data for calculating wavennumbers and cross sections with line profiles.\n",
    "    \n",
    "    '''\n",
    "    A = hitran_linelist_df['A'].values\n",
    "    Ep = hitran_linelist_df[\"E'\"].values\n",
    "    Epp = hitran_linelist_df['E\"'].values\n",
    "    n_air = hitran_linelist_df['n_air'].values\n",
    "    gamma_air = hitran_linelist_df['gamma_air'].values\n",
    "    gamma_self = hitran_linelist_df['gamma_self'].values\n",
    "    delta_air = hitran_linelist_df['delta_air'].values\n",
    "    gp = hitran_linelist_df[\"g'\"].values\n",
    "    v = hitran_linelist_df['v'].values\n",
    "    # if broad == 'Air':\n",
    "    #   v = hitran_df['v'].values + delta_air * (P - P_ref) / P\n",
    "    # else:\n",
    "    #   v = hitran_df['v'].values\n",
    "    return (A, v, Ep, Epp, gp, n_air, gamma_air, gamma_self, delta_air)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Parition Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_partition(En, gn, T):\n",
    "    partition_func = ne.evaluate('sum(gn * exp(-c2 * En / T))') \n",
    "    return(partition_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition function\n",
    "def exomol_partition(states_df, Ntemp, Tmax):\n",
    "    print('Calculate partition functions.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    En = states_df['E'].astype('float').values\n",
    "    gn = states_df['g'].astype('int').values\n",
    "    Ts = np.array(range(Ntemp, Tmax+1, Ntemp)) \n",
    "    partition_func = [calculate_partition(En, gn, T) for T in Ts]\n",
    "    t.end()\n",
    "    print('Finished calculating partition functions!\\n')\n",
    "\n",
    "    print('Saving partition functions into file ...')   \n",
    "    ts = Timer()    \n",
    "    ts.start()     \n",
    "    partition_func_df = pd.DataFrame()\n",
    "    partition_func_df['T'] = Ts\n",
    "    partition_func_df['partition function'] = partition_func\n",
    "    \n",
    "    pf_folder = save_path + 'partition/'\n",
    "    if os.path.exists(pf_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(pf_folder, exist_ok=True)\n",
    "    pf_path = pf_folder + isotopologue + '__' + dataset + '.pf'\n",
    "    np.savetxt(pf_path, partition_func_df, fmt=\"%8.1f %15.4f\")\n",
    "    ts.end()\n",
    "    print('Partition functions file has been saved:', pf_path, '\\n') \n",
    "    print('Partition functions have been saved!\\n')  \n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ntemp = 1\n",
    "# Tmax = int(5000.00)\n",
    "# states_df = read_all_states(read_path)\n",
    "# exomol_partition(states_df, Ntemp, Tmax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Heat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_specific_heats(En, gn, T):\n",
    "    pf = ne.evaluate('sum(gn * exp(-c2 * En / T)) ')  \n",
    "    pfp = ne.evaluate('sum(gn * exp(-c2 * En / T) * (c2 * En / T))')\n",
    "    pfpp = ne.evaluate('sum(gn * exp(-c2 * En / T) * (c2 * En / T) ** 2)')\n",
    "    specificheat_func = ne.evaluate('R * (pfpp / pf - (pfp / pf)**2) + 2.5 * R') \n",
    "    return(specificheat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific heat\n",
    "def exomol_specificheat(states_df, Ntemp, Tmax):\n",
    "    print('Calculate specific heats.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    En = states_df['E'].astype('float').values\n",
    "    gn = states_df['g'].astype('int').values\n",
    "    Ts = np.array(range(Ntemp, Tmax+1, Ntemp)) \n",
    "    specificheat_func = [calculate_specific_heats(En, gn, T) for T in Ts]\n",
    "    t.end()\n",
    "    print('Finished calculating specific heats!\\n')\n",
    "\n",
    "    print('Saving specific heats into file ...')   \n",
    "    ts = Timer()    \n",
    "    ts.start() \n",
    "    specificheat_func_df = pd.DataFrame()\n",
    "    specificheat_func_df['T'] = Ts\n",
    "    specificheat_func_df['specific heat'] = specificheat_func \n",
    "    \n",
    "    cp_folder = save_path + 'specific_heat/'\n",
    "    if os.path.exists(cp_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(cp_folder, exist_ok=True)  \n",
    "    cp_path = cp_folder + isotopologue + '__' + dataset + '.cp'\n",
    "    np.savetxt(cp_path, specificheat_func_df, fmt=\"%8.1f %15.4f\")\n",
    "    ts.end()\n",
    "    print('Specific heats file has been saved:', cp_path, '\\n')  \n",
    "    print('Specific heats have been saved!\\n')    \n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ntemp = 1\n",
    "# Tmax = int(10000.0)\n",
    "# states_df = read_all_states(read_path)\n",
    "# exomol_specificheat(states_df, Ntemp, Tmax)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lifetime\n",
    "def ProcessLifetime(states_df, trans_df):\n",
    "    A_sum = trans_df.groupby('u')['A'].sum()\n",
    "    ids = states_df['id']\n",
    "    lifetime_whole = np.zeros(max(ids)+1)\n",
    "    lifetime_whole[A_sum.index] = A_sum.values\n",
    "    lifetime = lifetime_whole[ids]\n",
    "    return lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lifetime(states_df, trans_filepath):\n",
    "    trans_filename = trans_filepath.split('/')[-1]\n",
    "    print('Processeing transitions file:', trans_filename)\n",
    "    if trans_filepath.split('.')[-1] == 'bz2':\n",
    "        trans_df_chunk_list = pd.read_csv(trans_filepath, compression='bz2', sep='\\s+', header=None,\n",
    "                                          usecols=[0,1,2],names=['u','l','A'], chunksize=chunk_size, \n",
    "                                          iterator=True, low_memory=False, encoding='utf-8')\n",
    "        # Process multiple files in parallel\n",
    "        with ThreadPoolExecutor(max_workers=ncputrans) as trans_executor:\n",
    "            futures = [trans_executor.submit(ProcessLifetime,states_df,trans_df_chunk) \n",
    "                       for trans_df_chunk in tqdm(trans_df_chunk_list, desc='Processing '+trans_filename)]\n",
    "            lifetime = sum(np.array([future.result() for future in tqdm(futures)]))\n",
    "    else:\n",
    "        trans_dd = dd.read_csv(trans_filepath, sep='\\s+', header=None, usecols=[0,1,2],names=['u','l','A'],encoding='utf-8')\n",
    "        trans_dd_list = trans_dd.partitions\n",
    "        # Process multiple files in parallel\n",
    "        with ThreadPoolExecutor(max_workers=ncputrans) as trans_executor:\n",
    "            futures = [trans_executor.submit(ProcessLifetime,states_df,trans_dd_list[i].compute(scheduler='threads')) \n",
    "                       for i in tqdm(range(len(list(trans_dd_list))), desc='Processing '+trans_filename)]\n",
    "            lifetime = sum(np.array([future.result() for future in tqdm(futures)]))\n",
    "    return lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lifetime\n",
    "def exomol_lifetime(read_path, states_df):\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    print('Calculate lifetimes.')  \n",
    "    print('Running on ', ncputrans, 'cores.')\n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    print('Reading all transitions and calculating lifetimes ...')\n",
    "    trans_filepaths = get_transfiles(read_path)\n",
    "    # Process multiple files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=ncpufiles) as executor:\n",
    "        # Submit reading tasks for each file\n",
    "        futures = [executor.submit(calculate_lifetime, states_df, \n",
    "                                   trans_filepath) for trans_filepath in tqdm(trans_filepaths)]\n",
    "        lifetime_result = 1 / sum(np.array([future.result() for future in futures]))\n",
    "        lifetime = np.array([f'{x:>12.4E}'.replace('INF','Inf') for x in lifetime_result])\n",
    "    t.end()\n",
    "    print('Finished reading all transitions and calculating lifetimes!\\n')\n",
    "\n",
    "    print('Saving lifetimes into file ...')   \n",
    "    ts = Timer()    \n",
    "    ts.start()  \n",
    "    states_filename = (read_path + molecule + '/' + isotopologue + '/' + dataset \n",
    "                        + '/' + isotopologue + '__' + dataset + '.states.bz2')\n",
    "    s_df = pd.read_csv(states_filename, compression='bz2', header=None, dtype=object)[0]\n",
    "    nrows = len(s_df)           \n",
    "    new_rows = []\n",
    "    if check_uncertainty == 0:\n",
    "        for i in range(nrows):\n",
    "            new_rows.append(s_df[i][:41]+lifetime[i]+s_df[i][53:]+'\\n')\n",
    "    if check_uncertainty == 1:\n",
    "        for i in range(nrows):\n",
    "            new_rows.append(s_df[i][:53]+lifetime[i]+s_df[i][65:]+'\\n')\n",
    "\n",
    "    lf_folder = save_path + 'lifetime/'\n",
    "    if os.path.exists(lf_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(lf_folder, exist_ok=True)  \n",
    "       \n",
    "    if CompressYN == 'Ym':\n",
    "        lf_path = lf_folder + isotopologue + '__' + dataset + '.states.bz2'\n",
    "        with bz2.open(lf_path, 'wt') as f:\n",
    "            for i in range(nrows):\n",
    "                f.write(new_rows[i])\n",
    "            f.close\n",
    "    else:\n",
    "        lf_path = lf_folder + isotopologue + '__' + dataset + '.states'\n",
    "        with open(lf_path, 'wt') as f:\n",
    "            for i in range(nrows):\n",
    "                f.write(new_rows[i])\n",
    "            f.close\n",
    "    ts.end()\n",
    "    print('Lifetimes file has been saved:', lf_path, '\\n') \n",
    "    print('Lifetimes have been saved!\\n')    \n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states_df = read_all_states(read_path)\n",
    "# exomol_lifetime(read_path, states_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_hitran2exomol_path = save_path + 'conversion/HITRAN2ExoMol/'\n",
    "# states_df = read_all_states(read_hitran2exomol_path)\n",
    "# exomol_lifetime(read_hitran2exomol_path, states_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cooling(A, v, Ep, gp, T, Q):\n",
    "    # cooling_func = np.sum(A * h * c * v * gp * np.exp(-c2 * Ep / T)) / (4 * PI * Q) \n",
    "    _sum = ne.evaluate('sum(A * hc * v * gp * exp(-c2 * Ep / T))')  \n",
    "    cooling_func = ne.evaluate('_sum / (4 * PI * Q)')\n",
    "    return(cooling_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cooling function\n",
    "def ProcessCoolingFunction(states_df,Ts,Qs,trans_df):\n",
    "    merged_df = dd.merge(trans_df, states_df, left_on='u', right_on='id', \n",
    "                         how='inner').merge(states_df, left_on='l', right_on='id', how='inner', suffixes=(\"'\", '\"'))\n",
    "    cooling_func_df = merged_df[['A',\"E'\",'E\"',\"g'\"]]\n",
    "    cooling_func_df['v'] = cal_v(cooling_func_df[\"E'\"].values, cooling_func_df['E\"'].values)\n",
    "    num = len(cooling_func_df)\n",
    "    if num > 0:\n",
    "        A = cooling_func_df['A'].values\n",
    "        v = cooling_func_df['v'].values\n",
    "        Ep = cooling_func_df[\"E'\"].values\n",
    "        gp = cooling_func_df[\"g'\"].values\n",
    "        cooling_func = [calculate_cooling(A, v, Ep, gp, Ts[i], Qs[i]) for i in tqdm(range(Tmax), desc='Calculating')]\n",
    "    else:\n",
    "        cooling_func = np.zeros(Tmax)\n",
    "    return cooling_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cooling_func(states_df, Ts, Qs, trans_filepath):\n",
    "    trans_filename = trans_filepath.split('/')[-1]\n",
    "    print('Processeing transitions file:', trans_filename)\n",
    "    if trans_filepath.split('.')[-1] == 'bz2':\n",
    "        trans_df_chunk_list = pd.read_csv(trans_filepath, compression='bz2', sep='\\s+', header=None,\n",
    "                                          usecols=[0,1,2],names=['u','l','A'], chunksize=chunk_size, \n",
    "                                          iterator=True, low_memory=False, encoding='utf-8')\n",
    "        # Process multiple files in parallel\n",
    "        with ThreadPoolExecutor(max_workers=ncputrans) as trans_executor:\n",
    "            futures = [trans_executor.submit(ProcessCoolingFunction,states_df,Ts,Qs,trans_df_chunk) \n",
    "                       for trans_df_chunk in tqdm(trans_df_chunk_list, desc='Processing '+trans_filename)]\n",
    "            cooling_func = sum(np.array([future.result() for future in tqdm(futures)]))\n",
    "    else:\n",
    "        trans_dd = dd.read_csv(trans_filepath, sep='\\s+', header=None, usecols=[0,1,2],names=['u','l','A'],encoding='utf-8')\n",
    "        trans_dd_list = trans_dd.partitions\n",
    "        # Process multiple files in parallel\n",
    "        with ThreadPoolExecutor(max_workers=ncputrans) as trans_executor:\n",
    "            futures = [trans_executor.submit(ProcessCoolingFunction,states_df,Ts,Qs,trans_dd_list[i].compute(scheduler='threads')) \n",
    "                       for i in tqdm(range(len(list(trans_dd_list))), desc='Processing '+trans_filename)]\n",
    "            cooling_func = sum(np.array([future.result() for future in tqdm(futures)]))\n",
    "    return cooling_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cooling function for ExoMol database\n",
    "def exomol_cooling(states_df, Ntemp, Tmax):\n",
    "    # tqdm.write('Calculate cooling functions.') \n",
    "    print('Calculate cooling functions.') \n",
    "    print('Running on ', ncputrans, 'cores.') \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    Ts = np.array(range(Ntemp, Tmax+1, Ntemp)) \n",
    "    Qs = read_exomol_pf(read_path, Ts)\n",
    "    print('Reading all transitions and calculating cooling functions ...')\n",
    "    trans_filepaths = get_transfiles(read_path)\n",
    "    # Process multiple files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=ncpufiles) as executor:\n",
    "        # Submit reading tasks for each file\n",
    "        futures = [executor.submit(calculate_cooling_func, states_df, Ts, Qs,\n",
    "                                   trans_filepath) for trans_filepath in tqdm(trans_filepaths)]\n",
    "        cooling_func = sum(np.array([future.result() for future in futures]))\n",
    "    \n",
    "    cooling_func_df = pd.DataFrame()\n",
    "    cooling_func_df['T'] = Ts\n",
    "    cooling_func_df['cooling function'] = cooling_func\n",
    "    t.end()\n",
    "    print('Finished reading all transitions and calculating cooling functions!\\n')\n",
    "    \n",
    "    print('Saving cooling functions into file ...')   \n",
    "    ts = Timer()    \n",
    "    ts.start()     \n",
    "    cf_folder = save_path + 'cooling/'\n",
    "    if os.path.exists(cf_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(cf_folder, exist_ok=True)  \n",
    "    cf_path = cf_folder + isotopologue + '__' + dataset + '.cf' \n",
    "    np.savetxt(cf_path, cooling_func_df, fmt=\"%8.1f %20.8E\")\n",
    "    ts.end()\n",
    "    print('Cooling functions file has been saved:', cf_path, '\\n')  \n",
    "    print('Cooling functions have been saved!\\n')  \n",
    "    # tqdm.write('Cooling functions has been saved!\\n') \n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cooling function for HITRAN database\n",
    "def hitran_cooling(hitran_df, Ntemp, Tmax):\n",
    "    print('Calculate cooling functions.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    Ep = cal_Ep(hitran_df['Epp'].values,hitran_df['v'].values)\n",
    "    A = hitran_df['A'].values\n",
    "    v = hitran_df['v'].values\n",
    "    gp = hitran_df['gp'].values\n",
    "    Ts = np.array(range(Ntemp, Tmax+1, Ntemp)) \n",
    "    Qs = read_hitran_pf(Ts)\n",
    "    cooling_func = [calculate_cooling(A, v, Ep, gp, Ts[i], Qs[i]) for i in tqdm(range(Tmax), desc='Calculating')]\n",
    "    t.end()\n",
    "     \n",
    "    cooling_func_df = pd.DataFrame()\n",
    "    cooling_func_df['T'] = Ts\n",
    "    cooling_func_df['cooling function'] = cooling_func\n",
    "\n",
    "    cf_folder = save_path + '/cooling/'\n",
    "    if os.path.exists(cf_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(cf_folder, exist_ok=True)  \n",
    "    cf_path = cf_folder + isotopologue + '__' + dataset + '.cf' \n",
    "    np.savetxt(cf_path, cooling_func_df, fmt=\"%8.1f %20.8E\")\n",
    "    print('Cooling functions file has been saved:', cf_path, '\\n')\n",
    "    print('Cooling functions have been saved!\\n')     \n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ntemp = 1\n",
    "# Tmax = int(5000.00)\n",
    "# states_df = read_all_states(read_path)\n",
    "# exomol_cooling(states_df, Ntemp, Tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ntemp = 1\n",
    "# Tmax = int(1500.00)\n",
    "# parfile_df = read_parfile(read_path)\n",
    "# hitran_df = read_hitran_parfile(read_path,parfile_df,min_wn,max_wn,'None','None').reset_index().drop(columns='index')\n",
    "# hitran_cooling(hitran_df, Ntemp, Tmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oscillator Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gf or f\n",
    "def calculate_oscillator_strength(gp, gpp, A, v):\n",
    "    gf = ne.evaluate('gp * A / (c * v)**2')\n",
    "    f = ne.evaluate('gf / gpp')\n",
    "    if 'G' not in gfORf:\n",
    "        oscillator_strength = f\n",
    "    else:\n",
    "        oscillator_strength = gf\n",
    "    return oscillator_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate oscillator strength\n",
    "def ProcessOscillatorStrengths(states_df,trans_df):\n",
    "    merged_df = dd.merge(trans_df, states_df, left_on='u', right_on='id', \n",
    "                         how='inner').merge(states_df, left_on='l', right_on='id', how='inner', suffixes=(\"'\", '\"'))\n",
    "    oscillator_strength_df = merged_df[['u','l','A',\"E'\",'E\"',\"g'\",'g\"']]\n",
    "    v = cal_v(oscillator_strength_df[\"E'\"].values, oscillator_strength_df['E\"'].values)\n",
    "    A = oscillator_strength_df['A'].values\n",
    "    gp = oscillator_strength_df[\"g'\"].values\n",
    "    gpp = oscillator_strength_df['g\"'].values\n",
    "    oscillator_strength_df['os'] = calculate_oscillator_strength(gp, gpp, A, v)    \n",
    "    oscillator_strength_df['v'] = v\n",
    "    oscillator_strength_df = oscillator_strength_df[['u','l','os','v']]\n",
    "    return (oscillator_strength_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_oscillator_strengths(states_df, trans_filepath):\n",
    "    trans_filename = trans_filepath.split('/')[-1]\n",
    "    print('Processeing transitions file:', trans_filename)\n",
    "    if trans_filepath.split('.')[-1] == 'bz2':\n",
    "        trans_df_chunk_list = pd.read_csv(trans_filepath, compression='bz2', sep='\\s+', header=None,\n",
    "                                          usecols=[0,1,2],names=['u','l','A'], chunksize=chunk_size, \n",
    "                                          iterator=True, low_memory=False, encoding='utf-8')\n",
    "        # Process multiple files in parallel\n",
    "        with ThreadPoolExecutor(max_workers=ncputrans) as trans_executor:\n",
    "            futures = [trans_executor.submit(ProcessOscillatorStrengths,states_df,trans_df_chunk) \n",
    "                       for trans_df_chunk in tqdm(trans_df_chunk_list, desc='Processing '+trans_filename)]\n",
    "            oscillator_strength_df = pd.concat([future.result() for future in tqdm(futures)])\n",
    "    else:\n",
    "        trans_dd = dd.read_csv(trans_filepath, sep='\\s+', header=None, usecols=[0,1,2],names=['u','l','A'],encoding='utf-8')\n",
    "        trans_dd_list = trans_dd.partitions\n",
    "        # Process multiple files in parallel\n",
    "        with ThreadPoolExecutor(max_workers=ncputrans) as trans_executor:\n",
    "            futures = [trans_executor.submit(ProcessOscillatorStrengths,states_df,trans_dd_list[i].compute(scheduler='threads')) \n",
    "                       for i in tqdm(range(len(list(trans_dd_list))), desc='Processing '+trans_filename)]\n",
    "            oscillator_strength_df = pd.concat([future.result() for future in tqdm(futures)])\n",
    "    return oscillator_strength_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot oscillator strength\n",
    "def plot_oscillator_strength(oscillator_strength_df):\n",
    "    v = oscillator_strength_df['v']\n",
    "    osc_str = oscillator_strength_df['os']\n",
    "    parameters = {'axes.labelsize': 18, \n",
    "                  'legend.fontsize': 18,\n",
    "                  'xtick.labelsize': 14,\n",
    "                  'ytick.labelsize': 14}\n",
    "    plt.rcParams.update(parameters)\n",
    "    os_plot_folder = save_path + 'oscillator_strength/plots/'+molecule+'/'+database+'/'\n",
    "    if os.path.exists(os_plot_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(os_plot_folder, exist_ok=True)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.ylim([limitYaxisOS, 10*max(osc_str)])\n",
    "    plt.plot(v, osc_str, label=molecule, linewidth=0.4)\n",
    "    plt.semilogy()\n",
    "    #plt.title(database+' '+molecule+' oscillator strengths') \n",
    "    plt.xlabel('Wavenumber, cm$^{-1}$')\n",
    "    plt.ylabel('Oscillator strength, ('+gfORf.lower()+')')\n",
    "    plt.legend()\n",
    "    leg = plt.legend()                  # Get the legend object.\n",
    "    for line in leg.get_lines():\n",
    "        line.set_linewidth(1.0)         # Change the line width for the legend.\n",
    "    os_plot = os_plot_folder+molecule+'__'+isotopologue+'__'+database+'__'+gfORf.lower()+'__os.png'\n",
    "    plt.savefig(os_plot, dpi=500)\n",
    "    plt.show()\n",
    "    print('Oscillator strengths plot has been saved:', os_plot, '\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oscillator strength for ExoMol database\n",
    "def exomol_oscillator_strength(states_df):\n",
    "    print('Calculate oscillator strengths.')  \n",
    "    print('Running on ', ncputrans, 'cores.')\n",
    "    tot = Timer()\n",
    "    tot.start()\n",
    "    print('Reading transitions and calculating oscillator strengths ...')    \n",
    "    trans_filepaths = get_transfiles(read_path)\n",
    "    # Process multiple files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=ncpufiles) as executor:\n",
    "        # Submit reading tasks for each file\n",
    "        futures = [executor.submit(calculate_oscillator_strengths,states_df,\n",
    "                                   trans_filepath) for trans_filepath in tqdm(trans_filepaths)]\n",
    "        oscillator_strength_df = pd.concat([future.result() for future in futures])\n",
    "    oscillator_strength_df.sort_values(by=['v'], ascending=True, inplace=True)  \n",
    "    # oscillator_strength_df  = oscillator_strength_df.sort_values('v').reset_index(drop=True)\n",
    "    tot.end()\n",
    "    print('Finished reading all transitions and calculating oscillator strengths!\\n')\n",
    "\n",
    "    print('Saving oscillator strengths into file ...')   \n",
    "    ts = Timer()    \n",
    "    ts.start()\n",
    "    os_folder = save_path + 'oscillator_strength/files/'+molecule+'/'+database+'/'\n",
    "    if os.path.exists(os_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(os_folder, exist_ok=True)  \n",
    "    os_path = os_folder+isotopologue+'__'+dataset+'_'+gfORf.lower()+'.os' \n",
    "    os_format = \"%12d %12d %10.4E %15.6f\"\n",
    "    np.savetxt(os_path, oscillator_strength_df, fmt=os_format)\n",
    "    ts.end()\n",
    "    print('Oscillator strengths file has been saved:', os_path, '\\n')  \n",
    "    \n",
    "    # Plot oscillator strengths and save it as .png.\n",
    "    if PlotOscillatorStrengthYN == 'Y':\n",
    "        plot_oscillator_strength(oscillator_strength_df)\n",
    "    print('Oscillator strengths have been saved!\\n')  \n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oscillator strength for HITRAN database\n",
    "def hitran_oscillator_strength(hitran_df):\n",
    "    print('Calculate oscillator strengths.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    A = hitran_df['A'].values\n",
    "    v = hitran_df['v'].values\n",
    "    gp = hitran_df['gp'].values\n",
    "    gpp = hitran_df['gp'].values\n",
    "    oscillator_strength_df = hitran_df[['gp', 'gpp', 'v']]\n",
    "    oscillator_strength_df['os'] = calculate_oscillator_strength(gp, gpp, A, v)\n",
    "    oscillator_strength_df = oscillator_strength_df.sort_values(by=['v'])\n",
    "    t.end()\n",
    "    \n",
    "    os_folder = save_path + '/oscillator_strength/files/'\n",
    "    if os.path.exists(os_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(os_folder, exist_ok=True)  \n",
    "    os_path = os_folder+isotopologue+'__'+dataset+'_'+gfORf.lower()+'.os' \n",
    "    os_format = \"%7.1f %7.1f %10.4E %15.6f\"\n",
    "    np.savetxt(os_path, oscillator_strength_df, fmt=os_format)\n",
    "    print('Oscillator strengths file has been saved:', os_path, '\\n')  \n",
    "    \n",
    "    # Plot oscillator strengths and save it as .png.\n",
    "    if PlotOscillatorStrengthYN == 'Y':\n",
    "        plot_oscillator_strength(oscillator_strength_df)  \n",
    "    print('Oscillator strengths have been saved!\\n') \n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states_df = read_all_states(read_path)\n",
    "# exomol_oscillator_strength(states_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parfile_df = read_parfile(read_path)\n",
    "# hitran_df = read_hitran_parfile(read_path,parfile_df,min_wn,max_wn,'None','None').reset_index().drop(columns='index')\n",
    "# hitran_oscillator_strength(hitran_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_part_states(states_df):\n",
    "    if UncFilter != 'None' :\n",
    "        states_part_df = states_df[states_df['unc'] <= UncFilter]\n",
    "        # states_part_df['id'] = pd.to_numeric(states_part_df['id'])\n",
    "        states_part_df.set_index(['id'], inplace=True, drop=False)\n",
    "    else:\n",
    "        states_part_df = states_df\n",
    "        # states_part_df['id'] = pd.to_numeric(states_part_df['id'])\n",
    "        states_part_df.set_index(['id'], inplace=True, drop=False)\n",
    "    if check_uncertainty == 1:\n",
    "        col_unc = ['unc']\n",
    "    else:\n",
    "        col_unc = []\n",
    "    if check_lifetime == 1:\n",
    "        col_lifetime = ['tau']\n",
    "    else:\n",
    "        col_lifetime = []\n",
    "    if check_gfactor == 1:\n",
    "        col_gfac = ['gfac']\n",
    "    else:\n",
    "        col_gfac = []\n",
    "    colname = ['id','E','g','J'] + col_unc + col_lifetime + col_gfac + QNslabel_list\n",
    "    states_part_df.drop(states_part_df.columns[len(colname):], axis=1, inplace=True)\n",
    "    states_part_df.columns = colname\n",
    "    QNcolumns = ['id','E','g','J'] + col_unc + col_lifetime + col_gfac + QNs_label\n",
    "    states_part_df = states_part_df[QNcolumns]\n",
    "    if QNsFilter !=[]:    \n",
    "        for i in range(len(QNs_label)):\n",
    "            if QNs_value[i] != ['']:\n",
    "                list_QN_value = str(QNs_value[i]).replace(\"', '\",\",\").replace(\"'\",\"\").replace('[','').replace(']','').split(',')\n",
    "                if '' in list_QN_value:\n",
    "                    states_part_df = states_part_df\n",
    "                else:\n",
    "                    states_part_df = states_part_df[states_part_df[QNs_label[i]].isin(list_QN_value)]\n",
    "    states_part_df.index.name='index'\n",
    "    return(states_part_df)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_transfiles(read_path):\n",
    "    # Get all the transitions files from the folder including the older version files which are named by vn(version number).\n",
    "    trans_filepaths_all = glob.glob(read_path + molecule + '/' + isotopologue + '/' + dataset + '/' + '*trans.bz2')\n",
    "    num_transfiles_all = len(trans_filepaths_all)    # The number of all transitions files including the older version files.\n",
    "    trans_filepaths = []    # The list of the lastest transitions files.\n",
    "    all_decompress_num = 0\n",
    "    decompress_num = 0\n",
    "    for i in range(num_transfiles_all):\n",
    "        split_version = trans_filepaths_all[i].split('__')[-1].split('.')[0].split('_')    # Split the filenames.\n",
    "        num = len(split_version)\n",
    "        # There are four format filenames.\n",
    "        # The lastest transitions files named in four formats:\n",
    "        # 1. Filenames are named with the name of isotopologue and dataset. \n",
    "        #    End with .trans.bz2.\n",
    "        #    e.g. 14N-16O__XABC.trans.bz2'\n",
    "        # 2. Filenames are named with the name of isotopologue and dataset. \n",
    "        #    Also have the range of wavenumbers xxxxx-yyyyy.\n",
    "        #    End with .trans.bz2.\n",
    "        #    e.g. 1H2-16O__POKAZATEL__00000-00100.trans.bz2\n",
    "        # 3. The older version transitions files are named with vn(version number) based on the first format of the lastest files.\n",
    "        #    e.g. 14N-16O__XABC_v2.trans.bz2\n",
    "        # 4. The older version transitions files are named with updated date (yyyymmdd).\n",
    "        #    e.g. 1H3_p__MiZATeP__20170330.trans.bz2\n",
    "        # After split the filenames:\n",
    "        # The first format filenames only leave the dataset name, e.g. XABC.\n",
    "        # The second format filenames only leave the range of the wavenumber, e.g. 00000-00100.\n",
    "        # The third format filenames leave two parts(dataset name and version number), e.g. XABC and v2.\n",
    "        # The fourth format filenames only leave the updated date, e.g. 20170330.\n",
    "        # This program only process the lastest data, so extract the filenames named by the first two formats.\n",
    "        if num == 1:     \n",
    "            if split_version[0] == dataset:        \n",
    "                trans_filepaths.append(trans_filepaths_all[i])\n",
    "            elif len(split_version[0].split('-')) == 2:\n",
    "                lower = int(split_version[0].split('-')[0])\n",
    "                upper = int(split_version[0].split('-')[1])\n",
    "                if ((lower <= int(min_wn) < upper) or \n",
    "                    (lower >= int(min_wn) and upper <= int(max_wn)) or \n",
    "                    (lower <= int(max_wn) < upper)):\n",
    "                    file_size_bytes = os.path.getsize(trans_filepaths_all[i])\n",
    "                    if file_size_bytes/1024**3 > 2:  \n",
    "                        (trans_filepath, num) = command_decompress(trans_filepaths_all[i])\n",
    "                        all_decompress_num += 1\n",
    "                        decompress_num += num\n",
    "                    else:\n",
    "                        trans_filepath = trans_filepaths_all[i]\n",
    "                    trans_filepaths.append(trans_filepath)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    print('Number of all transitions files \\t\\t:', num_transfiles_all)\n",
    "    print('Number of selected transitions files \\t\\t:', len(trans_filepaths))\n",
    "    print('Number of all decompressed transitions files \\t:', all_decompress_num)\n",
    "    print('Number of new decompressed transitions files \\t:', decompress_num)\n",
    "    return(trans_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_broad(broad_df, st_df):\n",
    "    max_broad_J = max(broad_df['Jpp'])\n",
    "    Jpp = st_df['J\"'].values\n",
    "    Jpp[Jpp > max_broad_J] = max_broad_J\n",
    "    id_broad = (st_df['J\"']-0.1).round(0).astype('int32').values\n",
    "    gamma_L = broad_df['gamma_L'][id_broad]\n",
    "    n_air = broad_df['n_air'][id_broad]\n",
    "    return(gamma_L, n_air)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absorption coefficient\n",
    "def cal_abscoefs(T, v, gp, A, Epp, Q, abundance):\n",
    "    # abscoef = gp * A * np.exp(- c2 * Epp / T) * (1 - np.exp(- c2 * v / T)) / (8 * np.pi * c * v**2 * Q) * abundance  \n",
    "    abscoef = ne.evaluate('gp * A * exp(- c2 * Epp / T) * (1 - exp(- c2 * v / T)) * Inv8Pic / (v ** 2 * Q) * abundance')  \n",
    "    return abscoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate emission coefficient\n",
    "def cal_emicoefs(T, v, gp, A, Ep, Q, abundance):\n",
    "    # emicoef = gp * h * c A * v * np.exp(- c2 * Ep / T) / (4 * np.pi) / Q * abundance   \n",
    "    emicoef = ne.evaluate('gp * hc * A * v * exp(- c2 * Ep / T) * Inv4Pi / Q * abundance')\n",
    "    return emicoef"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate uncertainty\n",
    "def cal_uncertainty(unc_u, unc_l):\n",
    "    unc = ne.evaluate('sqrt(unc_u ** 2 + unc_l ** 2)')\n",
    "    return unc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion\n",
    "\n",
    "## ExoMol to HITRAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_QNValues_exomol2hitran(states_unc_df, GlobalQNLabel_list, LocalQNLabel_list):\n",
    "    QNLabel_list = GlobalQNLabel_list+LocalQNLabel_list\n",
    "    if 'Gtot' in QNLabel_list:\n",
    "        states_unc_df[\"Gtot\"] = (states_unc_df[\"Gtot\"].replace('1',\"A1'\").replace('2',\"A2'\").replace('3',\"E'\")\n",
    "                                 .replace('4','A1\"').replace('5','A2\"').replace('6','E\"'))\n",
    "    if 'Gvib' in QNLabel_list:\n",
    "        states_unc_df[\"Gvib\"] = (states_unc_df[\"Gvib\"].replace('1',\"A1'\").replace('2',\"A2'\").replace('3',\"E'\")\n",
    "                                 .replace('4','A1\"').replace('5','A2\"').replace('6','E\"'))   \n",
    "    if 'Grot' in QNLabel_list:\n",
    "        states_unc_df[\"Grot\"] = (states_unc_df[\"Grot\"].replace('1',\"A1'\").replace('2',\"A2'\").replace('3',\"E'\")\n",
    "                                 .replace('4','A1\"').replace('5','A2\"').replace('6','E\"'))                   \n",
    "    if 'taui' in QNLabel_list:\n",
    "        states_unc_df[\"taui\"] = states_unc_df[\"taui\"].replace('0','s').replace('1','a')\n",
    "    return(states_unc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_unc_states(states_df):\n",
    "    if Conversion != 0:\n",
    "        if ConversionUnc != 'None':\n",
    "            states_unc_df = states_df[states_df['unc'].astype(float) <= ConversionUnc]\n",
    "        else:\n",
    "            states_unc_df = states_df\n",
    "        states_unc_df['id'] = pd.to_numeric(states_unc_df['id'])\n",
    "        states_unc_df.set_index(['id'], inplace=True, drop=False)\n",
    "    else:\n",
    "        states_unc_df = states_df\n",
    "        states_unc_df['id'] = pd.to_numeric(states_unc_df['id'])\n",
    "        states_unc_df.set_index(['id'], inplace=True, drop=False)\n",
    "    if check_uncertainty == 1:\n",
    "        col_unc = ['unc']\n",
    "    else:\n",
    "        col_unc = []\n",
    "    if check_lifetime == 1:\n",
    "        col_lifetime = ['tau']\n",
    "    else:\n",
    "        col_lifetime = []\n",
    "    if check_gfactor == 1:\n",
    "        col_gfac = ['gfac']\n",
    "    else:\n",
    "        col_gfac = []\n",
    "    fullcolname = ['id','E','g','J'] + col_unc + col_lifetime + col_gfac + QNslabel_list\n",
    "    states_unc_df = states_unc_df.iloc[:, : len(fullcolname)]\n",
    "    states_unc_df.columns = fullcolname  \n",
    "    colnames = ['id','E','g'] + col_unc + GlobalQNLabel_list + LocalQNLabel_list\n",
    "    states_unc_df = states_unc_df[colnames] \n",
    "    states_unc_df = convert_QNValues_exomol2hitran(states_unc_df, GlobalQNLabel_list, LocalQNLabel_list)\n",
    "    states_unc_df.index.name='index'\n",
    "    # pd.set_option(\"display.max_columns\",30) \n",
    "    return(states_unc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linelist_ExoMol2HITRAN(states_unc_df,trans_part_df):\n",
    "    merged_df = dd.merge(trans_part_df, states_unc_df, left_on='u', right_on='id', \n",
    "                         how='inner').merge(states_unc_df, left_on='l', right_on='id', how='inner', suffixes=(\"'\", '\"'))\n",
    "\n",
    "    merged_df['v'] = cal_v(merged_df[\"E'\"].values, merged_df['E\"'].values)\n",
    "    merged_df = merged_df[merged_df['v'].between(min_wn, max_wn)]\n",
    "    v = merged_df['v']\n",
    "    A = merged_df['A'].values\n",
    "    Epp = merged_df['E\"'].values\n",
    "    gp = merged_df[\"g'\"].values\n",
    "    gpp = merged_df['g\"'].values\n",
    "    exomolst_df = merged_df.drop(columns=[\"id'\", 'id\"','u','l','A',\"E'\",'E\"','v',\"g'\",'g\"'])\n",
    "    unc = cal_uncertainty(exomolst_df[\"unc'\"], exomolst_df['unc\"'])\n",
    "    exomolst_df.drop(columns=[\"unc'\",'unc\"'], inplace=True)\n",
    "    return (exomolst_df, v, A, Epp, gp, gpp, unc)\n",
    "\n",
    "def broadener_ExoMol2HITRAN(exomolst_df):\n",
    "    broad_col_name = ['code', 'gamma_L', 'n_air', 'Jpp']\n",
    "    default_broad_df = pd.DataFrame(columns=broad_col_name)\n",
    "    default_gamma_L = 0.07\n",
    "    default_n_air = 0.5\n",
    "    default_broad_df = pd.DataFrame([['code', default_gamma_L, default_n_air,'Jpp']],columns=broad_col_name)\n",
    "    air_broad_df = pd.DataFrame(columns=broad_col_name)\n",
    "    rows = len(exomolst_df)\n",
    "    pattern_air = read_path + molecule + '/**/*air.broad'\n",
    "    if glob.glob(pattern_air, recursive=True) != []:\n",
    "        for fname_air in glob.glob(pattern_air, recursive=True):\n",
    "            air_broad_df = pd.read_csv(fname_air, sep='\\s+', names=broad_col_name, header=None, engine='python')\n",
    "            gamma_air = extract_broad(air_broad_df,exomolst_df)[0].values\n",
    "            n_air = extract_broad(air_broad_df,exomolst_df)[1].values\n",
    "    else:\n",
    "        gamma_air= np.full((1,rows),default_broad_df['gamma_L'][0])[0]\n",
    "        n_air = np.full((1,rows),default_broad_df['n_air'][0])[0]\n",
    "    pattern_self = read_path + molecule + '/**/*self.broad'\n",
    "    if glob.glob(pattern_self, recursive=True) != []:\n",
    "        for fname_self in glob.glob(pattern_self, recursive=True):\n",
    "            self_broad_df = pd.read_csv(fname_self, sep='\\s+', names=broad_col_name, header=None, engine='python')\n",
    "            gamma_self = extract_broad(self_broad_df,exomolst_df)[0].values\n",
    "    else:\n",
    "        gamma_self= np.full((1,rows),default_broad_df['gamma_L'][0])[0]  \n",
    "    return (gamma_air, gamma_self, n_air)\n",
    "        \n",
    "def convert_QNFormat_exomol2hitran(exomolst_df, GlobalQNLabel_list, GlobalQNFormat_list, \n",
    "                                   LocalQNLabel_list, LocalQNFormat_list):\n",
    "    # from pandarallel import pandarallel\n",
    "    # pandarallel.initialize(nb_workers=32,progress_bar=False)    # Initialize.\n",
    "\n",
    "    n_gQN = len(GlobalQNLabel_list)\n",
    "    for i in range(n_gQN):\n",
    "        gQN_format = GlobalQNFormat_list[i].replace(\"%\",'{: >')+'}'\n",
    "        gQN_label = GlobalQNLabel_list[i]\n",
    "        try:\n",
    "            if 'd' in gQN_format or 'f' in gQN_format: \n",
    "                exomolst_df[gQN_label+\"'\"] = pd.Series(pd.to_numeric(exomolst_df[gQN_label+\"'\"].values)).parallel_map(gQN_format.format)\n",
    "                exomolst_df[gQN_label+'\"'] = pd.Series(pd.to_numeric(exomolst_df[gQN_label+'\"'].values)).parallel_map(gQN_format.format)\n",
    "            elif 's' in gQN_format or 'a' in gQN_format: \n",
    "                exomolst_df[gQN_label+\"'\"] = pd.Series(exomolst_df[gQN_label+\"'\"].str.replace('(','').str.replace(')','')).parallel_map(gQN_format.format)\n",
    "                exomolst_df[gQN_label+'\"'] = pd.Series(exomolst_df[gQN_label+'\"'].str.replace('(','').str.replace(')','')).parallel_map(gQN_format.format)\n",
    "        except:\n",
    "            if 'd' in gQN_format or 'f' in gQN_format: \n",
    "                exomolst_df[gQN_label+\"'\"] = pd.Series(pd.to_numeric(exomolst_df[gQN_label+\"'\"].values)).map(gQN_format.format)\n",
    "                exomolst_df[gQN_label+'\"'] = pd.Series(pd.to_numeric(exomolst_df[gQN_label+'\"'].values)).map(gQN_format.format)\n",
    "            elif 's' in gQN_format or 'a' in gQN_format: \n",
    "                exomolst_df[gQN_label+\"'\"] = pd.Series(exomolst_df[gQN_label+\"'\"].str.replace('(','').str.replace(')','')).map(gQN_format.format)\n",
    "                exomolst_df[gQN_label+'\"'] = pd.Series(exomolst_df[gQN_label+'\"'].str.replace('(','').str.replace(')','')).map(gQN_format.format)\n",
    "    try:\n",
    "        globalQNp = exomolst_df[[GlobalQNLabel_list[i]+\"'\" for i in range(n_gQN)]].sum(axis=1).parallel_map('{: >15}'.format)  \n",
    "        globalQNpp = exomolst_df[[GlobalQNLabel_list[i]+'\"' for i in range(n_gQN)]].sum(axis=1).parallel_map('{: >15}'.format)            \n",
    "    except:\n",
    "        globalQNp = exomolst_df[[GlobalQNLabel_list[i]+\"'\" for i in range(n_gQN)]].sum(axis=1).map('{: >15}'.format)  \n",
    "        globalQNpp = exomolst_df[[GlobalQNLabel_list[i]+'\"' for i in range(n_gQN)]].sum(axis=1).map('{: >15}'.format)     \n",
    "                \n",
    "    n_lQN = len(LocalQNLabel_list)\n",
    "    for i in range(n_lQN):\n",
    "        lQN_format = LocalQNFormat_list[i].replace(\"%\",'{: >')+'}'\n",
    "        lQN_label = LocalQNLabel_list[i]\n",
    "        try:\n",
    "            if 'd' in lQN_format or 'f' in lQN_format: \n",
    "                exomolst_df[lQN_label+\"'\"] = pd.Series(pd.to_numeric(exomolst_df[lQN_label+\"'\"].values)).parallel_map(lQN_format.format)\n",
    "                exomolst_df[lQN_label+'\"'] = pd.Series(pd.to_numeric(exomolst_df[lQN_label+'\"'].values)).parallel_map(lQN_format.format)\n",
    "            elif 's' in lQN_format or 'a' in lQN_format: \n",
    "                exomolst_df[lQN_label+\"'\"] = pd.Series(exomolst_df[lQN_label+\"'\"].str.replace('(','').str.replace(')','')).parallel_map(lQN_format.format)\n",
    "                exomolst_df[lQN_label+'\"'] = pd.Series(exomolst_df[lQN_label+'\"'].str.replace('(','').str.replace(')','')).parallel_map(lQN_format.format)\n",
    "        except:\n",
    "            if 'd' in lQN_format or 'f' in lQN_format: \n",
    "                exomolst_df[lQN_label+\"'\"] = pd.Series(pd.to_numeric(exomolst_df[lQN_label+\"'\"].values)).map(lQN_format.format)\n",
    "                exomolst_df[lQN_label+'\"'] = pd.Series(pd.to_numeric(exomolst_df[lQN_label+'\"'].values)).map(lQN_format.format)\n",
    "            elif 's' in lQN_format or 'a' in gQN_format: \n",
    "                exomolst_df[lQN_label+\"'\"] = pd.Series(exomolst_df[lQN_label+\"'\"].str.replace('(','').str.replace(')','')).map(lQN_format.format)\n",
    "                exomolst_df[lQN_label+'\"'] = pd.Series(exomolst_df[lQN_label+'\"'].str.replace('(','').str.replace(')','')).map(lQN_format.format)\n",
    "    try:\n",
    "        localQNp = exomolst_df[[LocalQNLabel_list[i]+\"'\" for i in range(n_lQN)]].sum(axis=1).parallel_map('{: >15}'.format)  \n",
    "        localQNpp = exomolst_df[[LocalQNLabel_list[i]+'\"' for i in range(n_lQN)]].sum(axis=1).parallel_map('{: >15}'.format)            \n",
    "    except:\n",
    "        localQNp = exomolst_df[[LocalQNLabel_list[i]+\"'\" for i in range(n_lQN)]].sum(axis=1).map('{: >15}'.format)  \n",
    "        localQNpp = exomolst_df[[LocalQNLabel_list[i]+'\"' for i in range(n_lQN)]].sum(axis=1).map('{: >15}'.format)    \n",
    "    QN_df = pd.concat([globalQNp,globalQNpp,localQNp,localQNpp],axis='columns')\n",
    "    QN_df.columns = [\"V'\", 'V\"', \"Q'\", 'Q\"'] \n",
    "    return QN_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_code(unc):\n",
    "    unc[(1<=unc)] = '000000'\n",
    "    unc[(0.1<=unc) & (unc<1)] = '100000'\n",
    "    unc[(0.01<=unc) & (unc<0.1)] = '200000'\n",
    "    unc[(0.001<=unc) & (unc<0.01)] = '300000'\n",
    "    unc[(0.0001<=unc) & (unc<0.001)] = '400000'\n",
    "    unc[(0.00001<=unc) & (unc<0.0001)] = '500000'\n",
    "    unc[(0.000001<=unc) & (unc<0.00001)] = '600000'\n",
    "    unc[(0.0000001<=unc) & (unc<0.000001)] = '700000'\n",
    "    unc[(0.00000001<=unc) & (unc<0.0000001)] = '800000'\n",
    "    unc[(unc<0.00000001)] = '900000'\n",
    "    unc = unc.astype(int)\n",
    "    return(unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertExoMol2HITRAN(states_df, trans_part_df):\n",
    "    states_unc_df = read_unc_states(states_df)\n",
    "    (exomolst_df, v, A, Epp, gp, gpp, unc_values) = linelist_ExoMol2HITRAN(states_unc_df,trans_part_df)\n",
    "    (gamma_air, gamma_self, n_air) = broadener_ExoMol2HITRAN(exomolst_df)\n",
    "    QN_df =  convert_QNFormat_exomol2hitran(exomolst_df, GlobalQNLabel_list, GlobalQNFormat_list, \n",
    "                                            LocalQNLabel_list, LocalQNFormat_list)\n",
    "    Q = read_exomol_pf(read_path, Tref)\n",
    "    I = cal_abscoefs(Tref, v, gp, A, Epp, Q, abundance)\n",
    "    unc = error_code(unc_values)\n",
    "    nrows = len(A)\n",
    "    delta_air = ['']*nrows \n",
    "    iref = ['']*nrows \n",
    "    flag = ['']*nrows \n",
    "    '''\n",
    "    hitran_column_name = ['M','I','v','S','A','gamma_air','gamma_self',\n",
    "                          'E\"','n_air','delta_air','Vp','Vpp','Qp','Qpp',\n",
    "                          'Ierr','Iref','flag','gp','gpp']\n",
    "    '''\n",
    "    hitran_begin_dic = {'M':molecule_id, 'I':isotopologue_id, 'v':v, 'S':I, 'A':A, \n",
    "                        'gamma_air':gamma_air,'gamma_self':gamma_self,'E\"':Epp,'n_air':n_air,'delta_air':delta_air}\n",
    "    hitran_begin_df = pd.DataFrame(hitran_begin_dic)\n",
    "    hitran_end_dic = {'Error':unc,'Iref':iref,'*':flag,\"g'\":gp, 'g\"':gpp}\n",
    "    hitran_end_df = pd.DataFrame(hitran_end_dic)\n",
    "\n",
    "    hitran_res_df = pd.concat([hitran_begin_df, QN_df, hitran_end_df], axis='columns')\n",
    "    if ConversionThreshold != 'None':\n",
    "        hitran_res_df = hitran_res_df[hitran_res_df['S'] >= ConversionThreshold]\n",
    "    # hitran_res_df = hitran_res_df.sort_values('v')\n",
    "    return(hitran_res_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessExoMol2HITRAN(states_df, trans_filepath):\n",
    "    trans_filename = trans_filepath.split('/')[-1]\n",
    "    print('Processeing transitions file:', trans_filename)\n",
    "    if trans_filepath.split('.')[-1] == 'bz2':\n",
    "        trans_df_chunk_list = pd.read_csv(trans_filepath, compression='bz2', sep='\\s+', header=None,\n",
    "                                          usecols=[0,1,2],names=['u','l','A'], chunksize=chunk_size, \n",
    "                                          iterator=True, low_memory=False, encoding='utf-8')\n",
    "        # Process multiple files in parallel\n",
    "        with ThreadPoolExecutor(max_workers=ncputrans) as trans_executor:\n",
    "            futures = [trans_executor.submit(ConvertExoMol2HITRAN,states_df,trans_df_chunk) \n",
    "                       for trans_df_chunk in tqdm(trans_df_chunk_list, desc='Processing '+trans_filename)]\n",
    "            hitran_res_df = pd.concat([future.result() for future in tqdm(futures)])\n",
    "    else:\n",
    "        trans_dd = dd.read_csv(trans_filepath, sep='\\s+', header=None, usecols=[0,1,2],names=['u','l','A'],encoding='utf-8')\n",
    "        trans_dd_list = trans_dd.partitions\n",
    "        # Process multiple files in parallel\n",
    "        with ThreadPoolExecutor(max_workers=ncputrans) as trans_executor:\n",
    "            futures = [trans_executor.submit(ConvertExoMol2HITRAN,states_df,trans_dd_list[i].compute(scheduler='threads')) \n",
    "                       for i in tqdm(range(len(list(trans_dd_list))), desc='Processing '+trans_filename)]\n",
    "            hitran_res_df = pd.concat([future.result() for future in tqdm(futures)])\n",
    "    return hitran_res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion_exomol2hitran(states_df):\n",
    "    print('Convert data format from ExoMol to HITRAN.')  \n",
    "    print('Running on ', ncputrans, 'cores.')\n",
    "    tot = Timer()\n",
    "    tot.start()\n",
    "    print('Reading transitions and converting data format from ExoMol to HITRAN ...')    \n",
    "    trans_filepaths = get_transfiles(read_path)\n",
    "    # Process multiple files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=ncpufiles) as executor:\n",
    "        # Submit reading tasks for each file\n",
    "        futures = [executor.submit(ProcessExoMol2HITRAN,states_df,\n",
    "                                   trans_filepath) for trans_filepath in tqdm(trans_filepaths)]\n",
    "        hitran_res_df = pd.concat([future.result() for future in futures])\n",
    "    hitran_res_df.sort_values(by=['v'], ascending=True, inplace=True)  \n",
    "    tot.end()\n",
    "    print('Finished reading all transitions and converting data format from ExoMol to HITRAN!\\n')\n",
    "\n",
    "    print('Saving HITRAN format data into file ...')   \n",
    "    ts = Timer()    \n",
    "    ts.start()\n",
    "    conversion_folder = save_path + '/conversion/ExoMol2HITRAN/'\n",
    "    if os.path.exists(conversion_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(conversion_folder, exist_ok=True)  \n",
    "    conversion_path = conversion_folder + isotopologue + '__' + dataset + '.par'\n",
    "    hitran_format = \"%2s%1s%12.6f%10.3E%10.3E%5.3f%5.3f%10.4f%4.2f%8s%15s%15s%15s%15s%6s%12s%1s%7.1f%7.1f\"\n",
    "    np.savetxt(conversion_path, hitran_res_df, fmt=hitran_format)\n",
    "    ts.end()\n",
    "    print('Converted par file has been saved!\\n')  \n",
    "    print('Finished converting data format from ExoMol to HITRAN!')\n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states_df = read_all_states(read_path)\n",
    "# conversion_exomol2hitran(states_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HITRAN to ExoMol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_QNValues_hitran2exomol(hitran2exomol_states_df, GlobalQNLabel_list, LocalQNLabel_list):\n",
    "    QNLabel_list = GlobalQNLabel_list+LocalQNLabel_list\n",
    "    if 'Gtot' in QNLabel_list:\n",
    "        hitran2exomol_states_df[\"Gtot\"] = (hitran2exomol_states_df[\"Gtot\"]\n",
    "                                           .str.replace('A1',\"1'\").str.replace('A2',\"2'\").str.replace(\"E'\",'3')\n",
    "                                           .str.replace('A1\"','4').str.replace('A2\"','5').str.replace('E\"','6'))    \n",
    "    if 'Gvib' in QNLabel_list:\n",
    "        hitran2exomol_states_df[\"Gvib\"] = (hitran2exomol_states_df[\"Gvib\"]\n",
    "                                           .str.replace('A1',\"1'\").str.replace('A2',\"2'\").str.replace(\"E'\",'3')\n",
    "                                           .str.replace('A1\"','4').str.replace('A2\"','5').str.replace('E\"','6'))    \n",
    "    if 'Grot' in QNLabel_list:\n",
    "        hitran2exomol_states_df[\"Grot\"] = (hitran2exomol_states_df[\"Grot\"]\n",
    "                                           .str.replace('A1',\"1'\").str.replace('A2',\"2'\").str.replace(\"E'\",'3')\n",
    "                                           .str.replace('A1\"','4').str.replace('A2\"','5').str.replace('E\"','6'))                  \n",
    "    if 'taui' in QNLabel_list:\n",
    "        hitran2exomol_states_df[\"taui\"] = hitran2exomol_states_df[\"taui\"].str.replace('s','0').str.replace('a','1')\n",
    "    return(hitran2exomol_states_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hitran2StatesTrans(hitran_df, QNu_df, QNl_df):\n",
    "    hitran2exomol_upper_df = pd.concat([hitran_df[['A','v','gp','Unc','Ep']],QNu_df], axis=1, join='inner')\n",
    "    hitran2exomol_lower_df = pd.concat([hitran_df[['A','v','gpp','Unc','Epp']],QNl_df], axis=1, join='inner')\n",
    "    Jpp_df = hitran2exomol_lower_df['J']\n",
    "    # hitran2exomol_upper_df['F'] = cal_F(hitran2exomol_upper_df['gp']).astype(str) \n",
    "    # hitran2exomol_lower_df['F'] = cal_F(hitran2exomol_lower_df['gpp']).astype(str) \n",
    "\n",
    "    hitran2exomol_upper_df.columns = list(map(lambda x: x.replace('gp','g').replace('Ep','E'), list(hitran2exomol_upper_df.columns)))\n",
    "    hitran2exomol_lower_df.columns = list(map(lambda x: x.replace('gpp','g').replace('Epp','E'), list(hitran2exomol_lower_df.columns)))\n",
    "\n",
    "    # hitran2exomol_lower_df = hitran2exomol_lower_df.reset_index(drop=True)\n",
    "    # hitran2exomol_upper_df = hitran2exomol_upper_df.reset_index(drop=True)\n",
    "    # if ('Sym\"' in QNl_col) and (\"Sym'\" not in QNu_col):\n",
    "    #     hitran2exomol_upper_df['Sym'] = hitran2exomol_lower_df['Sym']\n",
    "    #     index_change_sym = np.where(np.array(hitran2exomol_lower_df['J']-hitran2exomol_upper_df['J'])==0.0)[0]\n",
    "    #     hitran2exomol_upper_df['Sym'][index_change_sym] = (hitran2exomol_upper_df['Sym'][index_change_sym]\n",
    "    #                                                        .replace('e','e2f').replace('f','e').replace('e2f','f'))\n",
    "        \n",
    "    hitran2exomol_ul_df = pd.concat([hitran2exomol_upper_df, hitran2exomol_lower_df], axis=0)\n",
    "\n",
    "    hitranQNlabels = [x for x in list(hitran2exomol_ul_df.columns)[5:] if x != 'M' and x != 'm']\n",
    "    hitran2exomol_states_noid = (hitran2exomol_ul_df.loc[hitran2exomol_ul_df.groupby(['g']+hitranQNlabels)['Unc'].idxmax()][['E','g','Unc']+hitranQNlabels]\n",
    "                                .drop_duplicates().sort_values('E').groupby(['E','g']+hitranQNlabels)['Unc'].max().reset_index())\n",
    "    hitran2exomol_states_id = hitran2exomol_states_noid\n",
    "    hitran2exomol_states_id['id'] = hitran2exomol_states_noid.index+1\n",
    "\n",
    "    states_columns_order = ['id','E','g','F','Unc']+[x for x in hitranQNlabels if x != 'F']\n",
    "    hitran2exomol_states_id = hitran2exomol_states_id[states_columns_order]\n",
    "\n",
    "    # Transitions\n",
    "    upper_QNlabel = list(hitran2exomol_upper_df.columns[5:])\n",
    "    upper_idAv = hitran2exomol_upper_df.merge(hitran2exomol_states_id, on=['g']+upper_QNlabel, how='inner').drop(columns=upper_QNlabel)\n",
    "    upper_idAv['diffE'] = np.abs(upper_idAv['E_x']-upper_idAv['E_y'])\n",
    "    upper_AvEid = upper_idAv.loc[upper_idAv.groupby(['v'])['diffE'].idxmin()][['id','A','v','E_y']].rename(columns={'id':'u','E_y':'Ep'})\n",
    "\n",
    "    lower_QNlabel = list(hitran2exomol_lower_df.columns[5:])\n",
    "    lower_idAv = hitran2exomol_lower_df.merge(hitran2exomol_states_id, on=['g']+lower_QNlabel, how='inner').drop(columns=lower_QNlabel)\n",
    "    lower_idAv['diffE'] = np.abs(lower_idAv['E_x']-lower_idAv['E_y'])\n",
    "    lower_AvEid = lower_idAv.loc[lower_idAv.groupby(['v'])['diffE'].idxmin()][['id','A','v','E_y',]].rename(columns={'id':'l','E_y':'Epp'})\n",
    "\n",
    "    hitran2exomol_idAv = upper_AvEid.merge(lower_AvEid, on=['v'], how='inner').drop(columns=['A_y']).rename(columns={'A_x':'A','v':'v_x'})\n",
    "    hitran2exomol_idAv['v'] = hitran2exomol_idAv['Ep']-hitran2exomol_idAv['Epp']\n",
    "    diff = hitran2exomol_idAv[['u','l','A','v','v_x']]\n",
    "    diff['diffv'] = np.abs(diff['v_x'] - diff['v'])\n",
    "    hitran2exomol_trans_df = diff.loc[diff.groupby(['v'])['diffv'].idxmin()][['u','l','A','v']].sort_values('v')\n",
    "    \n",
    "    # States\n",
    "    hitran2exomol_states_df = convert_QNValues_hitran2exomol(hitran2exomol_states_id, GlobalQNLabel_list, LocalQNLabel_list)\n",
    "    hitran2exomol_states_df['Unc'] = (hitran2exomol_states_df['Unc'].replace(10,1e-09).replace(9,1e-08)\n",
    "                                      .replace(8,1e-07).replace(7,1e-06).replace(6,1e-05).replace(5,1e-04)\n",
    "                                      .replace(4,0.001).replace(3,0.01).replace(2,0.1).replace(0,10))\n",
    "    \n",
    "    return(Jpp_df, hitran2exomol_states_df, hitran2exomol_trans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hitran2broad(hitran_df, Jpp_df):\n",
    "    broad_code_df = pd.DataFrame(np.full_like(Jpp_df.astype(str),'a0'), columns=['code'])\n",
    "    hitran2exomol_air_df = pd.concat([broad_code_df, hitran_df[['gamma_air','n_air']], Jpp_df], axis=1).drop_duplicates().dropna()\n",
    "    hitran2exomol_self_df = pd.concat([broad_code_df, hitran_df[['gamma_self','n_air']], Jpp_df], axis=1).drop_duplicates().dropna()\n",
    "    return(hitran2exomol_air_df, hitran2exomol_self_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion_states(hitran2exomol_states_df, conversion_folder):\n",
    "    print('Convert states data format from HITRAN to ExoMol.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    conversion_states_path = conversion_folder + isotopologue + '__' + dataset + '.states.bz2'\n",
    "    hitranQNlabels_noF = hitran2exomol_states_df.columns[5:].tolist()\n",
    "    hitranQNformats = [QNsformat_list[j] for j in [QNslabel_list.index(i) for i in hitranQNlabels_noF]]\n",
    "\n",
    "    # states_format = (\"%12s %12.6f %6s %7s %12.6f \" \n",
    "    #                  + str(QNsformat_list).replace(\"['\",\"\").replace(\"']\",\"\")\n",
    "    #                  .replace(\"'\",\"\").replace(\",\",\"\").replace(\"d\",\"s\").replace(\"i\",\"s\"))\n",
    "    states_format = (\"%12s %12.6f %6s %7s %12.6f \" \n",
    "                    + str(hitranQNformats).replace(\"['\",\"\").replace(\"']\",\"\")\n",
    "                    .replace(\"'\",\"\").replace(\",\",\"\").replace(\"d\",\"s\").replace(\"i\",\"s\"))\n",
    "    np.savetxt(conversion_states_path, hitran2exomol_states_df, fmt=states_format)\n",
    "    t.end()\n",
    "    print('Converted states file has been saved!\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion_trans(hitran2exomol_trans_df, conversion_folder): \n",
    "    print('Convert transitions data format from HITRAN to ExoMol.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    conversion_trans_path = conversion_folder + isotopologue + '__' + dataset + '.trans.bz2'\n",
    "    trans_format = \"%12d %12d %10.4e %15.6f\"\n",
    "    np.savetxt(conversion_trans_path, hitran2exomol_trans_df, fmt=trans_format)\n",
    "    t.end()\n",
    "    print('Converted transition file has been saved!\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion_broad(hitran2exomol_air_df, hitran2exomol_self_df, conversion_folder):\n",
    "    print('Convert broadening data format from HITRAN to ExoMol.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    nair = len(hitran2exomol_air_df)\n",
    "    nself = len(hitran2exomol_self_df)\n",
    "    nbroad = nair + nself\n",
    "    broad_format = \"%2s %6.4f %6.3f %7s\"    \n",
    "    if nair != 0:\n",
    "        conversion_airbroad_path = conversion_folder + isotopologue + '__air.broad'\n",
    "        np.savetxt(conversion_airbroad_path, hitran2exomol_air_df, fmt=broad_format)\n",
    "    else:\n",
    "        print('No air broadening file.')\n",
    "    if nself != 0:\n",
    "        conversion_selfbroad_path = conversion_folder + isotopologue + '__self.broad'\n",
    "        np.savetxt(conversion_selfbroad_path, hitran2exomol_self_df, fmt=broad_format)\n",
    "    else:\n",
    "        print('No self broadening file.')\n",
    "    t.end()\n",
    "    if nbroad != 0:\n",
    "        print('Converted broadening files have been saved!\\n')  \n",
    "    else:\n",
    "        print('No broadening files need to be saved!\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion_hitran2exomol(hitran_df):\n",
    "\n",
    "    hitran_df = hitran_df[~hitran_df['Vp'].isin([' '*15])]\n",
    "    hitran_df = hitran_df[~hitran_df['Vpp'].isin([' '*15])]\n",
    "    hitran_df = hitran_df[~hitran_df['Qp'].isin([' '*15])]\n",
    "    hitran_df = hitran_df[~hitran_df['Qpp'].isin([' '*15])]\n",
    "    GlobalQNLabels,GlobalQNFormats = globalQNclasses(molecule,isotopologue)\n",
    "    LocalQNupperLabels, LocalQNlowerLabels, LocalQNupperFormats, LocalQNlowerFormats = localQNgroups(molecule,isotopologue)\n",
    "    QNu_df, QNl_df, QNu_col, QNl_col = separate_QN_hitran(hitran_df,GlobalQNLabels,LocalQNupperLabels,LocalQNlowerLabels,\n",
    "                                                          GlobalQNFormats,LocalQNupperFormats,LocalQNlowerFormats)\n",
    "    hitran_df['Ep'] = cal_Ep(hitran_df['Epp'].values,hitran_df['v'].values)\n",
    "    \n",
    "    Jpp_df, hitran2exomol_states_df, hitran2exomol_trans_df = convert_hitran2StatesTrans(hitran_df, QNu_df, QNl_df)\n",
    "    \n",
    "    hitran2exomol_air_df, hitran2exomol_self_df = convert_hitran2broad(hitran_df, Jpp_df)\n",
    "    \n",
    "    conversion_folder = save_path+'/conversion/HITRAN2ExoMol/'+molecule+'/'+isotopologue+'/'+dataset+'/' \n",
    "    if os.path.exists(conversion_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(conversion_folder, exist_ok=True)      \n",
    "    \n",
    "    conversion_states(hitran2exomol_states_df, conversion_folder)\n",
    "    conversion_trans(hitran2exomol_trans_df, conversion_folder)\n",
    "    conversion_broad(hitran2exomol_air_df, hitran2exomol_self_df, conversion_folder)\n",
    "    print('Finished converting data format from HITRAN to ExoMol!\\n')\n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parfile_df = read_parfile(read_path)\n",
    "# hitran_df = read_hitran_parfile(read_path,parfile_df,ConversionMinFreq,ConversionMaxFreq,\n",
    "#                                 ConversionUnc,ConversionThreshold).reset_index().drop(columns='index')\n",
    "# conversion_hitran2exomol(hitran_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stick Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessStickSpectra(states_part_df,T,Q,trans_part_df):\n",
    "    merged_df = dd.merge(trans_part_df, states_part_df, left_on='u', right_on='id', \n",
    "                         how='inner').merge(states_part_df, left_on='l', right_on='id', how='inner', suffixes=(\"'\", '\"'))\n",
    "    stick_spectra_df = merged_df.drop(columns=[\"id'\", 'id\"','u','l'])\n",
    "    stick_spectra_df['v'] = cal_v(stick_spectra_df[\"E'\"].values, stick_spectra_df['E\"'].values)\n",
    "    stick_spectra_df = stick_spectra_df[stick_spectra_df['v'].between(min_wn, max_wn)]\n",
    "    if len(stick_spectra_df) != 0 and QNsFilter != []:\n",
    "        stick_spectra_df = QNfilter_linelist(stick_spectra_df, QNs_value, QNs_label)\n",
    "    num = len(stick_spectra_df)\n",
    "    if num > 0:\n",
    "        v = stick_spectra_df['v'].values\n",
    "        if abs_emi == 'Ab':\n",
    "            stick_spectra_df['S'] = cal_abscoefs(T,v,stick_spectra_df[\"g'\"].values,stick_spectra_df['A'].values,\n",
    "                                                    stick_spectra_df['E\"'].values,Q,abundance)\n",
    "        elif abs_emi == 'Em':\n",
    "            stick_spectra_df['S'] = cal_emicoefs(T,v,stick_spectra_df[\"g'\"].values,stick_spectra_df['A'].values,\n",
    "                                                    stick_spectra_df[\"E'\"].values,Q,abundance)\n",
    "        else:\n",
    "            raise ImportError(\"Please choose one from: 'Absorption' or 'Emission'.\")  \n",
    "        if threshold != 'None':\n",
    "            stick_spectra_df = stick_spectra_df[stick_spectra_df['S'] >= threshold]\n",
    "        else:\n",
    "            pass  \n",
    "    else:\n",
    "        stick_spectra_df['S'] = np.array([])\n",
    "    stick_spectra_df.drop(columns=['A',\"g'\",'g\"'], inplace=True)\n",
    "    col_main = ['v','S',\"J'\",\"E'\",'J\"','E\"']\n",
    "    col_qn = [col for col in stick_spectra_df.columns if col not in col_main]\n",
    "    # col_stick_spectra = col_main + col_qn\n",
    "    col_stick_spectra = ['v','S']\n",
    "    stick_spectra_df = stick_spectra_df[col_stick_spectra]\n",
    "    return stick_spectra_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stick_spectra(states_part_df,T,Q,trans_filepath):   \n",
    "    trans_filename = trans_filepath.split('/')[-1]\n",
    "    print('Processeing transitions file:', trans_filename)\n",
    "    if trans_filepath.split('.')[-1] == 'bz2':\n",
    "        trans_df_chunk_list = pd.read_csv(trans_filepath, compression='bz2', sep='\\s+', header=None,\n",
    "                                          usecols=[0,1,2],names=['u','l','A'], chunksize=chunk_size, \n",
    "                                          iterator=True, low_memory=False, encoding='utf-8')\n",
    "        # Process multiple files in parallel\n",
    "        with ThreadPoolExecutor(max_workers=ncputrans) as trans_executor:\n",
    "            futures = [trans_executor.submit(ProcessStickSpectra,states_part_df,T,Q,trans_df_chunk) \n",
    "                       for trans_df_chunk in tqdm(trans_df_chunk_list, desc='Processing '+trans_filename)]\n",
    "            stick_spectra_df = pd.concat([future.result() for future in tqdm(futures)])\n",
    "    else:\n",
    "        trans_dd = dd.read_csv(trans_filepath, sep='\\s+', header=None, usecols=[0,1,2],names=['u','l','A'],encoding='utf-8')\n",
    "        trans_dd_list = trans_dd.partitions\n",
    "        # Process multiple files in parallel\n",
    "        with ThreadPoolExecutor(max_workers=ncputrans) as trans_executor:\n",
    "            futures = [trans_executor.submit(ProcessStickSpectra,states_part_df,T,Q,trans_dd_list[i].compute(scheduler='threads')) \n",
    "                       for i in tqdm(range(len(list(trans_dd_list))), desc='Processing '+trans_filename)]\n",
    "            stick_spectra_df = pd.concat([future.result() for future in tqdm(futures)])\n",
    "    return stick_spectra_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stick spectra\n",
    "def plot_stick_spectra(stick_spectra_df):  \n",
    "    print('Plotting stick spectra ...')\n",
    "    tp = Timer()\n",
    "    tp.start()\n",
    "    v = stick_spectra_df['v'].values\n",
    "    S = stick_spectra_df['S'].values\n",
    "    \n",
    "    parameters = {'axes.labelsize': 18, \n",
    "                  'legend.fontsize': 18,\n",
    "                  'xtick.labelsize': 14,\n",
    "                  'ytick.labelsize': 14}\n",
    "    plt.rcParams.update(parameters)\n",
    "    ss_plot_folder = save_path + 'stick_spectra/plots/'+molecule+'/'+database+'/'\n",
    "    if os.path.exists(ss_plot_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(ss_plot_folder, exist_ok=True)\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.fill_between(v, 0, S, label='T = '+str(T), linewidth=1.5, alpha=1)\n",
    "    ax.semilogy()\n",
    "    ax.set_xlim([min_wn, max_wn])\n",
    "    ax.set_ylim([limitYaxisStickSpectra, 10*max(S)])\n",
    "    #plt.title(database+' '+molecule+' intensity') \n",
    "    ax.set_xlabel('Wavenumber, cm$^{-1}$')\n",
    "    ax.set_ylabel('Intensity, cm/molecule')\n",
    "    leg = ax.legend()                  # Get the legend object.\n",
    "    for line in leg.legend_handles:\n",
    "        line.set_height(1.5)           # Change the line width for the legend.\n",
    "    ss_plot = (ss_plot_folder+molecule+'__'+isotopologue+'__'+ dataset+'__T'+str(T)+'__'+\n",
    "               str(min_wn)+'-'+str(max_wn)+'__unc'+str(UncFilter)+'__'+abs_emi+'__sp.png')\n",
    "    plt.savefig(ss_plot, dpi=500)\n",
    "    plt.show()\n",
    "    tp.end()\n",
    "    print('Stick spectra plot has been saved:', ss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stick spectra for ExoMol database\n",
    "def exomol_stick_spectra(states_part_df, T):\n",
    "    print('Calculate stick spectra.')  \n",
    "    print('Running on ', ncputrans, 'cores.')\n",
    "    print('{:25s} : {:<6}'.format('Uncertainty filter', UncFilter), u'cm\\u207B\\u00B9')\n",
    "    print('{:25s} : {:<6}'.format('Threshold filter', threshold), u'cm\\u207B\\u00B9/(molecule cm\\u207B\\u00B2)')\n",
    "    print('{:25s} : {} {} {} {}'.format('Wavenumber range selected', min_wn, u'cm\\u207B\\u00B9 -', max_wn, 'cm\\u207B\\u00B9'))\n",
    "    tot = Timer()\n",
    "    tot.start()\n",
    "    Q = read_exomol_pf(read_path, T)\n",
    "    if check_uncertainty == 1:\n",
    "        states_part_df.drop(columns=['unc'], inplace=True)\n",
    "    if check_lifetime == 1:\n",
    "        states_part_df.drop(columns=['tau'], inplace=True)\n",
    "    if check_gfactor == 1:\n",
    "        states_part_df.drop(columns=['gfac'], inplace=True)\n",
    "    \n",
    "    print('Reading transitions and calculating stick spectra ...')    \n",
    "    trans_filepaths = get_part_transfiles(read_path)   \n",
    "    # Process multiple files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=ncpufiles) as executor:\n",
    "        # Submit reading tasks for each file\n",
    "        futures = [executor.submit(calculate_stick_spectra,states_part_df,T,Q,\n",
    "                                   trans_filepath) for trans_filepath in tqdm(trans_filepaths)]\n",
    "        stick_spectra_df = pd.concat([future.result() for future in futures])\n",
    "        \n",
    "    if len(stick_spectra_df) == 0:\n",
    "        raise ImportError(\"Empty result with the input filter values. Please type new filter values in the input file.\")  \n",
    "    else:\n",
    "        stick_spectra_df.sort_values(by=['v'], ascending=True, inplace=True) \n",
    "    tot.end()\n",
    "    print('Finished reading transitions and calculating stick spectra!')\n",
    "    \n",
    "    print('Saving stick spectra into file ...')   \n",
    "    ts = Timer()    \n",
    "    ts.start()\n",
    "    QNsfmf = (str(QNs_format).replace(\"'\",\"\").replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "                .replace('d','s').replace('i','s').replace('.1f','s'))\n",
    "    ss_folder = save_path + 'stick_spectra/files/'+molecule+'/'+database+'/'\n",
    "    if os.path.exists(ss_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(ss_folder, exist_ok=True)\n",
    "    ss_path = (ss_folder+molecule+'__'+isotopologue+'__'+ dataset+'__T'+str(T)+'__'\n",
    "               +str(min_wn)+'-'+str(max_wn)+'__unc'+str(UncFilter)\n",
    "               +'__thres'+str(threshold)+'__'+abs_emi+'.stick')\n",
    "    # ss_colname = stick_spectra_df.columns\n",
    "    if QNsfmf == '':\n",
    "        # fmt = '%12.8E %12.8E %7s %12.4f %7s %12.4f'\n",
    "        fmt = '%12.8E %12.8E'\n",
    "    else:\n",
    "        fmt = '%12.8E %12.8E %7s %12.4f %7s %12.4f ' + QNsfmf + ' ' + QNsfmf\n",
    "    np.savetxt(ss_path, stick_spectra_df, fmt=fmt, header='')\n",
    "    ts.end()\n",
    "    print('Stick spectra file has been saved:', ss_path, '\\n')\n",
    "\n",
    "    # Plot stick spectra and save it as .png.\n",
    "    if PlotStickSpectraYN == 'Y':\n",
    "        plot_stick_spectra(stick_spectra_df)\n",
    "    print('Stick spectra have been saved!\\n')\n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stick spectra for HITRAN database\n",
    "def hitran_stick_spectra(hitran_linelist_df, QNs_col, T):\n",
    "    print('Calculate stick spectra.')  \n",
    "    print('{:25s} : {:<6}'.format('Uncertainty filter', UncFilter), u'cm\\u207B\\u00B9')\n",
    "    print('{:25s} : {:<6}'.format('Threshold filter', threshold), u'cm\\u207B\\u00B9/(molecule cm\\u207B\\u00B2)')\n",
    "    print('{:25s} : {} {} {} {}'.format('Wavenumber range selected', min_wn, u'cm\\u207B\\u00B9 -', max_wn, 'cm\\u207B\\u00B9'))\n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    A, v, Ep, Epp, gp, n_air, gamma_air, gamma_self, delta_air = linelist_hitran(hitran_linelist_df)\n",
    "    Q = read_hitran_pf(T)\n",
    "    # Absorption or emission stick spectra.\n",
    "    if abs_emi == 'Ab': \n",
    "        print('Absorption stick spectra') \n",
    "        I = cal_abscoefs(T, v, gp, A, Epp, Q, abundance)\n",
    "    elif abs_emi == 'Em': \n",
    "        print('Emission stick spectra')\n",
    "        I = cal_emicoefs(T, v, gp, A, Ep, Q, abundance)\n",
    "    else:\n",
    "        raise ImportError(\"Please choose one from: 'Absorption' or 'Emission'.\")\n",
    "    \n",
    "    ss_colname = ['v','S',\"J'\",\"E'\",'J\"','E\"'] + QNs_col\n",
    "    stick_spectra_df = hitran_linelist_df[ss_colname]\n",
    "    stick_spectra_df = stick_spectra_df.sort_values('v') \n",
    "    t.end() \n",
    "    print('Finished calculating stick spectra!')\n",
    "    \n",
    "    print('Saving stick spectra into file ...')   \n",
    "    ts = Timer()    \n",
    "    ts.start()    \n",
    "    QN_format_noJ = [QNsformat_list[i] for i in [QNslabel_list.index(j) for j in QNs_label]]\n",
    "    QNsfmf = (str(QN_format_noJ).replace(\"'\",\"\").replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "              .replace('d','s').replace('i','s').replace('.1f','s'))\n",
    "    ss_folder = save_path + 'stick_spectra/stick/'+molecule+'/'+database+'/'\n",
    "    if os.path.exists(ss_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(ss_folder, exist_ok=True)\n",
    "    ss_path = ss_folder + isotopologue + '__' + dataset + '.stick'\n",
    "    fmt = '%12.8E %12.8E %7s %12.4f %7s %12.4f ' + QNsfmf + ' ' + QNsfmf\n",
    "    np.savetxt(ss_path, stick_spectra_df, fmt=fmt, header='')\n",
    "    ts.end()\n",
    "    print('Stick spectra file has been saved:', ss_path)\n",
    "    \n",
    "    # Plot stick spectra and save it as .png.\n",
    "    if PlotStickSpectraYN == 'Y':\n",
    "        plot_stick_spectra(stick_spectra_df)\n",
    "    print('Stick spectra have been saved!\\n') \n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = 300\n",
    "# states_df = read_all_states(read_path)\n",
    "# states_part_df = read_part_states(states_df)\n",
    "# exomol_stick_spectra(states_part_df, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = 300\n",
    "# abs_emi = 'Ab'\n",
    "# PlotStickSpectraYN = 'Y'\n",
    "# parfile_df = read_parfile(read_path)\n",
    "# hitran_df = read_hitran_parfile (read_path,parfile_df,min_wn,max_wn,UncFilter,threshold).reset_index().drop(columns='index')\n",
    "# (hitran_linelist_df, QNs_col) = hitran_linelist_QN(hitran_df)\n",
    "# hitran_stick_spectra(hitran_linelist_df, QNs_col, T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_profile(profile):\n",
    "    if profile[0:3] == 'DOP':\n",
    "        profile_label = 'Doppler'\n",
    "    elif profile[0:3] == 'GAU':\n",
    "        profile_label = 'Gaussian'\n",
    "    elif profile[0:3] == 'LOR':\n",
    "        profile_label = 'Lorentzian'\n",
    "    elif 'SCI' in profile and 'W' not in profile:\n",
    "        profile_label = 'SciPy Voigt'\n",
    "    elif 'W' in profile:\n",
    "        profile_label = 'SciPy wofz Voigt'\n",
    "    elif 'H' in profile:\n",
    "        profile_label = 'Humlicek Voigt'  \n",
    "    elif 'TH' in profile:\n",
    "        profile_label = 'Thompson pseudo-Voigt'\n",
    "    elif 'K' in profile and 'H' not in profile:\n",
    "        profile_label = 'Kielkopf pseudo-Voigt'\n",
    "    elif 'OL' in profile:\n",
    "        profile_label = 'Olivero pseudo-Voigt'\n",
    "    elif 'LI' in profile or 'LL' in profile:\n",
    "        profile_label = 'Liu-Lin pseudo-Voigt'\n",
    "    elif 'RO' in profile:\n",
    "        profile_label = 'Rocco pseudo-Voigt'\n",
    "    elif 'BIN' in profile and 'DOP' in profile:\n",
    "        profile_label = 'Binned Doppler'     \n",
    "    elif 'BIN' in profile and 'GAU' in profile:\n",
    "        profile_label = 'Binned Gaussion'\n",
    "    elif 'BIN' in profile and 'LOR' in profile:\n",
    "        profile_label = 'Binned Lorentzian'\n",
    "    elif 'BIN' in profile and 'VOI' in profile:\n",
    "        profile_label = 'Binned Voigt'\n",
    "    else:\n",
    "        raise ImportError('Please choose line profile from the list.')\n",
    "    return profile_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Doppler_HWHM(v,T):\n",
    "    '''Return the Doppler half-width at half-maximum (HWHM) -- alpha.'''\n",
    "    # alpha = np.sqrt(2 * N_A * kB * T * np.log(2) / mass) * v / c\n",
    "    alpha = ne.evaluate('Sqrt2NAkBln2mInvc * sqrt(T) * v')\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_standard_deviation(alpha):\n",
    "    '''Return the Gaussian standard deviation -- sigma.'''\n",
    "    # sigma = alpha / np.sqrt(2 * np.log(2))\n",
    "    sigma = ne.evaluate('alpha * InvSqrt2ln2')\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lorentzian_HWHM(gamma_L, n_air,T,P):\n",
    "    '''Return the Lorentzian half-width at half-maximum (HWHM) -- gamma.'''\n",
    "    gamma = ne.evaluate('gamma_L * (Tref / T)**n_air * (P / Pref)')\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifetime_broadening(tau):\n",
    "    '''Return the lifetime broadening -- gamma_tau.'''\n",
    "    gamma_tau = ne.evaluate('1 / (PI4c) * tau')\n",
    "    return gamma_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DopplerHWHM_alpha(num_v, alpha_HWHM, v, T):\n",
    "    if alpha_HWHM != 'None':\n",
    "        alpha = np.full(num_v, alpha_HWHM)\n",
    "    else:\n",
    "        alpha = Doppler_HWHM(v,T)\n",
    "    return(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LorentzianHWHM_gamma(num, gamma_HWHM, nbroad, gamma_L, n_air, gamma_air, gamma_self, tau, T, P):\n",
    "    if gamma_HWHM != 'None':\n",
    "        gamma = np.full(num, gamma_HWHM)\n",
    "    elif database == 'ExoMol' and num > 0:\n",
    "        gamma = sum([Lorentzian_HWHM(gamma_L[i].values, n_air[i].values,T,P) for i in range(nbroad)])\n",
    "        if predissocYN == 'Y' and check_predissoc == 0 and 'VOI' in profile:\n",
    "            gamma += lifetime_broadening(tau)\n",
    "        else:\n",
    "            pass\n",
    "    elif database == 'HITRAN' and num > 0:  \n",
    "        gamma_L = gamma_air*0.7 + gamma_self*0.3\n",
    "        gamma = Lorentzian_HWHM(gamma_L, n_air,T,P) \n",
    "    else:\n",
    "        gamma = np.zeros(num)\n",
    "    return(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def FWHM(alpha, gamma):\n",
    "#     '''Return the Gaussian full-width at half-maximum (FWHM)   -- fG.\n",
    "#        Return the Lorentzian full-width at half-maximum (FWHM) -- fL.\n",
    "#     '''\n",
    "#     # fG = 2 * sigma * np.sqrt(2 * np.log(2)) = 2 * alpha\n",
    "#     # fG = ne.evaluate('sigma * TwoSqrt2ln2')\n",
    "#     fG = ne.evaluate('2 * alpha')\n",
    "#     fL = ne.evaluate('2 * gamma')\n",
    "#     return (fG, fL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Doppler_profile(dv, alpha):\n",
    "    '''Return Doppler line profile at dv with HWHM alpha.'''\n",
    "    # Doppler_profile = np.sqrt(np.log(2) / np.pi) / alpha * np.exp(-np.log(2) * (dv / alpha)**2) \n",
    "    DopplerProfile = ne.evaluate('Sqrtln2InvPi / alpha * exp(Negln2 * (dv / alpha)**2)')\n",
    "    return DopplerProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lorentzian_profile(dv, gamma):\n",
    "    '''Return Lorentzian line profile at dv with HWHM gamma.'''\n",
    "    LorentzianProfile = ne.evaluate('gamma / PI / (dv**2 + gamma**2)')\n",
    "    return LorentzianProfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SciPyVoigt_profile(dv, sigma, gamma):\n",
    "    '''Return the Voigt line profile with Doppler component HWHM alpha and Lorentzian component HWHM gamma.'''\n",
    "    SciPyVoigtProfile = voigt_profile(dv, sigma, gamma)\n",
    "    return SciPyVoigtProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SciPyWofzVoigt_profile(dv, sigma, gamma):\n",
    "    '''Return the Voigt line profile with Doppler component HWHM alpha and Lorentzian component HWHM gamma.'''\n",
    "    # scipy_wofz_Voigt_profile = np.real(wofz((dv + 1j*gamma)/sigma/np.sqrt(2))) / sigma / np.sqrt(2*np.pi)\n",
    "    z = ne.evaluate('(dv + 1j*gamma)/sigma*InvSqrt2')\n",
    "    wz = wofz(z)\n",
    "    SciPyWofzVoigtProfile = ne.evaluate('real(wz) / sigma * InvSqrt2Pi')\n",
    "    return SciPyWofzVoigtProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Humlicek1(t):\n",
    "    w = ne.evaluate('t * InvSqrtPi / (0.5 + t**2)')\n",
    "    return(w)\n",
    "\n",
    "def Humlicek2(t, u):\n",
    "    w = ne.evaluate('(t*(1.4104739589+u*InvSqrtPi))/(0.75+u*(3+u))')\n",
    "    return(w)\n",
    "\n",
    "def Humlicek3(t):\n",
    "    w = ne.evaluate('(16.4955+t*(20.20933+t*(11.96482+t*(3.778987+0.5642236*t))))/(16.4955+t*(38.82363+t*(39.27121+t*(21.69274+t*(6.699398+t)))))')\n",
    "    return(w)\n",
    "\n",
    "def Humlicek4(t, u):\n",
    "    nom = ne.evaluate('t*(36183.31-u*(3321.99-u*(1540.787-u*(219.031-u*(35.7668-u*(1.320522-u*0.56419))))))')\n",
    "    den = ne.evaluate('32066.6-u*(24322.8-u*(9022.23-u*(2186.18-u*(364.219-u*(61.5704-u*(1.84144-u))))))')\n",
    "    w = ne.evaluate('exp(u)-nom/den')   \n",
    "    return(w)\n",
    "\n",
    "def HumlicekVoigt_profile(dv, alpha, gamma):\n",
    "    x = ne.evaluate('dv * Sqrtln2 / alpha')\n",
    "    y = ne.evaluate('gamma * Sqrtln2 / alpha')\n",
    "    t = ne.evaluate('y-1j*x')\n",
    "    s = ne.evaluate('abs(x)+y')\n",
    "    u = ne.evaluate('t**2')\n",
    "    w = np.zeros_like(s)\n",
    "    ybound = ne.evaluate('0.195*abs(x)-0.176')\n",
    "    # Region 1\n",
    "    humfilter1 = s >= 15\n",
    "    t1 = t[humfilter1]\n",
    "    w[humfilter1] = Humlicek1(t1)\n",
    "    # Region 2\n",
    "    humfilter2 = (5.5 <= s) & (s < 15)\n",
    "    t2 = t[humfilter2]\n",
    "    u2 = u[humfilter2]\n",
    "    w[humfilter2] = Humlicek2(t2, u2)\n",
    "    # Region 3\n",
    "    humfilter3 = (s < 5.5) & (y >= ybound)\n",
    "    t3 = t[humfilter3]\n",
    "    w[humfilter3] = Humlicek3(t3)\n",
    "    # Region 4\n",
    "    humfilter4 = (s < 5.5) & (y < ybound)\n",
    "    t4 = t[humfilter4]\n",
    "    u4 = u[humfilter4]\n",
    "    w[humfilter4] = Humlicek4(t4, u4)  \n",
    "    HumlicekVoigtProfile = ne.evaluate('real(w) / alpha * Sqrtln2InvPi')\n",
    "    return HumlicekVoigtProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PseudoVoigt_profile(dv, alpha, gamma, eta):\n",
    "    '''\n",
    "    hv is the Voigt half-width at half-maximum (HWHM), \n",
    "    which can be found from the HWHM of the associated Doppler and Lorentzian profile.\n",
    "    eta is a function of Voigt profile half width at half maximum (HWHM) parameters.\n",
    "    '''\n",
    "    GaussianProfile = Doppler_profile(dv, alpha)\n",
    "    LorentzianProfile = Lorentzian_profile(dv, gamma)\n",
    "    PseudoVoigtProfile = ne.evaluate('eta * LorentzianProfile + (1 - eta) * GaussianProfile')\n",
    "    return PseudoVoigtProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PseudoThompsonVoigt(alpha, gamma):\n",
    "    '''\n",
    "    hv is the Voigt half-width at half-maximum (HWHM), \n",
    "    which can be found from the HWHM of the associated Doppler and Lorentzian profile.\n",
    "    eta is a function of Voigt profile half width at half maximum (HWHM) parameters.\n",
    "    ''' \n",
    "    hV = ne.evaluate('(alpha**5+2.69269*alpha**4*gamma+2.42843*alpha**3*gamma**2+4.47163*alpha**2*gamma**3+0.07842*alpha*gamma**4+gamma**5)**0.2')\n",
    "    eta = ne.evaluate('1.36603*(gamma/hV) - 0.47719*(gamma/hV)**2 + 0.11116*(gamma/hV)**3')\n",
    "    return (eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PseudoKielkopfVoigt(alpha, gamma):\n",
    "    '''\n",
    "    hv is the Voigt half-width at half-maximum (HWHM), \n",
    "    which can be found from the HWHM of the associated Doppler and Lorentzian profile.\n",
    "    eta is a function of Voigt profile half width at half maximum (HWHM) parameters.\n",
    "    '''\n",
    "    hV = ne.evaluate('0.5346 * gamma + sqrt(0.2166 * gamma**2 + alpha**2)')\n",
    "    eta = ne.evaluate('1.36603*(gamma/hV) - 0.47719*(gamma/hV)**2 + 0.11116*(gamma/hV)**3')\n",
    "    return (eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PseudoOliveroVoigt(alpha, gamma):\n",
    "    '''\n",
    "    hv is the Voigt half-width at half-maximum (HWHM), \n",
    "    which can be found from the HWHM of the associated Doppler and Lorentzian profile.\n",
    "    eta is a function of Voigt profile half width at half maximum (HWHM) parameters.\n",
    "    '''\n",
    "    d = ne.evaluate('(gamma-alpha)/(gamma+alpha)')\n",
    "    hV = ne.evaluate('(1-0.18121*(1-d**2)-(0.023665*exp(0.6*d)+0.00418*exp(-1.9*d))*sin(PI*d))*(alpha+gamma)')\n",
    "    eta = ne.evaluate('1.36603*(gamma/hV) - 0.47719*(gamma/hV)**2 + 0.11116*(gamma/hV)**3')\n",
    "    return (eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PseudoLiuLinVoigt(alpha, gamma):\n",
    "    '''\n",
    "    hv is the Voigt half-width at half-maximum (HWHM), \n",
    "    which can be found from the HWHM of the associated Doppler and Lorentzian profile.\n",
    "    eta is a function of Voigt profile half width at half maximum (HWHM) parameters.\n",
    "    '''\n",
    "    d = ne.evaluate('(gamma-alpha)/(gamma+alpha)')\n",
    "    hV = ne.evaluate('(1-0.18121*(1-d**2)-(0.023665*exp(0.6*d)+0.00418*exp(-1.9*d))*sinPI*d)*(alpha+gamma)')\n",
    "    eta = ne.evaluate('0.68188+0.61293*d-0.18384*d**2-0.11568*d**3')\n",
    "    return (eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PseudoRoccoVoigt(alpha, gamma):\n",
    "    '''\n",
    "    hv is the Voigt half-width at half-maximum (HWHM), \n",
    "    which can be found from the HWHM of the associated Doppler and Lorentzian profile.\n",
    "    eta is a function of Voigt profile half width at half maximum (HWHM) parameters.\n",
    "    '''\n",
    "    y = gamma*Sqrtln2/alpha\n",
    "    erfy = erf(y)\n",
    "    bhalfy = ne.evaluate('y+Sqrtln2*exp(-0.6055*y+0.0718*y**2-0.0049*y**3+0.000136*y**4)')\n",
    "    Vy = ne.evaluate('bhalfy*exp(y**2)*(1-erfy)')\n",
    "    hV = ne.evaluate('alpha / Sqrtln2 * bhalfy')\n",
    "    eta = ne.evaluate('(Vy-Sqrtln2)/(Vy*OneminSqrtPIln2)')\n",
    "    return (eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinnedGaussian_profile(dv, alpha):\n",
    "    '''Return binned Gaussian line profile at dv with HWHM alpha.'''\n",
    "    erfxpos = erf(ne.evaluate('Sqrtln2*(dv+binSizeHalf)/alpha'))\n",
    "    erfxneg = erf(ne.evaluate('Sqrtln2*(dv-binSizeHalf)/alpha'))\n",
    "    BinnedGaussianProfile = ne.evaluate('erfxpos-erfxneg')\n",
    "    return BinnedGaussianProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinnedLorentzian_profile(dv, gamma, bnormBinsize):\n",
    "    '''Return binned Lorentzian line profile at dv with HWHM gamma.'''\n",
    "    BinnedLorentzianProfile = ne.evaluate('(arctan((dv+binSizeHalf)/gamma)-arctan((dv-binSizeHalf)/gamma))*bnormBinsize')\n",
    "    return BinnedLorentzianProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinnedVoigt_bnormq(wngrid_start, wngrid_end, v, sigma, gamma, x):\n",
    "    '''Return binned Voigt line profile at dv with HWHM gamma.'''\n",
    "    vxsigma = ne.evaluate('v+x*sigma')\n",
    "    bnormq = ne.evaluate('1/(arctan((wngrid_end-vxsigma)/gamma)-arctan((wngrid_start-vxsigma)/gamma))')\n",
    "    return bnormq\n",
    "\n",
    "def BinnedVoigt_lorenz(dv, sigma, gamma, x):\n",
    "    '''Return binned Voigt line profile at dv with HWHM gamma.'''\n",
    "    dvxsigma = ne.evaluate('dv-x*sigma')\n",
    "    lorenz = ne.evaluate('arctan((dvxsigma+binSizeHalf)/gamma)-arctan((dvxsigma-binSizeHalf)/gamma)')\n",
    "    return lorenz\n",
    "\n",
    "def BinnedVoigt_profile(w, bnormq, lorenz):\n",
    "    '''Return binned Voigt line profile at dv with HWHM gamma.'''\n",
    "    BinnedVoigtProfile = ne.evaluate('sum(w*bnormq*lorenz)')\n",
    "    return BinnedVoigtProfile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Cross Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_Doppler(wn_grid, v, alpha, coef, cutoff):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with Doppler profile.\n",
    "    \n",
    "    '''  \n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if cutoff == 'None':\n",
    "        start = max(0,wn_grid.searchsorted(min(v))-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            Doppler = Doppler_profile(dv, alpha)\n",
    "            _xsec[idx] = coef @ Doppler \n",
    "    else:\n",
    "        start = max(0,wn_grid.searchsorted(min(v)-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            if filter.sum() > 0:\n",
    "                _dv = dv[filter]\n",
    "                _alpha = alpha[filter]\n",
    "                _coef = coef[filter]\n",
    "                Doppler = Doppler_profile(_dv, _alpha)\n",
    "                _xsec[idx] = _coef @ Doppler  \n",
    "    xsec[start:end] += _xsec\n",
    "    return xsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_Lorentzian(wn_grid, v, gamma, coef, cutoff):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with Lorentzian profile.\n",
    "    \n",
    "    '''  \n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if cutoff == 'None':\n",
    "        start = max(0,wn_grid.searchsorted(min(v))-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            Lorentzian = Lorentzian_profile(dv, gamma)\n",
    "            _xsec[idx] = coef @ Lorentzian\n",
    "    else:\n",
    "        start = max(0,wn_grid.searchsorted(min(v)-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            if filter.sum() > 0:\n",
    "                _dv = dv[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                Lorentzian = Lorentzian_profile(_dv, _gamma)\n",
    "                _xsec[idx] = _coef @ Lorentzian\n",
    "    xsec[start:end] += _xsec   \n",
    "    return xsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_SciPyVoigt(wn_grid, v, sigma, gamma, coef, cutoff):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with SciPy Voigt profile.\n",
    "    \n",
    "    '''  \n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if cutoff == 'None':\n",
    "        start = max(0,wn_grid.searchsorted(min(v))-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            SciPyVoigt = SciPyVoigt_profile(dv, sigma, gamma)\n",
    "            _xsec[idx] = coef @ SciPyVoigt   \n",
    "    else:\n",
    "        start = max(0,wn_grid.searchsorted(min(v)-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            if filter.sum() > 0:\n",
    "                _dv = dv[filter]\n",
    "                _sigma = sigma[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                SciPyVoigt = SciPyVoigt_profile(_dv, _sigma, _gamma)\n",
    "                _xsec[idx] = _coef @ SciPyVoigt   \n",
    "    xsec[start:end] += _xsec\n",
    "    return xsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_SciPyWofzVoigt(wn_grid, v, sigma, gamma, coef, cutoff):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with SciPy Wofz Voigt profile.\n",
    "\n",
    "    '''\n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if cutoff == 'None':\n",
    "        start = max(0,wn_grid.searchsorted(min(v))-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            SciPyWofzVoigt = SciPyWofzVoigt_profile(dv, sigma, gamma)\n",
    "            _xsec[idx] = coef @ SciPyWofzVoigt\n",
    "    else:\n",
    "        start = max(0,wn_grid.searchsorted(min(v)-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            if filter.sum() > 0:\n",
    "                _dv = dv[filter]\n",
    "                _sigma = sigma[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                SciPyWofzVoigt = SciPyWofzVoigt_profile(_dv, _sigma, _gamma)\n",
    "                _xsec[idx] = _coef @ SciPyWofzVoigt\n",
    "    xsec[start:end] += _xsec\n",
    "    return xsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_HumlicekVoigt(wn_grid, v, alpha, gamma, coef, cutoff):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with Humlicek Voigt profile.\n",
    "    \n",
    "    '''\n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if cutoff == 'None':\n",
    "        start = max(0,wn_grid.searchsorted(min(v))-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            HumlicekVoigt = HumlicekVoigt_profile(dv, alpha, gamma)\n",
    "            _xsec[idx] = coef @ HumlicekVoigt\n",
    "    else:\n",
    "        start = max(0,wn_grid.searchsorted(min(v)-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            if filter.sum() > 0:\n",
    "                _dv = dv[filter]\n",
    "                _alpha = alpha[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                HumlicekVoigt = HumlicekVoigt_profile(_dv, _alpha, _gamma)\n",
    "                _xsec[idx] = _coef @ HumlicekVoigt\n",
    "    xsec[start:end] += _xsec\n",
    "    return xsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, coef, cutoff):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with Pseudo Voigt profile.\n",
    "\n",
    "    '''\n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if cutoff == 'None':\n",
    "        start = max(0,wn_grid.searchsorted(min(v))-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            PseudoVoigt = PseudoVoigt_profile(dv, alpha, gamma, eta)\n",
    "            _xsec[idx] = coef @ PseudoVoigt\n",
    "    else:\n",
    "        start = max(0,wn_grid.searchsorted(min(v)-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            if filter.sum() > 0:\n",
    "                _dv = dv[filter]\n",
    "                _alpha = alpha[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _eta = eta[filter]\n",
    "                _coef = coef[filter]\n",
    "                PseudoVoigt = PseudoVoigt_profile(_dv, _alpha, _gamma, _eta)\n",
    "                _xsec[idx] = _coef @ PseudoVoigt\n",
    "    xsec[start:end] += _xsec   \n",
    "    return xsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_BinnedGaussian(wn_grid, v, alpha, coef, cutoff):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with binned Gaussian profile.\n",
    "    \n",
    "    '''  \n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if cutoff == 'None':\n",
    "        start = max(0,wn_grid.searchsorted(min(v))-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            BinnedGaussian = BinnedGaussian_profile(dv, alpha)\n",
    "            _xsec[idx] = coef @ BinnedGaussian \n",
    "    else:\n",
    "        start = max(0,wn_grid.searchsorted(min(v)-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            if filter.sum() > 0:\n",
    "                _dv = dv[filter]\n",
    "                _alpha = alpha[filter]\n",
    "                _coef = coef[filter]\n",
    "                BinnedGaussian = BinnedGaussian_profile(_dv, _alpha)\n",
    "                _xsec[idx] = _coef @ BinnedGaussian \n",
    "    xsec[start:end] += _xsec / binSize2\n",
    "    return xsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_BinnedLorentzian(wn_grid, v, gamma, coef, cutoff):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with binned Lorentzian profile.\n",
    "    \n",
    "    ''' \n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if cutoff == 'None':\n",
    "        start = max(0,wn_grid.searchsorted(min(v))-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)),len(wn_grid))\n",
    "        wngrid_start = wn_grid[start]\n",
    "        wngrid_end = wn_grid[end-1]\n",
    "        bnormBinsize = ne.evaluate('PI/(arctan((wngrid_end-v)/gamma)-arctan((wngrid_start-v)/gamma))/bin_size')\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            BinnedLorentzian = BinnedLorentzian_profile(dv, gamma, bnormBinsize)\n",
    "            _xsec[idx] = coef @ BinnedLorentzian\n",
    "    else:\n",
    "        start = max(0,wn_grid.searchsorted(min(v)-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)+cutoff),len(wn_grid))\n",
    "        wngrid_start = wn_grid[start]\n",
    "        wngrid_end = wn_grid[end-1]\n",
    "        bnormBinsize = ne.evaluate('PI/(arctan((wngrid_end-v)/gamma)-arctan((wngrid_start-v)/gamma))/bin_size')\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            if filter.sum() > 0:\n",
    "                _dv = dv[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _bnormBinsize = bnormBinsize[filter]\n",
    "                _coef = coef[filter]\n",
    "                BinnedLorentzian = BinnedLorentzian_profile(_dv, _gamma, _bnormBinsize)\n",
    "                _xsec[idx] = _coef @ BinnedLorentzian\n",
    "    xsec[start:end] += _xsec  \n",
    "    return xsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_BinnedVoigt(wn_grid, v, sigma, gamma, coef, cutoff):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with binned Voigt profile.\n",
    "\n",
    "    '''\n",
    "    nquad = 20\n",
    "    roots, weights = roots_hermite(nquad, mu=False)\n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if cutoff == 'None':\n",
    "        start = max(0,wn_grid.searchsorted(min(v))-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)),len(wn_grid))\n",
    "        wngrid_start = wn_grid[start]\n",
    "        wngrid_end = wn_grid[end-1]\n",
    "        bnormqT = np.transpose(np.array([BinnedVoigt_bnormq(wngrid_start, wngrid_end, v, sigma, gamma, root) for root in roots]))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            lorenzT = np.transpose(np.array([BinnedVoigt_lorenz(dv, sigma, gamma, root) for root in roots]))\n",
    "            BinnedVoigtProfile = BinnedVoigt_profile(weights,bnormqT,lorenzT)                 \n",
    "            _xsec[idx] = ne.evaluate('sum(coef * BinnedVoigtProfile)')\n",
    "    else:\n",
    "        start = max(0,wn_grid.searchsorted(min(v)-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(max(v)+cutoff),len(wn_grid))\n",
    "        wngrid_start = wn_grid[start]\n",
    "        wngrid_end = wn_grid[end-1]\n",
    "        bnormq = [BinnedVoigt_bnormq(wngrid_start, wngrid_end, v, sigma, gamma, root) for root in roots]\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            dv = wn_grid[i] - v\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            if filter.sum() > 0:\n",
    "                _dv = dv[filter]\n",
    "                _sigma = sigma[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                bnormqT = np.transpose(np.array([bnormq[i][filter] for i in range(nquad)]))\n",
    "                lorenzT = np.transpose(np.array([BinnedVoigt_lorenz(_dv, _sigma, _gamma, root) for root in roots]))\n",
    "                BinnedVoigtProfile = BinnedVoigt_profile(weights,bnormqT,lorenzT)    \n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * BinnedVoigtProfile)')\n",
    "    xsec[start:end] += _xsec * InvbinSizePIhalf\n",
    "    return xsec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line list for calculating cross sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateExoMolCrossSection(states_part_df,T,P,Q,broad,ratio,nbroad,broad_dfs,profile_label,trans_part_df):\n",
    "    merged_df = dd.merge(trans_part_df, states_part_df, left_on='u', right_on='id', \n",
    "                         how='inner').merge(states_part_df, left_on='l', right_on='id', how='inner', suffixes=(\"'\", '\"'))\n",
    "    st_df = merged_df.drop(columns=[\"id'\", 'id\"'])\n",
    "    st_df['v'] = cal_v(st_df[\"E'\"].values, st_df['E\"'].values)\n",
    "    if cutoff == 'None':\n",
    "        st_df = st_df[st_df['v'].between(min_wn, max_wn)]\n",
    "    else:\n",
    "        st_df = st_df[st_df['v'].between(min_wn - cutoff, max_wn + cutoff)]\n",
    "    if len(st_df) != 0 and QNsFilter != []:\n",
    "        st_df = QNfilter_linelist(st_df, QNs_value, QNs_label)\n",
    "    if predissocYN == 'Y' and 'VOI' in profile:\n",
    "        st_df = st_df[['A','v',\"g'\",\"E'\",'E\"','J\"',\"tau'\"]]\n",
    "    else:\n",
    "        st_df = st_df[['A','v',\"g'\",\"E'\",'E\"','J\"']]\n",
    "        \n",
    "    num = len(st_df)\n",
    "    if num > 0:\n",
    "        v = st_df['v'].values\n",
    "        if abs_emi == 'Ab':\n",
    "            st_df['coef'] = cal_abscoefs(T,v,st_df[\"g'\"].values,st_df['A'].values,st_df['E\"'].values,Q,abundance)\n",
    "        elif abs_emi == 'Em':\n",
    "            st_df['coef'] = cal_emicoefs(T,v,st_df[\"g'\"].values,st_df['A'].values,st_df[\"E'\"].values,Q,abundance)\n",
    "        else:\n",
    "            raise ImportError(\"Please choose one from: 'Absorption' or 'Emission'.\")  \n",
    "        st_df.drop(columns=['A',\"g'\",\"E'\",'E\"'], inplace=True)\n",
    "        if threshold != 'None':\n",
    "            st_df = st_df[st_df['coef'] >= threshold]  \n",
    "            v = st_df['v'].values\n",
    "            num = len(st_df)         \n",
    "        else:\n",
    "            pass  \n",
    "        if num > 0:\n",
    "            if 'DOP' not in profile and 'GAU' not in profile:\n",
    "                gamma_L = pd.DataFrame()\n",
    "                n_air = pd.DataFrame()\n",
    "                for i in range(nbroad):\n",
    "                    if broad[i] == 'Default':\n",
    "                        gamma_L[i] = np.full((1,num),broad_dfs[i]['gamma_L'][0])[0] * ratio[i]\n",
    "                        n_air[i] = np.full((1,num),broad_dfs[i]['n_air'][0])[0] * ratio[i]\n",
    "                    else:\n",
    "                        (gammaL,nair) = extract_broad(broad_dfs[i], st_df)\n",
    "                        gamma_L[i] = gammaL * ratio[i]\n",
    "                        n_air[i] = nair * ratio[i]    \n",
    "                if predissocYN == 'Y' and 'VOI' in profile:\n",
    "                    tau = st_df[\"tau'\"].values\n",
    "                else:\n",
    "                    tau = np.array([])\n",
    "                gamma = LorentzianHWHM_gamma(num, gamma_HWHM, nbroad, gamma_L, n_air, [], [], tau, T, P)\n",
    "            else:\n",
    "                gamma = np.array([])\n",
    "            coef = st_df['coef'].values\n",
    "        \n",
    "            # Line profiles\n",
    "            if profile_label == 'Doppler':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                xsec = cross_section_Doppler(wn_grid, v, alpha, coef, cutoff)\n",
    "            elif profile_label == 'Gaussian':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                xsec = cross_section_Doppler(wn_grid, v, alpha, coef, cutoff)\n",
    "            elif profile_label == 'Lorentzian':\n",
    "                xsec = cross_section_Lorentzian(wn_grid, v, gamma, coef, cutoff)\n",
    "            elif profile_label == 'SciPy Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                sigma = Gaussian_standard_deviation(alpha)     \n",
    "                xsec = cross_section_SciPyVoigt(wn_grid, v, sigma, gamma, coef, cutoff)\n",
    "            elif profile_label == 'SciPy wofz Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                sigma = Gaussian_standard_deviation(alpha)     \n",
    "                xsec = cross_section_SciPyWofzVoigt(wn_grid, v, sigma, gamma, coef, cutoff)\n",
    "            elif profile_label == 'Humlicek Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                xsec = cross_section_HumlicekVoigt(wn_grid, v, alpha, gamma, coef, cutoff)  \n",
    "            elif profile_label == 'Thompson pseudo-Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                eta = PseudoThompsonVoigt(alpha, gamma)\n",
    "                xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, coef, cutoff)       \n",
    "            elif profile_label == 'Kielkopf pseudo-Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                eta = PseudoKielkopfVoigt(alpha, gamma)\n",
    "                xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, coef, cutoff)       \n",
    "            elif profile_label == 'Olivero pseudo-Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                eta = PseudoOliveroVoigt(alpha, gamma)\n",
    "                xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, coef, cutoff)       \n",
    "            elif profile_label == 'Liu-Lin pseudo-Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                eta = PseudoLiuLinVoigt(alpha, gamma)\n",
    "                xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, coef, cutoff)       \n",
    "            elif profile_label == 'Rocco pseudo-Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                eta = PseudoRoccoVoigt(alpha, gamma)\n",
    "                xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, coef, cutoff) \n",
    "            elif profile_label == 'Binned Doppler':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                xsec = cross_section_BinnedGaussian(wn_grid, v, alpha, coef, cutoff)        \n",
    "            elif profile_label == 'Binned Gaussion':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                xsec = cross_section_BinnedGaussian(wn_grid, v, alpha, coef, cutoff)\n",
    "            elif profile_label == 'Binned Lorentzian':\n",
    "                xsec = cross_section_BinnedLorentzian(wn_grid, v, gamma, coef, cutoff)\n",
    "            elif profile_label == 'Binned Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                sigma = Gaussian_standard_deviation(alpha)     \n",
    "                xsec = cross_section_BinnedVoigt(wn_grid, v, sigma, gamma, coef, cutoff)           \n",
    "            else:\n",
    "                raise ImportError('Please choose line profile from the list.')  \n",
    "        else:      \n",
    "                xsec = np.zeros_like(wn_grid)\n",
    "    else:\n",
    "        xsec = np.zeros_like(wn_grid)\n",
    "    return xsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateHITRANCrossSection(hitran_linelist_df, T, P, Q, profile_label):\n",
    "    A, v, Ep, Epp, gp, n_air, gamma_air, gamma_self, delta_air = linelist_hitran(hitran_linelist_df)\n",
    "    num = len(A)\n",
    "    if num > 0:\n",
    "        if abs_emi == 'Ab':\n",
    "            print('Absorption cross section') \n",
    "            coef = cal_abscoefs(T, v, gp, A, Epp, Q, abundance)\n",
    "        elif abs_emi == 'Em':\n",
    "            print('Emission cross section') \n",
    "            coef = cal_emicoefs(T, v, gp, A, Ep, Q, abundance)\n",
    "        else:\n",
    "            raise ImportError(\"Please choose one from: 'Absorption' or 'Emission'.\")  \n",
    "        if threshold != 'None':\n",
    "            ll_df = pd.DataFrame({'coef':coef, 'v':v, 'n_air':n_air, 'gamma_air':gamma_air, 'gamma_self':gamma_self})\n",
    "            ll_df = ll_df[ll_df['coef'] >= threshold]  \n",
    "            coef = ll_df['coef'].values\n",
    "            v = ll_df['v'].values\n",
    "            n_air = ll_df['n_air'].values\n",
    "            gamma_air = ll_df['gamma_air'].values\n",
    "            gamma_self = ll_df['gamma_self'].values\n",
    "            num = len(ll_df)         \n",
    "        else:\n",
    "            pass   \n",
    "        if num > 0:\n",
    "            # Line profiles\n",
    "            if profile_label == 'Doppler':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                xsec = cross_section_Doppler(wn_grid, v, alpha, coef, cutoff)\n",
    "            elif profile_label == 'Gaussian':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                xsec = cross_section_Doppler(wn_grid, v, alpha, coef, cutoff)\n",
    "            elif profile_label == 'Lorentzian':\n",
    "                gamma = LorentzianHWHM_gamma(num, gamma_HWHM, 0, [], n_air, gamma_air, gamma_self, [], T, P)\n",
    "                xsec = cross_section_Lorentzian(wn_grid, v, gamma, coef, cutoff)\n",
    "            elif profile_label == 'SciPy Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                sigma = Gaussian_standard_deviation(alpha)    \n",
    "                gamma = LorentzianHWHM_gamma(num, gamma_HWHM, 0, [], n_air, gamma_air, gamma_self, [], T, P) \n",
    "                xsec = cross_section_SciPyVoigt(wn_grid, v, sigma, gamma, coef, cutoff)\n",
    "            elif profile_label == 'SciPy wofz Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                sigma = Gaussian_standard_deviation(alpha)  \n",
    "                gamma = LorentzianHWHM_gamma(num, gamma_HWHM, 0, [], n_air, gamma_air, gamma_self, [], T, P)   \n",
    "                xsec = cross_section_SciPyWofzVoigt(wn_grid, v, sigma, gamma, coef, cutoff)\n",
    "            elif profile_label == 'Humlicek Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                gamma = LorentzianHWHM_gamma(num, gamma_HWHM, 0, [], n_air, gamma_air, gamma_self, [], T, P)\n",
    "                xsec = cross_section_HumlicekVoigt(wn_grid, v, alpha, gamma, coef, cutoff)  \n",
    "            elif profile_label == 'Thompson pseudo-Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                gamma = LorentzianHWHM_gamma(num, gamma_HWHM, 0, [], n_air, gamma_air, gamma_self, [], T, P)\n",
    "                eta = PseudoThompsonVoigt(alpha, gamma)\n",
    "                xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, coef, cutoff)       \n",
    "            elif profile_label == 'Kielkopf pseudo-Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                gamma = LorentzianHWHM_gamma(num, gamma_HWHM, 0, [], n_air, gamma_air, gamma_self, [], T, P)\n",
    "                eta = PseudoKielkopfVoigt(alpha, gamma)\n",
    "                xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, coef, cutoff)       \n",
    "            elif profile_label == 'Olivero pseudo-Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                gamma = LorentzianHWHM_gamma(num, gamma_HWHM, 0, [], n_air, gamma_air, gamma_self, [], T, P)\n",
    "                eta = PseudoOliveroVoigt(alpha, gamma)\n",
    "                xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, coef, cutoff)       \n",
    "            elif profile_label == 'Liu-Lin pseudo-Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                gamma = LorentzianHWHM_gamma(num, gamma_HWHM, 0, [], n_air, gamma_air, gamma_self, [], T, P)\n",
    "                eta = PseudoLiuLinVoigt(alpha, gamma)\n",
    "                xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, coef, cutoff)       \n",
    "            elif profile_label == 'Rocco pseudo-Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                gamma = LorentzianHWHM_gamma(num, gamma_HWHM, 0, [], n_air, gamma_air, gamma_self, [], T, P)\n",
    "                eta = PseudoRoccoVoigt(alpha, gamma)\n",
    "                xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, coef, cutoff) \n",
    "            elif profile_label == 'Binned Doppler':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                xsec = cross_section_BinnedGaussian(wn_grid, v, alpha, coef, cutoff)        \n",
    "            elif profile_label == 'Binned Gaussion':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                xsec = cross_section_BinnedGaussian(wn_grid, v, alpha, coef, cutoff)\n",
    "            elif profile_label == 'Binned Lorentzian':\n",
    "                gamma = LorentzianHWHM_gamma(num, gamma_HWHM, 0, [], n_air, gamma_air, gamma_self, [], T, P)\n",
    "                xsec = cross_section_BinnedLorentzian(wn_grid, v, gamma, coef, cutoff)\n",
    "            elif profile_label == 'Binned Voigt':\n",
    "                alpha = DopplerHWHM_alpha(num, alpha_HWHM, v, T)\n",
    "                sigma = Gaussian_standard_deviation(alpha)     \n",
    "                gamma = LorentzianHWHM_gamma(num, gamma_HWHM, 0, [], n_air, gamma_air, gamma_self, [], T, P)\n",
    "                xsec = cross_section_BinnedVoigt(wn_grid, v, sigma, gamma, coef, cutoff)           \n",
    "            else:\n",
    "                raise ImportError('Please choose line profile from the list.')  \n",
    "        else:      \n",
    "                xsec = np.zeros_like(wn_grid)\n",
    "    else:\n",
    "        xsec = np.zeros_like(wn_grid)    \n",
    "    return xsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xsec_part_trans(states_part_df,T,P,Q,broad,ratio,nbroad,broad_dfs,profile_label,trans_filepath): \n",
    "    trans_filename = trans_filepath.split('/')[-1]\n",
    "    print('Processeing transitions file:', trans_filename)\n",
    "    if trans_filepath.split('.')[-1] == 'bz2':\n",
    "        trans_df_chunk_list = pd.read_csv(trans_filepath, compression='bz2', sep='\\s+', header=None,\n",
    "                                          usecols=[0,1,2],names=['u','l','A'], chunksize=chunk_size, \n",
    "                                          iterator=True, low_memory=False, encoding='utf-8')\n",
    "        # Process multiple files in parallel\n",
    "        with ThreadPoolExecutor(max_workers=ncputrans) as trans_executor:\n",
    "            futures = [\n",
    "                trans_executor.submit(CalculateExoMolCrossSection,states_part_df,T,P,Q,\n",
    "                                        broad,ratio,nbroad,broad_dfs,profile_label,trans_df_chunk) \n",
    "                for trans_df_chunk in tqdm(trans_df_chunk_list, desc='Processing '+trans_filename)\n",
    "                ]\n",
    "            xsecs = sum([future.result() for future in futures])        \n",
    "    else:\n",
    "        trans_dd = dd.read_csv(trans_filepath, sep='\\s+', header=None, usecols=[0,1,2],names=['u','l','A'],encoding='utf-8')\n",
    "        trans_dd_list = trans_dd.partitions\n",
    "        # Process multiple files in parallel\n",
    "        with ThreadPoolExecutor(max_workers=ncputrans) as trans_executor:\n",
    "            futures = [\n",
    "                trans_executor.submit(CalculateExoMolCrossSection,states_part_df,T,P,Q,\n",
    "                                        broad,ratio,nbroad,broad_dfs,profile_label,trans_dd_list[i].compute(scheduler='threads')) \n",
    "                for i in tqdm(range(len(list(trans_dd_list))), desc='Processing '+trans_filename)\n",
    "                ]\n",
    "            xsecs = sum([future.result() for future in futures])   \n",
    "    return xsecs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_xsec(wn, xsec, database, profile_label):             \n",
    "    xsecs_foldername = save_path+'xsecs/files/'+molecule+'/'+database+'/'\n",
    "    if os.path.exists(xsecs_foldername):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(xsecs_foldername, exist_ok=True)\n",
    "    print(profile_label,'profile')\n",
    "    print('{:25s} : {:<6}'.format('Temperature selected', T), 'K')\n",
    "    print('{:25s} : {:<6}'.format('Pressure selected', P), 'bar')\n",
    "    \n",
    "    if 'L' not in wn_wl:\n",
    "        print('{:25s} : {:<6}'.format('Number of points is', N_point))\n",
    "        print('{:25s} : {:<6}'.format('Bin size is', bin_size), u'cm\\u207B\\u00B9')\n",
    "        print('{:25s} : {:<6}'.format('Cutoff is', cutoff), u'cm\\u207B\\u00B9')\n",
    "        print('{:25s} : {:<6}'.format('Uncertainty filter', UncFilter), u'cm\\u207B\\u00B9')\n",
    "        print('{:25s} : {:<6}'.format('Threshold filter', threshold), u'cm\\u207B\\u00B9/(molecule cm\\u207B\\u00B2)')\n",
    "        print('{:25s} : {} {} {} {}'.format('Wavenumber range selected', min_wn, u'cm\\u207B\\u00B9 -', max_wn, 'cm\\u207B\\u00B9'))\n",
    "        # Save cross sections into .xsec file.\n",
    "        xsec_df = pd.DataFrame()\n",
    "        xsec_df['wavenumber'] = wn\n",
    "        xsec_df['cross-section'] = xsec\n",
    "        xsec_filepath = (xsecs_foldername+molecule+'__T'+str(T)+'__'+wn_wl.lower()+str(min_wn)+'-'+str(max_wn)+'__'\n",
    "                         +database+'__'+abs_emi+'__'+profile_label.replace(' ','')+'.xsec')\n",
    "        np.savetxt(xsec_filepath, xsec_df, fmt=\"%12.6f %14.8E\")\n",
    "        print('Cross sections file has been saved:', xsec_filepath, '\\n')\n",
    "        if PlotCrossSectionYN == 'Y':\n",
    "            plots_foldername = save_path+'xsecs/plots/'+molecule+'/'+database+'/'\n",
    "            if os.path.exists(plots_foldername):\n",
    "                pass\n",
    "            else:\n",
    "                os.makedirs(plots_foldername, exist_ok=True)  \n",
    "            #plt.legend(fancybox=True, framealpha=0.0)\n",
    "            parameters = {'axes.labelsize': 18,\n",
    "                          'legend.fontsize': 18,\n",
    "                          'xtick.labelsize': 14,\n",
    "                          'ytick.labelsize': 14}\n",
    "            plt.rcParams.update(parameters)\n",
    "            # Plot cross sections and save it as .png.\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.xlim([min_wn, max_wn])\n",
    "            plt.ylim([limitYaxisXsec, 10*max(xsec)])\n",
    "            plt.plot(wn, xsec, label='T = '+str(T)+' K, '+profile_label, linewidth=0.4)   \n",
    "            plt.semilogy()\n",
    "            #plt.title(database+' '+molecule+' '+abs_emi+' Cross-Section with '+ profile_label) \n",
    "            plt.xlabel('Wavenumber, cm$^{-1}$')\n",
    "            plt.ylabel('Cross-section, cm$^{2}$/molecule')\n",
    "            plt.legend()\n",
    "            leg = plt.legend()                  # Get the legend object.\n",
    "            for line in leg.get_lines():\n",
    "                line.set_linewidth(1.0)         # Change the line width for the legend.\n",
    "            xsec_plotpath = (plots_foldername+molecule+'__T'+str(T)+'__'+wn_wl.lower()+str(min_wn)+'-'+str(max_wn)+'__'\n",
    "                         +database+'__'+abs_emi+'__'+profile_label.replace(' ','')+'.png')\n",
    "            plt.savefig(xsec_plotpath, dpi=500)\n",
    "            plt.show()\n",
    "            print('Cross sections plot has been saved:', xsec_plotpath, '\\n')\n",
    "    elif 'L' in wn_wl:\n",
    "        wl = 10000 / wn\n",
    "        min_wl = '%.02f' % (10000 / max_wn)\n",
    "        max_wl = '%.02f' % (10000 / min_wn)\n",
    "        print('{:25s} : {:<6}'.format('Number of points is', N_point))\n",
    "        print('{:25s} : {:<6}'.format('Bin size is', 10000/bin_size), u'\\xb5m')\n",
    "        print('{:25s} : {:<6}'.format('Cutoff is', 10000/cutoff),u'\\xb5m')\n",
    "        print('{:25s} : {:<6}'.format('Uncertainty filter', 10000/UncFilter),u'\\xb5m')\n",
    "        print('{:25s} : {:<6}'.format('Threshold filter',10000/threshold),u'\\xb5m/(moleculeu \\xb5m\\u00B2)')\n",
    "        print('{:25s} : {} {} {} {}'.format('Wavelength range selected',min_wl,u'\\xb5m -',max_wl,u'\\xb5m'))\n",
    "        # Save cross sections into .xsec file.\n",
    "        xsec_df = pd.DataFrame()\n",
    "        xsec_df['wavelength'] = wl\n",
    "        xsec_df['cross-section'] = xsec\n",
    "        xsec_filepath = (xsecs_foldername+molecule+'__T'+str(T)+'__'+wn_wl.lower()+str(min_wl)+'-'+str(max_wl)+'__'\n",
    "                         +database+'__'+abs_emi+'__'+profile_label.replace(' ','')+'.xsec')\n",
    "        np.savetxt(xsec_filepath, xsec_df, fmt=\"%12.6f %14.8E\")\n",
    "        print('Cross sections file has been saved:', xsec_filepath, '\\n')       \n",
    "        if PlotCrossSectionYN == 'Y':\n",
    "            plots_foldername = save_path+'xsecs/plots/'+molecule+'/'+database+'/'\n",
    "            if os.path.exists(plots_foldername):\n",
    "                pass\n",
    "            else:\n",
    "                os.makedirs(plots_foldername, exist_ok=True)  \n",
    "            #plt.legend(fancybox=True, framealpha=0.0)\n",
    "            parameters = {'axes.labelsize': 18, \n",
    "                          'legend.fontsize': 18,\n",
    "                          'xtick.labelsize': 14,\n",
    "                          'ytick.labelsize': 14}\n",
    "            plt.rcParams.update(parameters)\n",
    "            # Plot cross sections and save it as .png.\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.xlim([min_wl, max_wl])\n",
    "            plt.ylim([limitYaxisXsec, 10*max(xsec)])\n",
    "            plt.plot(wl, xsec, label='T = '+str(T)+' K, '+profile_label, linewidth=0.4)             \n",
    "            plt.semilogy()\n",
    "            #plt.title(database+' '+molecule+' '+abs_emi+' Cross-Section with '+ profile_label) \n",
    "            plt.xlabel(u'Wavelength, \\xb5m')\n",
    "            plt.ylabel(u'Cross-section, \\xb5m\\u207B\\u00B2/molecule')\n",
    "            plt.legend()\n",
    "            leg = plt.legend()                  # Get the legend object.\n",
    "            for line in leg.get_lines():\n",
    "                line.set_linewidth(1.0)         # Change the line width for the legend.\n",
    "            xsec_plotpath = (plots_foldername+molecule+'__T'+str(T)+'__'+wn_wl.lower()+str(min_wl)+'-'+str(max_wl)+'__'\n",
    "                         +database+'__'+abs_emi+'__'+profile_label.replace(' ','')+'.png')\n",
    "            plt.savefig(xsec_plotpath, dpi=500)\n",
    "            plt.show()\n",
    "            print('Cross sections plot has been saved:', xsec_plotpath, '\\n')\n",
    "    else:\n",
    "        raise ImportError('Please choose wavenumber or wavelength and type in correct format: wn or wl.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross sections for ExoMol database\n",
    "def exomol_cross_section(states_part_df, T, P):\n",
    "    print('Calculate cross sections.')\n",
    "    print('Running on ', ncputrans, 'cores.')\n",
    "    tot = Timer()\n",
    "    tot.start()\n",
    "    broad, ratio, nbroad, broad_dfs = read_broad(read_path)\n",
    "    Q = read_exomol_pf(read_path, T)\n",
    "    profile_label = line_profile(profile)\n",
    "    \n",
    "    print('Reading transitions and calculating cross sections ...')    \n",
    "    trans_filepaths = get_part_transfiles(read_path) \n",
    "    # Process multiple files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=ncpufiles) as executor:\n",
    "        # Submit reading tasks for each file\n",
    "        futures = [executor.submit(xsec_part_trans,states_part_df,T,P,Q,broad,ratio,nbroad,broad_dfs,\n",
    "                                   profile_label,trans_filepath) for trans_filepath in tqdm(trans_filepaths)]\n",
    "        xsec = sum([future.result() for future in futures])\n",
    "        \n",
    "    if len(xsec) == 0:\n",
    "        raise ImportError(\"Empty result with the input filter values. Please type new filter values in the input file.\")   \n",
    "    \n",
    "    tot.end()\n",
    "    print('Finished reading transitions and calculating cross sections!\\n')\n",
    "    save_xsec(wn_grid, xsec, database, profile_label) \n",
    "    print('Cross sections have been saved!\\n')\n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')\n",
    "    return xsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross sections forHITRAN database\n",
    "def hitran_cross_section(hitran_linelist_df, T, P):\n",
    "    print('Calculating cross sections ...')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "\n",
    "    Q = read_hitran_pf(T)\n",
    "    profile_label = line_profile(profile)\n",
    "    print(profile_label,'profile')\n",
    "    pool = Pool(processes=ncputrans) \n",
    "    print('Running on', ncputrans, 'cores.')\n",
    "    process = pool.apply_async(CalculateHITRANCrossSection,\n",
    "                               args=(hitran_linelist_df, T, P, Q, profile_label))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    xsec = process.get()\n",
    "    if len(xsec) == 0:\n",
    "        raise ImportError(\"Empty result with the input filter values. Please type new filter values in the input file.\")  \n",
    "    \n",
    "    t.end()  \n",
    "    print('Finished calculating cross sections!\\n')\n",
    "    save_xsec(wn_grid, xsec, database, profile_label) \n",
    "    print('Cross sections have been saved!\\n')\n",
    "    print('* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\\n')\n",
    "    return xsec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Results\n",
    "def get_results(read_path): \n",
    "    t_tot = Timer()\n",
    "    t_tot.start()  \n",
    "    # ExoMol or HITRAN\n",
    "    if database == 'ExoMol':\n",
    "        print('ExoMol database')\n",
    "        print('Molecule\\t:', molecule, '\\nIsotopologue\\t:', isotopologue, '\\nDataset\\t\\t:', dataset)\n",
    "        # All functions need whole states.\n",
    "        states_df = read_all_states(read_path)\n",
    "        # Calculate predissociation cross sections if lifetimes are not exit in the states file.\n",
    "        if CrossSections == 1 and predissocYN == 'Y' and check_predissoc+check_lifetime == 0 and 'VOI' in profile:\n",
    "            np.seterr(divide='ignore', invalid='ignore')\n",
    "            print('Calculate lifetimes.')  \n",
    "            print('Running on ', ncputrans, 'cores.')\n",
    "            t = Timer()\n",
    "            t.start()\n",
    "            print('Reading all transitions and calculating lifetimes ...')\n",
    "            trans_filepaths = get_transfiles(read_path)\n",
    "            # Process multiple files in parallel\n",
    "            with ThreadPoolExecutor(max_workers=ncpufiles) as executor:\n",
    "                # Submit reading tasks for each file\n",
    "                futures = [executor.submit(calculate_lifetime, states_df, \n",
    "                                           trans_filepath) for trans_filepath in trans_filepaths]\n",
    "                lifetime_result = 1 / sum(np.array([future.result() for future in tqdm(futures)]))\n",
    "                states_df['tau'] = np.array([f'{x:>12.4E}'.replace('INF','Inf') for x in lifetime_result])\n",
    "            t.end()\n",
    "            print('Finished reading all transitions and calculating lifetimes!\\n')\n",
    "                \n",
    "        # Only calculating stick spectra and cross sections need part of states.\n",
    "        NeedPartStates = StickSpectra + CrossSections\n",
    "        if NeedPartStates != 0:\n",
    "            if check_uncertainty == 1:\n",
    "                states_part_df = read_part_states(states_df) \n",
    "            else:\n",
    "                raise ImportError(\"No uncertainties in states file. Please do not use uncertainty filter.\")   \n",
    "            \n",
    "\n",
    "        # Functions\n",
    "        Nfunctions = (PartitionFunctions + SpecificHeats + Lifetimes + CoolingFunctions \n",
    "                      + OscillatorStrengths + Conversion + StickSpectra  + CrossSections)\n",
    "        if Nfunctions > 0:\n",
    "            if PartitionFunctions == 1:\n",
    "                exomol_partition(states_df, Ntemp, Tmax)\n",
    "            if SpecificHeats == 1:\n",
    "                exomol_specificheat(states_df, Ntemp, Tmax)\n",
    "            if Lifetimes == 1:\n",
    "                exomol_lifetime(read_path, states_df)\n",
    "            if CoolingFunctions == 1:\n",
    "                exomol_cooling(states_df, Ntemp, Tmax)\n",
    "            if OscillatorStrengths == 1:\n",
    "                exomol_oscillator_strength(states_df)\n",
    "            if (Conversion == 1 and ConversionFormat == 1):\n",
    "                conversion_exomol2hitran(states_df)\n",
    "            if StickSpectra == 1:\n",
    "                exomol_stick_spectra(states_part_df, T)\n",
    "            if CrossSections == 1:\n",
    "                exomol_cross_section(states_part_df, T, P)\n",
    "        else:   \n",
    "            raise ImportError(\"Please choose functions which you want to calculate.\")\n",
    "    elif database == 'HITRAN':\n",
    "        print('HITRAN database')\n",
    "        print('Molecule\\t:', molecule, '\\t\\t\\tMolecule ID\\t:', molecule_id, \n",
    "              '\\nIsotopologue\\t:', isotopologue, '\\t\\tIsotopologue ID\\t:', isotopologue_id,\n",
    "              '\\nDataset\\t\\t:', dataset)\n",
    "        parfile_df = read_parfile(read_path)\n",
    "        if (Conversion == 1 and ConversionFormat == 2):\n",
    "            hitran_df = read_hitran_parfile(read_path,parfile_df,ConversionMinFreq,ConversionMaxFreq,\n",
    "                                            ConversionUnc,ConversionThreshold).reset_index().drop(columns='index')\n",
    "            conversion_hitran2exomol(hitran_df)\n",
    "        # Use ExoMol functions\n",
    "        NuseExoMolFunc = PartitionFunctions+SpecificHeats+Lifetimes\n",
    "        if NuseExoMolFunc > 0:\n",
    "            read_hitran2exomol_path = save_path + 'conversion/HITRAN2ExoMol/'\n",
    "            if ConversionFormat != 2:\n",
    "                conversion_foldername = read_hitran2exomol_path+molecule+'/'+isotopologue+'/'+dataset+'/'\n",
    "                if os.path.exists(conversion_foldername):\n",
    "                    states_list = glob.glob(conversion_foldername + isotopologue + '__' + dataset + '.states.bz2')\n",
    "                    trans_list = glob.glob(conversion_foldername + isotopologue + '__' + dataset + '*.trans.bz2')\n",
    "                    if (states_list == [] and trans_list == []):\n",
    "                        hitran_df = read_hitran_parfile(read_path,parfile_df,ConversionMinFreq,ConversionMaxFreq,\n",
    "                                                        'None','None').reset_index().drop(columns='index')\n",
    "                        conversion_hitran2exomol(hitran_df)\n",
    "            # All functions need whole states.\n",
    "            states_df = read_all_states(read_hitran2exomol_path)\n",
    "            if PartitionFunctions == 1:\n",
    "                exomol_partition(states_df, Ntemp, Tmax)\n",
    "            if SpecificHeats == 1:\n",
    "                exomol_specificheat(states_df, Ntemp, Tmax)\n",
    "            if Lifetimes == 1:\n",
    "                exomol_lifetime(read_hitran2exomol_path, states_df)\n",
    "        # Use HITRAN functions\n",
    "        # Calculate cooling functions or oscillatore strengths.\n",
    "        NeedWholeHITRAN = CoolingFunctions+OscillatorStrengths\n",
    "        if NeedWholeHITRAN != 0: \n",
    "            hitran_df = read_hitran_parfile(read_path,parfile_df,min_wn,max_wn,'None','None').reset_index().drop(columns='index')\n",
    "        if CoolingFunctions == 1:\n",
    "            hitran_cooling(hitran_df, Ntemp, Tmax)\n",
    "        if OscillatorStrengths == 1:\n",
    "            hitran_oscillator_strength(hitran_df)\n",
    "        if (StickSpectra == 1 or CrossSections == 1):\n",
    "            hitran_df = read_hitran_parfile(read_path,parfile_df,min_wn,max_wn,\n",
    "                                             UncFilter,threshold).reset_index().drop(columns='index')\n",
    "            (hitran_linelist_df, QNs_col) = hitran_linelist_QN(hitran_df)\n",
    "        if StickSpectra == 1:\n",
    "            hitran_stick_spectra(hitran_linelist_df, QNs_col, T)\n",
    "        if CrossSections == 1:\n",
    "            hitran_cross_section(hitran_linelist_df, T, P)\n",
    "    else:\n",
    "        raise ImportError(\"Please add the name of the database 'ExoMol' or 'HITRAN' into the input file.\")     \n",
    "    print('\\nThe program total running time:')    \n",
    "    t_tot.end()\n",
    "    print('\\nFinished!')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExoMol database\n",
      "Molecule\t: CH4 \n",
      "Isotopologue\t: 12C-1H4 \n",
      "Dataset\t\t: MM\n",
      "Reading states ...\n",
      "Running time on CPU       : 121.40519530699999 s\n",
      "Running time on system    : 129.25678777694702 s\n",
      "Finished reading states!\n",
      "\n",
      "* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\n",
      "\n",
      "Calculate cross sections.\n",
      "Running on  16 cores.\n",
      "Reading broadening file ...\n",
      "Broadeners \t: H2, He\n",
      "Ratios \t\t: 0.75, 0.15 \n",
      "\n",
      "Reading transitions and calculating cross sections ...\n",
      "Number of all transitions files \t\t: 120\n",
      "Number of selected transitions files \t\t: 3\n",
      "Number of all decompressed transitions files \t: 3\n",
      "Number of new decompressed transitions files \t: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f206185563be4c5bac7c3d0bcc29094c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processeing transitions file: 12C-1H4__MM__00200-00300.trans\n",
      "Processeing transitions file: 12C-1H4__MM__00100-00200.trans\n",
      "Processeing transitions file: 12C-1H4__MM__00000-00100.trans\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099c9ca9004c49598c4c64f5bdcb90e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 12C-1H4__MM__00100-00200.trans:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7c2004b8a74539a89f027410e9573e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 12C-1H4__MM__00200-00300.trans:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93e12e8a64747a9b28569d37d8e8daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 12C-1H4__MM__00000-00100.trans:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time on CPU       : 2078.499328408 s\n",
      "Running time on system    : 895.0006306171417 s\n",
      "Finished reading transitions and calculating cross sections!\n",
      "\n",
      "SciPy Voigt profile\n",
      "Temperature selected      : 296    K\n",
      "Pressure selected         : 1.0    bar\n",
      "Number of points is       : 2001  \n",
      "Bin size is               : 0.1    cm⁻¹\n",
      "Cutoff is                 : 1e-27  cm⁻¹\n",
      "Uncertainty filter        : None   cm⁻¹\n",
      "Threshold filter          : 1e-40  cm⁻¹/(molecule cm⁻²)\n",
      "Wavenumber range selected : 0.0 cm⁻¹ - 200.0 cm⁻¹\n",
      "Cross sections file has been saved: /home/jingxin/data/pyexocross/xsecs/files/CH4/ExoMol/CH4__T296__wn0.0-200.0__ExoMol__Ab__SciPyVoigt.xsec \n",
      "\n",
      "Cross sections have been saved!\n",
      "\n",
      "* * * * * - - - - - * * * * * - - - - - * * * * * - - - - - * * * * *\n",
      "\n",
      "\n",
      "The program total running time:\n",
      "Running time on CPU       : 2203.340927079 s\n",
      "Running time on system    : 1028.0161912441254 s\n",
      "\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "get_results(read_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('exomol')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "722cf535e3d158a42a418157a233af576121476252bfbc7c5af47c8831a1fc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
