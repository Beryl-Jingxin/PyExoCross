{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import os\n",
    "import bz2\n",
    "import csv\n",
    "import glob\n",
    "import time\n",
    "import requests\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numexpr as ne\n",
    "from io import StringIO\n",
    "import astropy.units as au\n",
    "from itertools import chain\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import voigt_profile, wofz, erf, roots_hermite\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", np.ComplexWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=4,progress_bar=False)    # Initialize."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Input File Path\n",
    "\n",
    "\n",
    "<table><tr><td bgcolor=skyblue><font size=24> Could be changed ! </font></td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "inp_filepath = '/home/jingxin/PyExoCross/input/MgH_ExoMol.inp'\n",
    "#########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:    \n",
    "    def start(self):\n",
    "        self.start_CPU = time.process_time()\n",
    "        self.start_sys = time.time()\n",
    "        return self\n",
    "\n",
    "    def end(self, *args):\n",
    "        self.end_CPU = time.process_time()\n",
    "        self.end_sys = time.time()\n",
    "        self.interval_CPU = self.end_CPU - self.start_CPU\n",
    "        self.interval_sys = self.end_sys - self.start_sys\n",
    "        print('{:25s} : {}'.format('Running time on CPU', self.interval_CPU), 's')\n",
    "        print('{:25s} : {}'.format('Running time on system', self.interval_sys), 's')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Information from Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inp_para(inp_filepath):\n",
    "    # Find the maximum column for all the rows.\n",
    "    with open(inp_filepath, 'r') as temp_f:\n",
    "        col_count = max([len([x for x in l.split(\" \") if x.strip()]) for l in temp_f.readlines()])\n",
    "    # Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1).\n",
    "    column_names = [i for i in range(col_count)] \n",
    "    inp_df = pd.read_csv(inp_filepath, sep='\\\\s+', header = None, names=column_names, usecols=column_names)\n",
    "    col0 = inp_df[0]\n",
    "    \n",
    "    # Database\n",
    "    database = inp_df[col0.isin(['Database'])][1].values[0].upper().replace('EXOMOL','ExoMol')\n",
    "    \n",
    "    # Basic information\n",
    "    molecule = inp_df[col0.isin(['Molecule'])][1].values[0]\n",
    "    isotopologue = inp_df[col0.isin(['Isotopologue'])][1].values[0]\n",
    "    dataset = inp_df[col0.isin(['Dataset'])][1].values[0]\n",
    "    mol_iso_id = int(inp_df[col0.isin(['MolIsoID'])][1])\n",
    "    \n",
    "    # File path\n",
    "    read_path = inp_df[col0.isin(['ReadPath'])][1].values[0]\n",
    "    save_path = inp_df[col0.isin(['SavePath'])][1].values[0]\n",
    "    if os.path.exists(save_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "    # Functions \n",
    "    Conversion = int(inp_df[col0.isin(['Conversion'])][1])\n",
    "    PartitionFunctions = int(inp_df[col0.isin(['PartitionFunctions'])][1])\n",
    "    CoolingFunctions = int(inp_df[col0.isin(['CoolingFunctions'])][1])\n",
    "    Lifetimes = int(inp_df[col0.isin(['Lifetimes'])][1])\n",
    "    OscillatorStrengths = int(inp_df[col0.isin(['OscillatorStrengths'])][1])\n",
    "    SpecificHeats = int(inp_df[col0.isin(['SpecificHeats'])][1])\n",
    "    StickSpectra = int(inp_df[col0.isin(['StickSpectra'])][1])\n",
    "    CrossSections = int(inp_df[col0.isin(['CrossSections'])][1])\n",
    "    \n",
    "    # Quantum numbers\n",
    "    NeedQNs = Conversion + StickSpectra + CrossSections\n",
    "    if NeedQNs != 0:\n",
    "        QNslabel_list = list(inp_df[col0.isin(['QNslabel'])].iloc[0])[1:]\n",
    "        QNsformat_list = list(inp_df[col0.isin(['QNsformat'])].iloc[0])[1:]\n",
    "        QNslabel_list = [x for x in QNslabel_list if x == x]\n",
    "        QNsformat_list = [x for x in QNsformat_list if x == x]\n",
    "    else:\n",
    "        QNslabel_list = []\n",
    "        QNsformat_list = []  \n",
    "    \n",
    "    # Convert from one format to another\n",
    "    if Conversion != 0:\n",
    "        ConversionFormat = int(inp_df[col0.isin(['ConversionFormat'])][1])\n",
    "        ConversionMinFreq = float(inp_df[col0.isin(['ConversionFrequncyRange'])][1])\n",
    "        ConversionMaxFreq = float(inp_df[col0.isin(['ConversionFrequncyRange'])][2])\n",
    "        GlobalQNLabel_list = list(inp_df[col0.isin(['GlobalQNLabel'])].iloc[0].dropna())[1:]\n",
    "        GlobalQNFormat_list = list(inp_df[col0.isin(['GlobalQNFormat'])].iloc[0].dropna())[1:]\n",
    "        LocalQNLabel_list = list(inp_df[col0.isin(['LocalQNLabel'])].iloc[0].dropna())[1:]\n",
    "        LocalQNFormat_list = list(inp_df[col0.isin(['LocalQNFormat'])].iloc[0].dropna())[1:]\n",
    "        # Uncertainty filter\n",
    "        ConversionUncYN = inp_df[col0.isin(['ConvUncFilter(Y/N)'])][1].values[0].upper()[0]\n",
    "        if ConversionUncYN == 'Y':\n",
    "            ConversionUnc = float(inp_df[col0.isin(['ConvUncFilter(Y/N)'])][2])\n",
    "        elif ConversionUncYN == 'N':\n",
    "            ConversionUnc = 'None'\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct uncertainty filter choice 'Y' or 'N' into the input file.\")  \n",
    "        # Threshold filter\n",
    "        ConversionThresholdYN = inp_df[col0.isin(['ConvThreshold(Y/N)'])][1].values[0].upper()[0]\n",
    "        if ConversionThresholdYN == 'Y':\n",
    "            ConversionThreshold = float(inp_df[col0.isin(['ConvThreshold(Y/N)'])][2])\n",
    "        elif ConversionThresholdYN == 'N':\n",
    "            ConversionThreshold = 'None'\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct threshold choice 'Y' or 'N' into the input file.\")       \n",
    "    else:\n",
    "        ConversionFormat = 0\n",
    "        ConversionMinFreq = 0\n",
    "        ConversionMaxFreq = 1e20\n",
    "        GlobalQNLabel_list = []\n",
    "        GlobalQNFormat_list = []\n",
    "        LocalQNLabel_list = []\n",
    "        LocalQNFormat_list = []\n",
    "        ConversionUnc = 'None'\n",
    "        ConversionThreshold = 'None'\n",
    "        \n",
    "    # Calculate partition, cooling functions or specific heats \n",
    "    if PartitionFunctions + CoolingFunctions + SpecificHeats != 0:\n",
    "        Ntemp = int(inp_df[col0.isin(['Ntemp'])][1])    # The number of temperature steps\n",
    "        Tmax = int(inp_df[col0.isin(['Tmax'])][1])      # Maximal temperature in K (minimal T = 1 K )\n",
    "    else:\n",
    "        Ntemp = 0\n",
    "        Tmax = 0  \n",
    "     \n",
    "    # Calculate lifetimes \n",
    "    if Lifetimes != 0:\n",
    "        CompressYN = inp_df[col0.isin(['Compress(Y/N)'])][1].values[0].upper()[0]\n",
    "    else:\n",
    "        CompressYN = 'N'\n",
    "        \n",
    "    # Calculate oscillator strengths\n",
    "    if OscillatorStrengths != 0:\n",
    "        fgORf = inp_df[col0.isin(['fg/f'])][1].values[0].upper()\n",
    "        Ncolumns = int(inp_df[col0.isin(['Ncolumns'])][1])      # Number of columns in result file\n",
    "    else:\n",
    "        fgORf = 'FG'\n",
    "        Ncolumns = 3\n",
    "    \n",
    "    # Calculate stick spectra or cross sections \n",
    "    if StickSpectra + CrossSections != 0:\n",
    "        T = int(inp_df[col0.isin(['Temperature'])][1])\n",
    "        min_wn = float(inp_df[col0.isin(['Range'])][1])\n",
    "        max_wn = float(inp_df[col0.isin(['Range'])][2])\n",
    "        abs_emi = inp_df[col0.isin(['Absorption/Emission'])][1].values[0].upper()[0].replace('A','Ab').replace('E','Em')\n",
    "        # Uncertainty filter\n",
    "        UncFilterYN = inp_df[col0.isin(['UncFilter(Y/N)'])][1].values[0].upper()[0]\n",
    "        if UncFilterYN == 'Y':\n",
    "            UncFilter = float(inp_df[col0.isin(['UncFilter(Y/N)'])][2])\n",
    "        elif UncFilterYN == 'N':\n",
    "            UncFilter = 'None'\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct uncertainty filter choice 'Y' or 'N' into the input file.\")  \n",
    "        # Threshold filter\n",
    "        thresholdYN = inp_df[col0.isin(['Threshold(Y/N)'])][1].values[0].upper()[0]\n",
    "        if thresholdYN == 'Y':\n",
    "            threshold = float(inp_df[col0.isin(['Threshold(Y/N)'])][2])\n",
    "        elif thresholdYN == 'N':\n",
    "            threshold = 'None'\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct threshold choice 'Y' or 'N' into the input file.\") \n",
    "        # Quantum number filter\n",
    "        QNsFilterYN = inp_df[col0.isin(['QNsFilter(Y/N)'])][1].values[0].upper()[0]\n",
    "        if QNsFilterYN == 'Y':\n",
    "            QNsFilter = list(inp_df[col0.isin(['QNsFilter(Y/N)'])].iloc[0].dropna())[2:]\n",
    "            QNs_label = []\n",
    "            QNs_value = []\n",
    "            for i in range(len(QNsFilter)):\n",
    "                QNs_label.append(QNsFilter[i].split('[')[0])\n",
    "                QNs_value.append(QNsFilter[i].split('[')[1].split(']')[0].split(';'))\n",
    "            QNs_format = [QNsformat_list[j] for j in [QNslabel_list.index(i) for i in QNs_label]]\n",
    "        elif QNsFilterYN == 'N':\n",
    "            QNsFilter = []\n",
    "            QNs_label = []\n",
    "            QNs_value = []\n",
    "            QNs_format = []\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct quantum number filter choice 'Y' or 'N' into the input file.\")\n",
    "    else:\n",
    "        T = 0\n",
    "        min_wn = 0\n",
    "        max_wn = 1e20\n",
    "        abs_emi = 'None'\n",
    "        UncFilter = 'None'\n",
    "        threshold = 'None'\n",
    "        QNsFilter = []\n",
    "        QNs_label = []\n",
    "        QNs_value = []  \n",
    "        QNs_format = []\n",
    "        \n",
    "    # Stick spectra\n",
    "    if StickSpectra != 0:\n",
    "        PlotStickSpectraYN = inp_df[col0.isin(['PlotStickSpectra(Y/N)'])][1].values[0].upper()[0]\n",
    "        if PlotStickSpectraYN == 'Y':\n",
    "            _limitYaxisStick = inp_df[col0.isin(['Y-axisLimitStick'])][1].values[0]\n",
    "            if _limitYaxisStick == '#':\n",
    "                limitYaxisStick = 1e-30\n",
    "            elif pd.isnull(_limitYaxisStick) == True:\n",
    "                limitYaxisStick = 1e-30\n",
    "            else:\n",
    "                limitYaxisStick = float(_limitYaxisStick)\n",
    "    else:\n",
    "        PlotStickSpectraYN = 'None'\n",
    "        limitYaxisStick = 1e-30\n",
    "\n",
    "    # Cross sections\n",
    "    if CrossSections != 0:\n",
    "        NpointsORBinSize = inp_df[col0.isin(['Npoints/BinSize'])][1].values[0].upper()\n",
    "        if 'POI' in NpointsORBinSize:\n",
    "            N_point = int(inp_df[col0.isin(['Npoints/BinSize'])][2])\n",
    "            bin_size = float((max_wn - min_wn)/(N_point-1))\n",
    "        elif 'BIN' in NpointsORBinSize or 'SIZ' in NpointsORBinSize:\n",
    "            bin_size = float(inp_df[col0.isin(['Npoints/BinSize'])][2])\n",
    "            N_point = int((max_wn - min_wn)/bin_size+1)\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct grid choice 'Npoints' or 'BinSize' into the input file.\")\n",
    "        # Cutoff\n",
    "        cutoffYN = inp_df[col0.isin(['Cutoff(Y/N)'])][1].values[0].upper()[0]\n",
    "        if cutoffYN == 'Y':\n",
    "            cutoff = float(inp_df[col0.isin(['Cutoff(Y/N)'])][2])\n",
    "        elif cutoffYN == 'N':\n",
    "            cutoff = 'None'\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct cutoff choice 'Y' or 'N' into the input file.\")\n",
    "        # Other parameters\n",
    "        P = float(inp_df[col0.isin(['Pressure'])][1])\n",
    "        broadeners = list(inp_df[col0.isin(['Broadeners'])].iloc[0])[1:]\n",
    "        broadeners = [i for i in broadeners if i is not np.nan]\n",
    "        ratios = np.array(list(inp_df[col0.isin(['Ratios'])].iloc[0])[1:], dtype=float)\n",
    "        ratios = ratios[~np.isnan(ratios)]\n",
    "        wn_grid = np.linspace(min_wn, max_wn, N_point)\n",
    "        wn_wl = inp_df[col0.isin(['Wavenumber(wn)/wavelength(wl)'])][1].values[0].upper()\n",
    "        profile = inp_df[col0.isin(['Profile'])][1].values[0].upper().replace('PRO','')\n",
    "        # Doppler HWHM\n",
    "        DopplerHWHMYN = inp_df[col0.isin(['DopplerHWHM(Y/N)'])][1].values[0].upper()[0]        \n",
    "        if 'DOP' in profile: \n",
    "            alpha_HWHM = 'None'\n",
    "        elif 'GAU' in profile:\n",
    "            if DopplerHWHMYN == 'Y':\n",
    "                alpha_HWHM = float(inp_df[col0.isin(['DopplerHWHM(Y/N)'])][2])\n",
    "            else:\n",
    "                raise ImportError(\"Gaussian line profile requires a HWHM. \" \n",
    "                                  + \"Please choose 'Y' and give a value for Doppler HWHM in the input file. \" \n",
    "                                  + \"Otherwise, please choose Doppler line profile \" \n",
    "                                  + \"(with calculated temperature-dependent Doppler HWHM).\")\n",
    "        elif 'VOI' in profile:\n",
    "            if DopplerHWHMYN == 'Y':\n",
    "                alpha_HWHM = float(inp_df[col0.isin(['DopplerHWHM(Y/N)'])][2])\n",
    "            elif DopplerHWHMYN == 'N':\n",
    "                alpha_HWHM = 'None'\n",
    "            else:\n",
    "                raise ImportError(\"Please type the correct Doppler HWHM choice 'Y' or 'N' into the input file.\")\n",
    "        else:\n",
    "            alpha_HWHM = 'None'\n",
    "        # Lorentzian HWHM \n",
    "        LorentzianHWHMYN = inp_df[col0.isin(['LorentzianHWHM(Y/N)'])][1].values[0].upper()[0]  \n",
    "        if LorentzianHWHMYN == 'Y':\n",
    "            gamma_HWHM = float(inp_df[col0.isin(['LorentzianHWHM(Y/N)'])][2])\n",
    "        elif LorentzianHWHMYN == 'N':\n",
    "            gamma_HWHM = 'None'\n",
    "        else:\n",
    "            raise ImportError(\"Please type the correct Lorentzian HWHM choice 'Y' or 'N' into the input file.\")\n",
    "        # Plot \n",
    "        PlotCrossSectionYN = inp_df[col0.isin(['PlotCrossSection(Y/N)'])][1].values[0].upper()[0]  \n",
    "        if PlotCrossSectionYN == 'Y':\n",
    "            _limitYaxisXsec = inp_df[col0.isin(['Y-axisLimitXsec'])][1].values[0]\n",
    "            if _limitYaxisXsec == '#':\n",
    "                limitYaxisXsec = 1e-30\n",
    "            elif pd.isnull(_limitYaxisXsec) == True:\n",
    "                limitYaxisXsec = 1e-30\n",
    "            else:\n",
    "                limitYaxisXsec = float(_limitYaxisXsec)\n",
    "    else:\n",
    "        bin_size = 'None'\n",
    "        N_point = 'None'\n",
    "        cutoff = 'None'         \n",
    "        alpha_HWHM = 'None'        \n",
    "        gamma_HWHM = 'None'\n",
    "        broadeners = []\n",
    "        ratios = np.array([])\n",
    "        P = 0\n",
    "        wn_grid = np.linspace(0,1,1)\n",
    "        profile = 'None'\n",
    "        wn_wl = 'None'\n",
    "        PlotCrossSectionYN = 'None'\n",
    "        limitYaxisXsec = 1e-30\n",
    "\n",
    "    # Molecule and isotopologue ID, abundance, mass uncertainty, lifetime and g-factor           \n",
    "    molecule_id = int(mol_iso_id/10)\n",
    "    isotopologue_id = mol_iso_id - molecule_id * 10\n",
    "    if database == 'ExoMol':\n",
    "        # Read ExoMol definition file (.def) to get the mass.\n",
    "        deffile_path = (read_path+'/'+molecule+'/'+isotopologue+'/'+dataset+'/'+isotopologue+'__'+dataset+'.def')\n",
    "        def_df = pd.read_csv(deffile_path,sep='\\\\s+',usecols=[0,1,2,3,4],names=['0','1','2','3','4'],header=None)\n",
    "        abundance = 1\n",
    "        mass = float(def_df[def_df['4'].isin(['mass'])]['0'].values[0])     # ExoMol mass (Dalton)\n",
    "        if def_df.to_string().find('Uncertainty') != -1:\n",
    "            check_uncertainty = int(def_df[def_df['2'].isin(['Uncertainty'])]['0'].values[0])\n",
    "        else:\n",
    "            check_uncertainty = 0\n",
    "        if def_df.to_string().find('Predissociative') != -1:\n",
    "            check_predissoc = int(def_df[def_df['2'].isin(['Predissociative'])]['0'].values[0])\n",
    "        else:\n",
    "            check_predissoc = 0\n",
    "        check_lifetime = int(def_df[def_df['2'].isin(['Lifetime'])]['0'].values[0])\n",
    "        check_gfactor = int(def_df[def_df['3'].isin(['g-factor'])]['0'].values[0])\n",
    "    elif database == 'HITRAN':\n",
    "        isometa_url = 'https://hitran.org/docs/iso-meta/'\n",
    "        iso_meta_table = pd.read_html(isometa_url)[molecule_id - 1]\n",
    "        iso_meta_row = iso_meta_table[iso_meta_table['local ID'].isin([isotopologue_id])]\n",
    "        abundance = float(iso_meta_row['Abundance'][0].replace('\\xa0×\\xa010','E'))\n",
    "        mass = float(iso_meta_row['Molar Mass /g·mol-1'])                   # HITRAN molar mass (g/mol)\n",
    "        check_uncertainty = 0\n",
    "        check_predissoc = 0\n",
    "        check_lifetime = 0\n",
    "        check_gfactor = 0\n",
    "    else:\n",
    "        raise ImportError(\"Please add the name of the database 'ExoMol' or 'HITRAN' into the input file.\")\n",
    "    \n",
    "    return (database, molecule, isotopologue, dataset, read_path, save_path, \n",
    "            Conversion, PartitionFunctions, SpecificHeats, CoolingFunctions, Lifetimes, OscillatorStrengths, StickSpectra, CrossSections,\n",
    "            ConversionFormat, ConversionMinFreq, ConversionMaxFreq, ConversionUnc, ConversionThreshold, \n",
    "            GlobalQNLabel_list, GlobalQNFormat_list, LocalQNLabel_list, LocalQNFormat_list,\n",
    "            Ntemp, Tmax, CompressYN, fgORf, Ncolumns, broadeners, ratios, T, P, min_wn, max_wn, N_point, bin_size, wn_grid, \n",
    "            cutoff, threshold, UncFilter, QNslabel_list, QNsformat_list, QNs_label, QNs_value, QNs_format, QNsFilter, \n",
    "            alpha_HWHM, gamma_HWHM, abs_emi, profile, wn_wl, molecule_id, isotopologue_id, abundance, mass,\n",
    "            check_uncertainty, check_lifetime, check_gfactor, check_predissoc, \n",
    "            PlotStickSpectraYN, limitYaxisStick, PlotCrossSectionYN, limitYaxisXsec)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for calculating\n",
    "import astropy.constants as ac\n",
    "#from astropy import constants, units as ac, au\n",
    "Tref = 296.0                        # Reference temperature is 296 K\n",
    "Pref = 1.0                          # Reference pressure is 1 bar\n",
    "N_A = ac.N_A.value                  # Avogadro number (1/mol)\n",
    "h = ac.h.to('erg s').value          # Planck's const (erg s)\n",
    "c = ac.c.to('cm/s').value           # Velocity of light (cm/s)\n",
    "kB = ac.k_B.to('erg/K').value       # Boltzmann's const (erg/K)\n",
    "R = ac.R.to('J / (K mol)').value    # Molar gas constant (J/(K mol))\n",
    "c2 = h * c / kB                     # Second radiation constant (cm K)\n",
    "\n",
    "(database, molecule, isotopologue, dataset, read_path, save_path, \n",
    " Conversion, PartitionFunctions, SpecificHeats, CoolingFunctions, Lifetimes, OscillatorStrengths, StickSpectra, CrossSections,\n",
    " ConversionFormat, ConversionMinFreq, ConversionMaxFreq, ConversionUnc, ConversionThreshold, \n",
    " GlobalQNLabel_list, GlobalQNFormat_list, LocalQNLabel_list, LocalQNFormat_list,\n",
    " Ntemp, Tmax, CompressYN, fgORf, Ncolumns, broadeners, ratios, T, P, min_wn, max_wn, N_point, bin_size, wn_grid, \n",
    " cutoff, threshold, UncFilter, QNslabel_list, QNsformat_list, QNs_label, QNs_value, QNs_format, QNsFilter, \n",
    " alpha_HWHM, gamma_HWHM, abs_emi, profile, wn_wl, molecule_id, isotopologue_id, abundance, mass, \n",
    " check_uncertainty, check_lifetime, check_gfactor, check_predissoc, \n",
    " PlotStickSpectraYN, limitYaxisStick, PlotCrossSectionYN, limitYaxisXsec) = inp_para(inp_filepath)\n",
    "\n",
    "# Constants\n",
    "c2InvTref = c2 / Tref                 # c2 / T_ref (cm)\n",
    "PI = np.pi\n",
    "ln22 = np.log(2)*2\n",
    "sinPI = np.sin(np.pi)\n",
    "SqrtPI = np.sqrt(np.pi)\n",
    "Sqrtln2 = np.sqrt(np.log(2))\n",
    "OneminSqrtPIln2 = 1 - np.sqrt(np.pi * np.log(2))\n",
    "Negln2 = -np.log(2)\n",
    "PI4c = np.pi * 4 * c\n",
    "Inv8Pic = 1 / (8 * np.pi * c)         # 8 * pi * c (s/cm)\n",
    "Inv4Pi = 1 / (4 * np.pi)\n",
    "Inv2ln2 = 1 / (2 * np.log(2))\n",
    "InvSqrt2 = 1 / np.sqrt(2)\n",
    "InvSqrtPi= 1 / np.sqrt(np.pi)\n",
    "InvSprtln2 = 1 / np.sqrt(np.log(2))\n",
    "InvSqrt2Pi = 1 / np.sqrt(2 * np.pi)\n",
    "InvSqrt2ln2 = 1 / np.sqrt(2 * np.log(2))\n",
    "TwoSqrt2ln2 = 2 * np.sqrt(2 * np.log(2))\n",
    "Sqrtln2InvPi = np.sqrt(np.log(2) / np.pi)\n",
    "Sqrt2NAkBln2mInvc = np.sqrt(2 * N_A * kB * np.log(2) / mass) / c\n",
    "if bin_size != 'None':\n",
    "    binSize2 = bin_size * 2\n",
    "    binSizePI = bin_size * np.pi\n",
    "    binSizeHalf = bin_size / 2 \n",
    "    InvbinSizePIhalf = 1 / (bin_size * np.pi**0.5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert frequency, upper and lower energy and J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate frequency\n",
    "def cal_v(Ep, Epp):\n",
    "    v = ne.evaluate('Ep - Epp')\n",
    "    return(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate upper state energy with ExoMol database\n",
    "def cal_Ep(Epp, v):\n",
    "    Ep = ne.evaluate('Epp + v')\n",
    "    return(Ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate upper state energy with HITRAN database\n",
    "def cal_Ep_hitran(hitran_df):\n",
    "    Epp = hitran_df['Epp'].values\n",
    "    v = hitran_df['v'].values\n",
    "    Ep = cal_Ep(Epp, v)\n",
    "    Ep_df = pd.DataFrame(Ep,columns=['Ep'])\n",
    "    return(Ep_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate upper J\n",
    "def cal_Jp(Fp, Fpp, Jpp):\n",
    "    Jp = ne.evaluate('Fp + Fpp - Jpp')\n",
    "    return(Jp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F\n",
    "def cal_F(g):\n",
    "    F = ne.evaluate('(g - 1) * 0.5')\n",
    "    return(F)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Input Files\n",
    "\n",
    "Read the parameters of the linelist in ExoMol or HITRAN format text file. Return the dataframe of the data for the following calculations.\n",
    "\n",
    "## Read ExoMol Database Files\n",
    "\n",
    "### Read States File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_states(read_path):\n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    print('Reading states ...')\n",
    "    s_df = dict()\n",
    "    states_df = pd.DataFrame()\n",
    "    states_filenames = glob.glob(read_path + molecule + '/' + isotopologue + '/' + dataset \n",
    "                                 + '/' + isotopologue + '__' + dataset + '.states.bz2')\n",
    "    for states_filename in states_filenames:\n",
    "        s_df[states_filename] = pd.read_csv(states_filename, compression='bz2', sep='\\s+', header=None,\n",
    "                                            chunksize=1_000_000, iterator=True, low_memory=False, dtype=object)\n",
    "        for chunk in s_df[states_filename]:\n",
    "            states_df = pd.concat([states_df, chunk])\n",
    "    if check_uncertainty == 1:\n",
    "        states_df = states_df.rename(columns={0:'id',1:'E',2:'g',3:'J',4:'unc'})\n",
    "    else:      \n",
    "        states_df = states_df.rename(columns={0:'id',1:'E',2:'g',3:'J'})  \n",
    "    t.end()     \n",
    "    print('Finished reading states!\\n')                       \n",
    "    return(states_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# states_df = read_all_states(read_path)\n",
    "# states_df "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read transitions File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transfiles(read_path):\n",
    "    # Get all the transitions files from the folder including the older version files which are named by vn(version number).\n",
    "    trans_filepaths_all = glob.glob(read_path + molecule + '/' + isotopologue + '/' + dataset + '/' + '*trans.bz2')\n",
    "    num_transfiles_all = len(trans_filepaths_all)    # The number of all transitions files including the older version files.\n",
    "    trans_filepaths = []    # The list of the lastest transitions files.\n",
    "    for i in range(num_transfiles_all):\n",
    "        split_version = trans_filepaths_all[i].split('__')[-1].split('.')[0].split('_')    # Split the filenames.\n",
    "        num = len(split_version)\n",
    "        # There are four format filenames.\n",
    "        # The lastest transitions files named in two formats:\n",
    "        # 1. Filenames are named with the name of isotopologue and dataset. \n",
    "        #    End with .trans.bz2.\n",
    "        #    e.g. 14N-16O__XABC.trans.bz2'\n",
    "        # 2. Filenames are named with the name of isotopologue and dataset. \n",
    "        #    Also have the range of wavenumbers xxxxx-yyyyy.\n",
    "        #    End with .trans.bz2.\n",
    "        #    e.g. 1H2-16O__POKAZATEL__00000-00100.trans.bz2\n",
    "        # 3. The older version transitions files are named with vn(version number) based on the first format of the lastest files.\n",
    "        #    e.g. 14N-16O__XABC_v2.trans.bz2\n",
    "        # 4. The older version transitions files are named with updated date (yyyymmdd).\n",
    "        #    e.g. 1H3_p__MiZATeP__20170330.trans.bz2\n",
    "        # After split the filenames:\n",
    "        # The first format filenames only leave the dataset name, e.g. XABC.\n",
    "        # The second format filenames only leave the range of the wavenumber, e.g. 00000-00100.\n",
    "        # The third format filenames leave two parts(dataset name and version number), e.g. XABC and v2.\n",
    "        # The fourth format filenames only leave the updated date, e.g. 20170330.\n",
    "        # This program only process the lastest data, so extract the filenames named by the first two format.\n",
    "        if num == 1:     \n",
    "            if split_version[0] == dataset:        \n",
    "                trans_filepaths.append(trans_filepaths_all[i])\n",
    "            if len(split_version[0].split('-')) == 2:\n",
    "                trans_filepaths.append(trans_filepaths_all[i])\n",
    "    return(trans_filepaths)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_trans(read_path):\n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    print('Reading all transitions ...')\n",
    "    t_df = dict()\n",
    "    all_trans_df = pd.DataFrame()\n",
    "    trans_filepaths = get_transfiles(read_path)\n",
    "    for trans_filename in tqdm(trans_filepaths, position=0, leave=True, desc='Reading transitions'):\n",
    "        t_df[trans_filename] = pd.read_csv(trans_filename, compression='bz2', sep='\\s+', header=None,\n",
    "                                           chunksize=1_000_000, iterator=True, low_memory=False)\n",
    "        for chunk in t_df[trans_filename]:\n",
    "            all_trans_df = pd.concat([all_trans_df,chunk])\n",
    "    ncolumn = len(all_trans_df.columns)\n",
    "    if ncolumn == 3: \n",
    "        trans_col_name={0:'u', 1:'l', 2:'A'}\n",
    "    else:\n",
    "        trans_col_name={0:'u', 1:'l', 2:'A', 3:'v'}\n",
    "    all_trans_df = all_trans_df.rename(columns=trans_col_name) \n",
    "    t.end()                \n",
    "    print('Finished reading all transitions!\\n')                         \n",
    "    return(all_trans_df, ncolumn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(all_trans_df, ncolumn) = read_all_trans(read_path)\n",
    "#all_trans_df, ncolumn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Partition Function File From ExoMol Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read partition function with online webpage\n",
    "def read_exomolweb_pf(T):\n",
    "    pf_url = ('http://www.exomol.com/db/' + molecule + '/' + isotopologue + '/' + dataset \n",
    "              + '/' + isotopologue + '__' + dataset + '.pf')\n",
    "    pf_content = requests.get(pf_url).text\n",
    "    pf_col_name = ['T', 'Q']\n",
    "    pf_df = pd.read_csv(StringIO(pf_content), sep='\\\\s+', names=pf_col_name, header=None)\n",
    "    Q = pf_df['Q'][T-1]\n",
    "    return(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read partition function with local partition function file\n",
    "def read_exomol_pf(read_path, T):\n",
    "    pf_filename = (read_path + molecule + '/' + isotopologue + '/' + dataset \n",
    "                   + '/' + isotopologue + '__' + dataset + '.pf')\n",
    "    pf_col_name = ['T', 'Q']\n",
    "    pf_df = pd.read_csv(pf_filename, sep='\\\\s+', names=pf_col_name, header=None)\n",
    "    Q = pf_df['Q'][T-1]\n",
    "    return(Q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Broadening File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_broad(read_path):\n",
    "    broad_df = pd.DataFrame()\n",
    "    broad_dfs = []\n",
    "    broad = []\n",
    "    ratio = []\n",
    "    for i in range(len(ratios)):\n",
    "        if ratios[i] != 0.0:\n",
    "            if broadeners[i].upper()[0:3] == 'DEF':\n",
    "                default_gamma_L = 0.07\n",
    "                default_n_air = 0.5\n",
    "                broad_df = pd.DataFrame([['code', default_gamma_L, default_n_air,'Jpp']])\n",
    "                broad_df = broad_df.rename(columns={0:'code', 1:'gamma_L', 2:'n_air', 3:'Jpp'})\n",
    "                broad_dfs.append(broad_df)\n",
    "            else:\n",
    "                broadener_name = str(broadeners[i])\n",
    "                pattern_broadener = read_path + molecule + '/**/*' + broadener_name + '.broad'\n",
    "                if glob.glob(pattern_broadener, recursive=True) != []:\n",
    "                    for fname_broadener in glob.glob(pattern_broadener, recursive=True):\n",
    "                        broad_df = pd.read_csv(fname_broadener, sep='\\s+', header=None, engine='python')\n",
    "                        broad_df = broad_df.rename(columns={0:'code', 1:'gamma_L', 2:'n_air', 3:'Jpp'})\n",
    "                        broad_dfs.append(broad_df)\n",
    "                else:\n",
    "                    raise ImportError('The ' + broadener_name + ' boradening file does not exist.') \n",
    "            broad.append(broadeners[i])\n",
    "            ratio.append(ratios[i])\n",
    "    nbroad = len(broad)\n",
    "    broad = list(i for i in broad if i==i)\n",
    "    ratio = list(i for i in ratio if i==i)\n",
    "    print('Broadeners \\t:', str(broad).replace('[','').replace(']','').replace(\"'\",''))\n",
    "    print('Ratios \\t\\t:', str(ratio).replace('[','').replace(']',''))\n",
    "    return(broad, ratio, nbroad, broad_dfs)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum number filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QNfilter_linelist(linelist_df, QNs_value, QNs_label):\n",
    "    for i in range(len(QNs_label)):\n",
    "        if QNs_value[i] != ['']:\n",
    "            linelist_df[i] = linelist_df[[QNs_label[i]+\"'\",QNs_label[i]+'\"']].agg(','.join, axis=1).astype(str).str.replace(' ', '')\n",
    "            uval = linelist_df[QNs_label[i]+\"'\"].astype(str).str.replace(' ', '').drop_duplicates().values\n",
    "            lval = linelist_df[QNs_label[i]+'\"'].astype(str).str.replace(' ', '').drop_duplicates().values\n",
    "            vallist = []\n",
    "            ulist = []\n",
    "            llist = []\n",
    "            for qnval in QNs_value[i]:\n",
    "                if '' not in qnval.split(','):\n",
    "                    vallist.append(qnval)\n",
    "                elif qnval.split(',')[0] == '':\n",
    "                    ulist.append([qnval.replace(\",\",val+\",\") for val in uval])\n",
    "                elif qnval.split(',')[1] == '':\n",
    "                    llist.append([qnval.replace(\",\",\",\"+val) for val in lval])\n",
    "            QNs_value[i] = vallist+list(chain(*ulist))+list(chain(*llist))\n",
    "            linelist_df = linelist_df[linelist_df[i].isin(QNs_value[i])].drop(columns=[i])   \n",
    "    linelist_df = linelist_df.sort_values('v')  \n",
    "    return(linelist_df)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read HITRAN Database Files\n",
    "\n",
    "### Read HITRAN Linelist File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parfile(read_path):\n",
    "    if not os.path.exists(read_path):\n",
    "        raise ImportError('The input file ' + read_path + ' does not exist.')\n",
    "    # Initialise the iterator object.\n",
    "    read_par = pd.read_csv(read_path, chunksize=1_000_000, iterator=True, header=None, encoding='utf-8')\n",
    "    par_df = pd.DataFrame()\n",
    "    for chunk in read_par:\n",
    "        par_df = pd.concat([par_df, chunk])\n",
    "    return(par_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process HITRAN linelist data\n",
    "def read_hitran_parfile(read_path, parfile_df, minv, maxv, unclimit, Slimit):\n",
    "    '''\n",
    "    Read the parameters of the molecular absorption features\n",
    "    of HITRAN2020 format text file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    par_filepath : str\n",
    "        Input file path for reading.\n",
    "    Return\n",
    "    ------\n",
    "    hitran_df : DataFrame\n",
    "        The DataFrame of HITRAN data for the molecule.\n",
    "    '''    \n",
    "    par_filename = read_path.split('/')[-1]\n",
    "    if (len(str(parfile_df[0][0])) < 160):\n",
    "        raise ImportError('The file ' + par_filename + ' is not a HITRAN2020 format data file.')\n",
    "    #hitran_column_name = ['M','I','v','S','Acoeff','gamma_air','gamma_self',\n",
    "    #                     'Epp','n_air','delta_air','Vp','Vpp','Qp','Qpp',\n",
    "    #                     'Ierr','Iref','flag','gp','gpp']\n",
    "\n",
    "    hitran_df = pd.DataFrame()\n",
    "    hitran_df['M'] = pd.to_numeric(parfile_df[0].map(lambda x: x[0:2]), errors='coerce').astype('int64')                 # Molecule identification number\n",
    "    hitran_df['I'] = pd.to_numeric(parfile_df[0].map(lambda x: x[2:3]), errors='coerce').astype('int64')                 # Isotopologue number\n",
    "    hitran_df['v'] = pd.to_numeric(parfile_df[0].map(lambda x: x[3:15]), errors='coerce').astype('float64')              # Transition wavenumber (in cm^{-1})\n",
    "    hitran_df['S'] = pd.to_numeric(parfile_df[0].map(lambda x: x[15:25]), errors='coerce').astype('float64')             # Intensity (cm^{-1} / (molecule cm^{-2}))\n",
    "    hitran_df['A'] = pd.to_numeric(parfile_df[0].map(lambda x: x[25:35]), errors='coerce').astype('float64')             # The Einstein-A coefficient (s^{-1}) of a transition\n",
    "    hitran_df['gamma_air'] = pd.to_numeric(parfile_df[0].map(lambda x: x[35:40]), errors='coerce').astype('float64')     # Air-broadened half-width at half maximum (HWHM) coefficient (cm^{-1} atm^{-1})\n",
    "    hitran_df['gamma_self'] = pd.to_numeric(parfile_df[0].map(lambda x: x[40:45]), errors='coerce').astype('float64')    # Self-broadened half-width at half maximum (HWHM) coefficient (cm^{-1} atm^{-1})\n",
    "    hitran_df['Epp'] = pd.to_numeric(parfile_df[0].map(lambda x: x[45:55]), errors='coerce').astype('float64')           # Lower state energy (cm^{-1})\n",
    "    hitran_df['n_air'] = pd.to_numeric(parfile_df[0].map(lambda x: x[55:59]), errors='coerce').astype('float64')         # Temperature-dependent exponent for gamma_air\n",
    "    hitran_df['delta_air'] = pd.to_numeric(parfile_df[0].map(lambda x: x[59:67]), errors='coerce').astype('float64')     # Air pressure_include line shift (cm^{-1} atm^{-1})\n",
    "    hitran_df['Vp'] = parfile_df[0].map(lambda x: x[67:82])                                                              # Upper-state \"global\" quanta\n",
    "    hitran_df['Vpp'] = parfile_df[0].map(lambda x: x[82:97])                                                             # Lower-state \"global\" quanta\n",
    "    hitran_df['Qp'] = parfile_df[0].map(lambda x: x[97:112])                                                             # Upper-state \"local\" quanta\n",
    "    hitran_df['Qpp'] = parfile_df[0].map(lambda x: x[112:127])                                                           # Lower-state \"local\" quanta\n",
    "    #hitran_df['Unc'] = parfile_df[0].map(lambda x: x[127:128])                                                          # Uncertainty code, first integer in the error code\n",
    "    hitran_df['Unc'] = pd.to_numeric(parfile_df[0].map(lambda x: x[127:128]), errors='coerce').astype('int64')           # Uncertainty code, first integer in the error code\n",
    "    hitran_df['gp'] = pd.to_numeric(parfile_df[0].map(lambda x: x[146:153]), errors='coerce').astype('int64')            # Statistical weight of the upper state\n",
    "    hitran_df['gpp'] = pd.to_numeric(parfile_df[0].map(lambda x: x[153:160]), errors='coerce').astype('int64')           # Statistical weight of the lower state\n",
    "    \n",
    "    hitran_df = hitran_df[hitran_df['M'].isin([molecule_id])]\n",
    "    hitran_df = hitran_df[hitran_df['I'].isin([isotopologue_id])]\n",
    "    hitran_df = hitran_df[hitran_df['v'].between(minv, maxv)]\n",
    "    if unclimit != 'None':\n",
    "        hitran_df = hitran_df[hitran_df['Unc'] >= int(('%e' % unclimit)[-1])]\n",
    "    if Slimit != 'None':\n",
    "        hitran_df = hitran_df[hitran_df['S'] >= Slimit]\n",
    "    return(hitran_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Partition Function File From HITRANOnline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read partition function file from HITRANOnline\n",
    "def read_hitran_pf(T):\n",
    "    isometa_url = 'https://hitran.org/docs/iso-meta/'\n",
    "    iso_meta_table = pd.read_html(isometa_url)[molecule_id - 1]\n",
    "    iso_meta_row = iso_meta_table[iso_meta_table['local ID'].isin([isotopologue_id])]\n",
    "    #Q_ref = float(iso_meta_row.loc[0][6].replace('\\xa0×\\xa010','E'))\n",
    "    Q_url = 'https://hitran.org/data/Q/' + iso_meta_row.loc[0][7]\n",
    "    Q_content = requests.get(Q_url).text\n",
    "    Q_col_name = ['T', 'Q']\n",
    "    Q_df = pd.read_csv(StringIO(Q_content), sep='\\\\s+', names=Q_col_name, header=None)\n",
    "    Q = Q_df['Q'][T - 1]          \n",
    "    return(Q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process HITRAN quantum numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global quantum numbers\n",
    "def globalQNclasses(molecule,isotopologue):\n",
    "    globalQNclass1a = {'class':['CO','HF','HBr','HI','N2','NO+','NO_p','H2','CS'],\n",
    "                       'label': ['none','v1'],\n",
    "                       'format':['%13s','%2d']}\n",
    "    globalQNclass1b = {'class':['O2','NO','OH','ClO','SO'],\n",
    "                       'label':['none','X','Omega','none','v1'],\n",
    "                       'format':['%6s','%2s','%3s','%2s','%2d']}\n",
    "    globalQNclass2a = {'class':['CO2'],\n",
    "                       'label':['none','v1','v2','l2','v3','r'],\n",
    "                       'format':['%6s','%2d','%2d','%2d','%2d','%1d']}\n",
    "    globalQNclass2b = {'class':['N2O','OCS','HCN','CS2'],\n",
    "                       'label':['none','v1','v2','l2','v3'],\n",
    "                       'format':['%7s','%2d','%2d','%2d','%2d']}\n",
    "    globalQNclass3  = {'class':['H2O','O3','SO2','NO2','HOCl','H2S','HO2','HOBr'],\n",
    "                       'label':['none','v1','v2','v3'],\n",
    "                       'format':['%9s','%2d','%2d','%2d']}\n",
    "    globalQNclass4a = {'class':['15N-1H3','PH3','NF3'],\n",
    "                       'label':['none','v1','v2','v3','v4','S'],\n",
    "                       'format':['%5s','%2d','%2d','%2d','%2d','%2s']}\n",
    "    globalQNclass4b = {'class':['14N-1H3'],\n",
    "                       'label':['none','v1','v2','v3','v4','none','l3','l4','none','l','none','Gvib'],\n",
    "                       'format':['%1s','%1d','%1d','%1d','%1d','%1s','%1d','%1d','%1s','%1d','%1s','%4s']}\n",
    "    globalQNclass5a = {'class':['C2H2'],\n",
    "                       'label':['none','v1','v2','v3','v4','v5','l4','l5','+-','none','S'],\n",
    "                       'format':['%1s','%1d','%1d','%1d','%2d','%2d','%2d','%2d','%1s','%1s','%1s']}\n",
    "    globalQNclass5b = {'class':['C4H2'],\n",
    "                       'label':['none','v1','v2','v3','v4','v5','v6','v7','v8','v9','none','Sym','none','S'],\n",
    "                       'format':['%1s','%1d','%1d','%1d','%1d','%1d','%1d','%1d','%1d','%1d','%1s','%1s','%1s','%2s']}\n",
    "    globalQNclass5c = {'class':['HC3N'],\n",
    "                       'label':['none','v1','v2','v3','v4','v5','v6','v7','l5','l6','l7'],\n",
    "                       'format':['%2s','%1d','%1d','%1d','%1d','%1d','%1d','%1d','%2d','%2d','%2d']}\n",
    "    globalQNclass5d = {'class':['C2N2'],\n",
    "                       'label':['v1','v2','v3','v4','v5','l','+-','r','S'],\n",
    "                       'format':['%2d','%2d','%2d','%2d','%2d','%2d','%1s','%1d','%1s']}\n",
    "    globalQNclass6a = {'class':['H2CO','COF2','COCl2'],\n",
    "                       'label':['none','v1','v2','v3','v4','v5','v6'],\n",
    "                       'format':['%3s','%2d','%2d','%2d','%2d','%2d','%2d']}\n",
    "    globalQNclass6b = {'class':['H2O2'],\n",
    "                       'label':['none','v1','v2','v3','n','r','v5','v6'],\n",
    "                       'format':['%3s','%2d','%2d','%2d','%1d','%1d','%2d','%2d']}\n",
    "    globalQNclass7  = {'class':['SO3'],\n",
    "                       'label':['v1','v2','v3','l3','v4','l4','Gvib'],\n",
    "                       'format':['%2d','%2d','%2d','%2d','%2d','%2d','%3s']}\n",
    "    globalQNclass8  = {'class':['12C-1H4','13C-1H4','CF4','GeH4'],\n",
    "                       'label':['none','v1','v2','v3','v4','n','C'],\n",
    "                       'format':['%3s','%2d','%2d','%2d','%2d','%2s','%2s']}\n",
    "    globalQNclass9  = {'class':['12C-1H3-2H','13C-1H3-2H','HNO3','CH3Cl','C2H6','SF6','HCOOH','ClONO2','C2H4','CH3OH','CH3Br','CH3CN','CH3F','CH3I'],\n",
    "                       'label':['vibband'],\n",
    "                       'format':['%15s']}\n",
    "    globalQNclass = [globalQNclass1a,globalQNclass1b,globalQNclass2a,globalQNclass2b,globalQNclass3,\n",
    "                     globalQNclass4a,globalQNclass4b,globalQNclass5a,globalQNclass5b,globalQNclass5c,globalQNclass5d,\n",
    "                     globalQNclass6a,globalQNclass6b,globalQNclass7,globalQNclass8,globalQNclass9]\n",
    "    count = 0\n",
    "    for gQNclass in globalQNclass:\n",
    "        if molecule in gQNclass.get('class'):\n",
    "            GlobalQNLabels = gQNclass.get('label')\n",
    "            GlobalQNFormats = gQNclass.get('format')   \n",
    "        elif isotopologue in gQNclass.get('class'):\n",
    "            GlobalQNLabels = gQNclass.get('label')\n",
    "            GlobalQNFormats = gQNclass.get('format')\n",
    "        else:\n",
    "            count += 1\n",
    "    if count == 16:\n",
    "        GlobalQNLabels = GlobalQNLabel_list\n",
    "        GlobalQNFormats = GlobalQNFormat_list\n",
    "    return(GlobalQNLabels,GlobalQNFormats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local quantum numbers\n",
    "def localQNgroups(molecule,isotopologue):\n",
    "    localQNgroup1  = {'group':['H2O','O3','SO2','NO2','HNO3','H2CO','HOCl','H2O2','COF2','H2S','HCOOH','HO2','ClONO2','HOBr','C2H4','COCl2'],\n",
    "                      'ulabel': ['J','Ka','Kc','F','Sym'],\n",
    "                      'uformat':['%3d','%3d','%3d','%5s','%1s'],\n",
    "                      'llabel': ['J','Ka','Kc','F','Sym'],\n",
    "                      'lformat':['%3d','%3d','%3d','%5s','%1s']}\n",
    "    localQNgroup2a = {'group':['CO2','N2O','CO','HF','HCl','HBr','HI','OCS','N2','HCN','NO+','NO_p','HC3N','H2','CS','C2N2','CS2'],\n",
    "                      'ulabel':['m','none','F'],\n",
    "                      'uformat':['%1s','%9s','%5s'],\n",
    "                      'llabel': ['none','Br','J','Sym','F'],\n",
    "                      'lformat':['%5s','%1s','%3d','%1s','%5s']}\n",
    "    localQNgroup2b = {'group':['C4H2'],\n",
    "                      'ulabel':['l6','l7','l8','l9','none'],\n",
    "                      'uformat':['%2s','%2s','%2s','%2s','%7s'],\n",
    "                      'llabel': ['l6','l7','l8','l9','none','Br','J','Sym','none'],\n",
    "                      'lformat':['%2s','%2s','%2s','%2s','%1s','%1s','%3d','%1s','%1s']}\n",
    "    localQNgroup3  = {'group':['12C-1H4','13C-1H4','SF6','CF4','GeH4'],\n",
    "                      'ulabel':['none','J','C','alpha','F'],\n",
    "                      'uformat':['%2s','%3d','%2s','%3d','%5s'],\n",
    "                      'llabel': ['none','J','C','alpha','F'],\n",
    "                      'lformat':['%2s','%3d','%2s','%3d','%5s']}\n",
    "    localQNgroup4a = {'group':['12C-1H3-2H','13C-1H3-2H','15N-1H3','CH3Cl','PH3','CH3OH','CH3Br','CH3CN','CH3F','CH3I','NF3'],\n",
    "                      'ulabel':['J','K','l','C','Sym','F'],\n",
    "                      'uformat':['%3d','%3d','%2d','%2s','%1s','%4s'],\n",
    "                      'llabel': ['J','K','l','C','Sym','F'],\n",
    "                      'lformat':['%3d','%3d','%2d','%2s','%1s','%4s']}\n",
    "    localQNgroup4b = {'group':['14N-1H3'],\n",
    "                      'ulabel':['J','K','l','none','Grot','Gtot','none'],\n",
    "                      'uformat':['%2d','%3d','%2d','%1s','%3s','%3s','%1s'],\n",
    "                      'llabel': ['J','K','l','none','Grot','Gtot','none'],\n",
    "                      'lformat':['%2d','%3d','%2d','%1s','%3s','%3s','%1s']}\n",
    "    localQNgroup4c = {'group':['C2H6'],\n",
    "                      'ulabel':['J','K','l','Sym','F'],\n",
    "                      'uformat':['%3d','%3d','%2d','%3s','%4s'],\n",
    "                      'llabel': ['J','K','l','Sym','F'],\n",
    "                      'lformat':['%3d','%3d','%2d','%3s','%4s']}\n",
    "    localQNgroup5  = {'group':['SO3'],\n",
    "                      'ulabel':['none','J','K','none','Gtot','none'],\n",
    "                      'uformat':['%3s','%3d','%3d','%2s','%3s','%1s'],\n",
    "                      'llabel': ['none','J','K','none','Grot','none'],\n",
    "                      'lformat':['%3s','%3d','%3d','%2s','%3s','%1s']}\n",
    "    localQNgroup6  = {'group':['O2','SO'],\n",
    "                      'ulabel':['none','F'],\n",
    "                      'uformat':['%10s','%5s'],\n",
    "                      'llabel': ['none','Br1','N','Br2','J','F','M'],\n",
    "                      'lformat':['%1s','%1s','%3d','%1s','%3d','%5s','%1s']}\n",
    "    localQNgroup7a = {'group':['NO','ClO'],\n",
    "                      'ulabel':['none','none','F'],     # m, none, F\n",
    "                      'uformat':['%1s','%9s','%5s'],\n",
    "                      'llabel': ['none','Br','J','Sym','F'],\n",
    "                      'lformat':['%2s','%2s','%5.1f','%1s','%5s']}\n",
    "    localQNgroup7b = {'group':['OH'],\n",
    "                      'ulabel':['none','F'],\n",
    "                      'uformat':['%10s','%5s'],\n",
    "                      'llabel': ['none','Br','J','Sym','F'],\n",
    "                      'lformat':['%1s','%2s','%5.1f','%2s','%5s']}\n",
    "    localQNgroup = [localQNgroup1,localQNgroup2a,localQNgroup2b,localQNgroup3,\n",
    "                    localQNgroup4a,localQNgroup4b,localQNgroup4c,\n",
    "                    localQNgroup5,localQNgroup6,localQNgroup7a,localQNgroup7b]\n",
    "    count = 0\n",
    "    for lQNgroup in localQNgroup:\n",
    "        if molecule in lQNgroup.get('group'):\n",
    "            LocalQNupperLabels = lQNgroup.get('ulabel')\n",
    "            LocalQNupperFormats = lQNgroup.get('uformat')\n",
    "            LocalQNlowerLabels = lQNgroup.get('llabel')\n",
    "            LocalQNlowerFormats = lQNgroup.get('lformat')\n",
    "        elif isotopologue in lQNgroup.get('group'):\n",
    "            LocalQNupperLabels = lQNgroup.get('ulabel')\n",
    "            LocalQNupperFormats = lQNgroup.get('uformat')\n",
    "            LocalQNlowerLabels = lQNgroup.get('llabel')\n",
    "            LocalQNlowerFormats = lQNgroup.get('lformat')\n",
    "        else:\n",
    "            count += 1\n",
    "    if count == 11:\n",
    "        LocalQNupperLabels = LocalQNLabel_list\n",
    "        LocalQNupperFormats = LocalQNFormat_list\n",
    "        LocalQNlowerLabels = LocalQNLabel_list\n",
    "        LocalQNlowerFormats = LocalQNFormat_list     \n",
    "    return(LocalQNupperLabels, LocalQNlowerLabels, LocalQNupperFormats, LocalQNlowerFormats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_QN_hitran(hitran_df,GlobalQNLabels,LocalQNupperLabels,LocalQNlowerLabels,\n",
    "                       GlobalQNFormats,LocalQNupperFormats,LocalQNlowerFormats):\n",
    "    GlobalQNLabels,GlobalQNFormats = globalQNclasses(molecule,isotopologue)\n",
    "    LocalQNupperLabels, LocalQNlowerLabels, LocalQNupperFormats, LocalQNlowerFormats = localQNgroups(molecule,isotopologue)\n",
    "    GQN_format = [int(s) for s in str(GlobalQNFormats).replace('.1','') if s.isdigit()]\n",
    "    LQNu_format = [int(s) for s in str(LocalQNupperFormats).replace('.1','') if s.isdigit()]\n",
    "    LQNl_format = [int(s) for s in str(LocalQNlowerFormats).replace('.1','') if s.isdigit()]\n",
    "    hitran_df['Qp'] = [x.replace(' .5','0.5') for x in hitran_df['Qp'].values]\n",
    "    hitran_df['Qpp'] = [x.replace(' .5','0.5') for x in hitran_df['Qpp'].values]\n",
    "    hitran_df['Vp'] = [x.replace(' .5','0.5') for x in hitran_df['Vp'].values]\n",
    "    hitran_df['Vpp'] = [x.replace(' .5','0.5') for x in hitran_df['Vpp'].values]\n",
    "    # Global quantum numbers    \n",
    "    n_GQN = len(GlobalQNLabels)\n",
    "    GQNlsum = []\n",
    "    GQNrsum = []\n",
    "    for i in range(n_GQN):\n",
    "        GQNlsum.append(sum(GQN_format[:i]))\n",
    "        GQNrsum.append(sum(GQN_format[:i+1]))  \n",
    "    GQNu_df = pd.DataFrame(columns=GlobalQNLabels)  \n",
    "    GQNl_df = pd.DataFrame(columns=GlobalQNLabels)  \n",
    "    for i in range(n_GQN):\n",
    "        GQNu_df[GlobalQNLabels[i]] = hitran_df['Vp'].map(lambda x: x[GQNlsum[i]:GQNrsum[i]]) \n",
    "        GQNl_df[GlobalQNLabels[i]] = hitran_df['Vpp'].map(lambda x: x[GQNlsum[i]:GQNrsum[i]]) \n",
    "    if 'none' in GlobalQNLabels:\n",
    "        GQNu_df = GQNu_df.drop(columns=['none'])\n",
    "        GQNl_df = GQNl_df.drop(columns=['none'])\n",
    "\n",
    "    # Local quantum numbers    \n",
    "    n_LQNu = len(LocalQNupperLabels)\n",
    "    LQNulsum = []\n",
    "    LQNursum = []\n",
    "    for i in range(n_LQNu):\n",
    "        LQNulsum.append(sum(LQNu_format[:i]))\n",
    "        LQNursum.append(sum(LQNu_format[:i+1]))\n",
    "    n_LQNl = len(LocalQNlowerLabels)\n",
    "    LQNllsum = []\n",
    "    LQNlrsum = []\n",
    "    for i in range(n_LQNl):\n",
    "        LQNllsum.append(sum(LQNl_format[:i]))\n",
    "        LQNlrsum.append(sum(LQNl_format[:i+1]))\n",
    "    LQNu_df = pd.DataFrame(columns=LocalQNupperLabels)  \n",
    "    for i in range(n_LQNu):\n",
    "        LQNu_df[LocalQNupperLabels[i]] = hitran_df['Qp'].map(lambda x: x[LQNulsum[i]:LQNursum[i]]) \n",
    "    if 'none' in LocalQNupperLabels:\n",
    "        LQNu_df = LQNu_df.drop(columns=['none'])\n",
    "    LQNl_df = pd.DataFrame(columns=LocalQNlowerLabels)  \n",
    "    for i in range(n_LQNl):\n",
    "        LQNl_df[LocalQNlowerLabels[i]] = hitran_df['Qpp'].map(lambda x: x[LQNllsum[i]:LQNlrsum[i]]) \n",
    "    if 'none' in LocalQNlowerLabels:\n",
    "        LQNl_df = LQNl_df.drop(columns=['none'])\n",
    "        \n",
    "    if 'Br' in LocalQNupperLabels:\n",
    "        LQNu_df = LQNu_df.drop(columns=['Br'])    \n",
    "    if 'Br' in LocalQNlowerLabels:\n",
    "        LQNl_df['Br'] = LQNl_df['Br'].map(lambda x: x[1]).replace(['Q','P','R','O','S'],[0,-1,1,-2,2])\n",
    "        LQNl_df['J'] = pd.to_numeric(LQNl_df['J'])\n",
    "        LQNu_df['J'] = LQNl_df['Br'] + LQNl_df['J']\n",
    "        LQNl_df = LQNl_df.drop(columns=['Br'])\n",
    "\n",
    "    if 'F' not in LocalQNupperLabels:\n",
    "        LQNu_df['F'] = LQNu_df['J']\n",
    "        LQNl_df['F'] = LQNl_df['J']\n",
    "        \n",
    "    QNu_df = pd.concat([GQNu_df, LQNu_df], axis='columns')\n",
    "    QNl_df = pd.concat([GQNl_df, LQNl_df], axis='columns')\n",
    "    QNu_col = [i+\"'\" for i in list(QNu_df.columns)] \n",
    "    QNl_col = [i+'\"' for i in list(QNl_df.columns)] \n",
    "    return(QNu_df, QNl_df, QNu_col, QNl_col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitran_linelist_QN(hitran_df):\n",
    "    GlobalQNLabels,GlobalQNFormats = globalQNclasses(molecule,isotopologue)\n",
    "    LocalQNupperLabels, LocalQNlowerLabels, LocalQNupperFormats, LocalQNlowerFormats = localQNgroups(molecule,isotopologue)\n",
    "    QNu_df, QNl_df, QNu_col, QNl_col = separate_QN_hitran(hitran_df,GlobalQNLabels,LocalQNupperLabels,LocalQNlowerLabels,\n",
    "                                                          GlobalQNFormats,LocalQNupperFormats,LocalQNlowerFormats)\n",
    "    Ep_df = cal_Ep_hitran(hitran_df)\n",
    "    hitran_linelist_df = hitran_df[['v','S','A','gamma_air','gamma_self','Epp','n_air','delta_air','gp','gpp']]\n",
    "    hitran_linelist_df = pd.concat([hitran_linelist_df, Ep_df, QNu_df, QNl_df], axis='columns')\n",
    "    hitran_main_colname = ['v','S','A','gamma_air','gamma_self','E\"','n_air','delta_air',\"g'\",'g\"',\"E'\"]\n",
    "    QN_col = QNu_col + QNl_col\n",
    "    hitran_linelist_colname = hitran_main_colname + QN_col\n",
    "    hitran_linelist_df.set_axis(hitran_linelist_colname, axis=1, inplace=True)\n",
    "    # Do quantum number filter.\n",
    "    if QNsFilter !=[]:   \n",
    "        QNs_col = [i+\"'\" for i in QNs_label] + [i+'\"' for i in QNs_label]\n",
    "        if 'J' not in QNs_label:\n",
    "            hitran_linelist_df = hitran_linelist_df[hitran_main_colname + QNs_col + [\"J'\", 'J\"']]\n",
    "        else:\n",
    "            hitran_linelist_df = hitran_linelist_df[hitran_main_colname + QNs_col]\n",
    "        hitran_linelist_df = QNfilter_linelist(hitran_linelist_df, QNs_value, QNs_label)\n",
    "    else:\n",
    "        QNs_col = QN_col    \n",
    "    return(hitran_linelist_df, QNs_col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linelist_hitran(hitran_linelist_df):\n",
    "    '''\n",
    "    Read HITRAN .par file as the input file.\n",
    "    Return the data for calculating wavennumbers and cross sections with line profiles.\n",
    "    \n",
    "    '''\n",
    "    A = hitran_linelist_df['A'].values\n",
    "    Ep = hitran_linelist_df[\"E'\"].values\n",
    "    Epp = hitran_linelist_df['E\"'].values\n",
    "    n_air = hitran_linelist_df['n_air'].values\n",
    "    gamma_air = hitran_linelist_df['gamma_air'].values\n",
    "    gamma_self = hitran_linelist_df['gamma_self'].values\n",
    "    delta_air = hitran_linelist_df['delta_air'].values\n",
    "    gp = hitran_linelist_df[\"g'\"].values\n",
    "    v = hitran_linelist_df['v'].values\n",
    "    # if broad == 'Air':\n",
    "    #   v = hitran_df['v'].values + delta_air * (P - P_ref) / P\n",
    "    # else:\n",
    "    #   v = hitran_df['v'].values\n",
    "    return (A, v, Ep, Epp, gp, n_air, gamma_air, gamma_self, delta_air)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Parition Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_partition(En, gn, T):\n",
    "    partition_func = ne.evaluate('sum(gn * exp(-c2 * En / T))') \n",
    "    return(partition_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition function\n",
    "def exomol_partition_func(states_df, Ntemp, Tmax):\n",
    "    \n",
    "    print('Calculate partition functions.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    \n",
    "    En = states_df['E'].astype('float').values\n",
    "    gn = states_df['g'].astype('int').values\n",
    "    Ts = np.array(range(Ntemp, Tmax+1, Ntemp)) \n",
    "    \n",
    "    partition_func = [calculate_partition(En, gn, T) for T in Ts]\n",
    "    \n",
    "    partition_func_df = pd.DataFrame()\n",
    "    partition_func_df['T'] = Ts\n",
    "    partition_func_df['partition function'] = partition_func\n",
    "        \n",
    "    pf_folder = save_path + '/partition/'\n",
    "    if os.path.exists(pf_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(pf_folder, exist_ok=True)\n",
    "    pf_path = pf_folder + isotopologue + '__' + dataset + '.pf'\n",
    "    np.savetxt(pf_path, partition_func_df, fmt=\"%8.1f %15.4f\")\n",
    "    \n",
    "    t.end()\n",
    "    print('Partition functions have been saved!\\n')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ntemp = 1\n",
    "# max = int(5000.00)\n",
    "# states_df = read_all_states(read_path)\n",
    "# exomol_partition_func(states_df, Ntemp, Tmax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Heat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_specific_heats(En, gn, T):\n",
    "    pf = ne.evaluate('sum(gn * exp(-c2 * En / T)) ')  \n",
    "    pfp = ne.evaluate('sum(gn * exp(-c2 * En / T) * (c2 * En / T))')\n",
    "    pfpp = ne.evaluate('sum(gn * exp(-c2 * En / T) * (c2 * En / T) ** 2)')\n",
    "    specificheat_func = ne.evaluate('R * (pfpp / pf - (pfp / pf)**2) + 2.5 * R') \n",
    "    return(specificheat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific heat\n",
    "def exomol_specificheat(states_df, Ntemp, Tmax):\n",
    "    \n",
    "    print('Calculate specific heats.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    \n",
    "    En = states_df['E'].astype('float').values\n",
    "    gn = states_df['g'].astype('int').values\n",
    "    Ts = np.array(range(200, Tmax+1, Ntemp)) \n",
    "    \n",
    "    specificheat_func = [calculate_specific_heats(En, gn, T) for T in Ts]\n",
    "    \n",
    "    specificheat_func_df = pd.DataFrame()\n",
    "    specificheat_func_df['T'] = Ts\n",
    "    specificheat_func_df['specific heat'] = specificheat_func\n",
    "        \n",
    "    cp_folder = save_path + '/specific_heat/'\n",
    "    if os.path.exists(cp_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(cp_folder, exist_ok=True)  \n",
    "    cp_path = cp_folder + isotopologue + '__' + dataset + '.cp'\n",
    "    np.savetxt(cp_path, specificheat_func_df, fmt=\"%8.1f %15.4f\")\n",
    "\n",
    "    t.end()\n",
    "    print('Specific heats have been saved!\\n')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ntemp = 1\n",
    "# Tmax = int(5000.0)\n",
    "# states_df = read_all_states(read_path)\n",
    "# exomol_specificheat(states_df, Ntemp, Tmax)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lifetime\n",
    "def cal_lifetime(states_df, all_trans_df):\n",
    "    sum_A = all_trans_df.groupby('u')['A'].sum()\n",
    "    lifetime = ne.evaluate('1 / sum_A') \n",
    "    lt_df = pd.Series(lifetime).map('{: >12.4E}'.format).reset_index()\n",
    "    lt_df.columns=['u','lt']\n",
    "    add_u = pd.DataFrame()\n",
    "    add_u['u'] = pd.concat([states_df['id'].astype('int'), pd.Series(sum_A.index)]).drop_duplicates(keep=False) - 1\n",
    "    add_u['lt'] = '         Inf'\n",
    "    lifetime_df = pd.concat([add_u, lt_df], ignore_index=True)\n",
    "    lifetime_df.sort_values('u',inplace=True)\n",
    "    return(lifetime_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lifetime\n",
    "def exomol_lifetime(read_path, states_df, all_trans_df):\n",
    "    \n",
    "    print('Calculate lifetimes.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "\n",
    "    lifetime_df = cal_lifetime(states_df, all_trans_df)\n",
    "    lifetime_list = list(lifetime_df['lt'])\n",
    "    \n",
    "    states_filenames = glob.glob(read_path + molecule + '/' + isotopologue + '/' + dataset \n",
    "                                 + '/' + isotopologue + '__' + dataset + '.states.bz2')\n",
    "    s_df = pd.read_csv(states_filenames[0], compression='bz2', header=None, dtype=object)\n",
    "    nrows = len(s_df)\n",
    "    new_rows = []\n",
    "    if check_uncertainty == 0:\n",
    "        for i in range(nrows):\n",
    "            new_rows.append(s_df[0][i][:41]+lifetime_list[i]+s_df[0][i][53:]+'\\n')\n",
    "    if check_uncertainty == 1:\n",
    "        for i in range(nrows):\n",
    "            new_rows.append(s_df[0][i][:53]+lifetime_list[i]+s_df[0][i][65:]+'\\n')\n",
    "\n",
    "    lf_folder = save_path + '/lifetime/'\n",
    "    if os.path.exists(lf_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(lf_folder, exist_ok=True)  \n",
    "        \n",
    "    if CompressYN == 'Y':\n",
    "        lf_path = lf_folder + isotopologue + '__' + dataset + '.states.bz2'\n",
    "        with bz2.open(lf_path, 'wt') as f:\n",
    "            for i in range(nrows):\n",
    "                f.write(new_rows[i])\n",
    "            f.close\n",
    "    else:\n",
    "        lf_path = lf_folder + isotopologue + '__' + dataset + '.states'\n",
    "        with open(lf_path, 'wt') as f:\n",
    "            for i in range(nrows):\n",
    "                f.write(new_rows[i])\n",
    "            f.close\n",
    "\n",
    "    t.end()\n",
    "    print('Lifetimes have been saved!\\n')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states_df = read_all_states(read_path)\n",
    "# (all_trans_df, ncolumn) = read_all_trans(read_path)\n",
    "# exomol_lifetime(read_path, states_df, all_trans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_hitran2exomol_path = save_path + 'conversion/HITRAN2ExoMol/'\n",
    "# states_df = read_all_states(read_hitran2exomol_path)\n",
    "# (all_trans_df, ncolumn) = read_all_trans(read_hitran2exomol_path)\n",
    "# exomol_lifetime(read_hitran2exomol_path, states_df, all_trans_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cooling function\n",
    "def linelist_coolingfunc(states_df, all_trans_df, ncolumn):\n",
    "    id_u = all_trans_df['u'].values\n",
    "    id_l = all_trans_df['l'].values\n",
    "    states_df['id'] = pd.to_numeric(states_df['id'])\n",
    "    states_df.set_index(['id'], inplace=True, drop=False)\n",
    "    id_s = states_df['id']\n",
    "    all_trans_df.set_index(['u'], inplace=True, drop=False)\n",
    "    id_us = list(set(id_u).intersection(set(id_s)))\n",
    "    trans_us_df = all_trans_df.loc[id_us]\n",
    "    id_l = trans_us_df['l'].values\n",
    "    id_ls = list(set(id_l).intersection(set(id_s)))\n",
    "    trans_us_df.set_index(['l'], inplace=True, drop=False)\n",
    "    trans_s_df = trans_us_df.loc[id_ls]\n",
    "    id_su = trans_s_df['u'].values\n",
    "    id_sl = trans_s_df['l'].values\n",
    "    states_u_df = states_df.loc[id_su]\n",
    "    states_l_df = states_df.loc[id_sl]\n",
    "\n",
    "    Ep = states_u_df['E'].values.astype('float')\n",
    "    gp = states_u_df['g'].values.astype('int')\n",
    "    A = trans_s_df['A'].values.astype('float')\n",
    "\n",
    "    if ncolumn == 4:\n",
    "        v = trans_s_df['v'].values.astype('float')\n",
    "    else:\n",
    "        Epp = states_l_df['E'].values.astype('float')     # Upper state energy\n",
    "        v = cal_v(Ep, Epp) \n",
    "    return (A, v, Ep, gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cooling(A, v, Ep, gp, T, Q):\n",
    "    # cooling_func = np.sum(A * h * c * v * gp * np.exp(-c2 * Ep / T)) / (4 * PI * Q) \n",
    "    _sum = ne.evaluate('sum(A * h * c * v * gp * exp(-c2 * Ep / T))')  \n",
    "    cooling_func = ne.evaluate('_sum / (4 * PI * Q)')\n",
    "    return(cooling_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cooling function for ExoMol database\n",
    "def exomol_cooling_func(read_path, states_df, all_trans_df, Ntemp, Tmax, ncolumn):\n",
    "    \n",
    "    # tqdm.write('Calculate cooling functions.') \n",
    "    print('Calculate cooling functions.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    \n",
    "    A, v, Ep, gp = linelist_coolingfunc(states_df, all_trans_df, ncolumn)\n",
    "    Ts = np.array(range(Ntemp, Tmax+1, Ntemp)) \n",
    "    Qs = read_exomol_pf(read_path, Ts)\n",
    "    # Qs = read_exomolweb_pf(Ts)\n",
    "    \n",
    "    cooling_func = [calculate_cooling(A, v, Ep, gp, Ts[i], Qs[i]) for i in tqdm(range(Tmax), desc='Calculating')]\n",
    "    cooling_func_df = pd.DataFrame()\n",
    "    cooling_func_df['T'] = Ts\n",
    "    cooling_func_df['cooling function'] = cooling_func\n",
    "\n",
    "    cf_folder = save_path + '/cooling/'\n",
    "    if os.path.exists(cf_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(cf_folder, exist_ok=True)  \n",
    "    cf_path = cf_folder + isotopologue + '__' + dataset + '.cf' \n",
    "    np.savetxt(cf_path, cooling_func_df, fmt=\"%8.1f %20.8E\")\n",
    "    \n",
    "    t.end()\n",
    "    print('Cooling functions have been saved!\\n')  \n",
    "    # tqdm.write('Cooling functions has been saved!\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cooling function for HITRAN database\n",
    "def hitran_cooling_func(hitran_df, Ntemp, Tmax):\n",
    "    \n",
    "    print('Calculate cooling functions.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "\n",
    "    Ep = cal_Ep(hitran_df['Epp'].values,hitran_df['v'].values)\n",
    "    A = hitran_df['A'].values\n",
    "    v = hitran_df['v'].values\n",
    "    gp = hitran_df['gp'].values\n",
    "    \n",
    "    Ts = np.array(range(Ntemp, Tmax+1, Ntemp)) \n",
    "    Qs = read_hitran_pf(Ts)\n",
    "    \n",
    "    cooling_func = [calculate_cooling(A, v, Ep, gp, Ts[i], Qs[i]) for i in tqdm(range(Tmax), desc='Calculating')]\n",
    "    cooling_func_df = pd.DataFrame()\n",
    "    cooling_func_df['T'] = Ts\n",
    "    cooling_func_df['cooling function'] = cooling_func\n",
    "\n",
    "    cf_folder = save_path + '/cooling/'\n",
    "    if os.path.exists(cf_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(cf_folder, exist_ok=True)  \n",
    "    cf_path = cf_folder + isotopologue + '__' + dataset + '.cf' \n",
    "    np.savetxt(cf_path, cooling_func_df, fmt=\"%8.1f %20.8E\")\n",
    "    \n",
    "    t.end()\n",
    "    print('Cooling functions have been saved!\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ntemp = 1\n",
    "# Tmax = int(1500.00)\n",
    "# states_df = read_all_states(read_path)\n",
    "# (all_trans_df, ncolumn) = read_all_trans(read_path)\n",
    "# exomol_cooling_func(read_path, states_df, all_trans_df, Ntemp, Tmax, ncolumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ntemp = 1\n",
    "# Tmax = int(1500.00)\n",
    "# parfile_df = read_parfile(read_path)\n",
    "# hitran_df = read_hitran_parfile(read_path,parfile_df,min_wn,max_wn,'None','None').reset_index().drop(columns='index')\n",
    "# hitran_cooling_func(hitran_df, Ntemp, Tmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oscillator Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linelist_oscillator_strength(states_df, all_trans_df, ncolumn):\n",
    "\n",
    "    id_u = all_trans_df['u'].values\n",
    "    id_l = all_trans_df['l'].values\n",
    "    states_df['id'] = pd.to_numeric(states_df['id'])\n",
    "    states_df.set_index(['id'], inplace=True, drop=False)\n",
    "    id_s = states_df['id']\n",
    "    all_trans_df.set_index(['u'], inplace=True, drop=False)\n",
    "    id_us = list(set(id_u).intersection(set(id_s)))\n",
    "    trans_us_df = all_trans_df.loc[id_us]\n",
    "    id_l = trans_us_df['l'].values\n",
    "    id_ls = list(set(id_l).intersection(set(id_s)))\n",
    "    trans_us_df.set_index(['l'], inplace=True, drop=False)\n",
    "    trans_s_df = trans_us_df.loc[id_ls]\n",
    "    id_su = trans_s_df['u'].values\n",
    "    id_sl = trans_s_df['l'].values\n",
    "    states_u_df = states_df.loc[id_su]\n",
    "    states_l_df = states_df.loc[id_sl]\n",
    "\n",
    "    gp = states_u_df['g'].values.astype('int')\n",
    "    gpp = states_l_df['g'].values.astype('int')\n",
    "    A = trans_s_df['A'].values.astype('float')\n",
    "\n",
    "    if ncolumn == 4:\n",
    "        v = trans_s_df['v'].values.astype('float')\n",
    "    else:\n",
    "        Ep = states_u_df['E'].values.astype('float')\n",
    "        Epp = states_l_df['E'].values.astype('float')     # Upper state energy\n",
    "        v = cal_v(Ep, Epp) \n",
    "    return (gp, gpp, A, v, trans_s_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_oscillator_strength(gp, gpp, A, v):\n",
    "    fg = ne.evaluate('gp * A / (c * v)**2')\n",
    "    f = ne.evaluate('fg / gpp')\n",
    "    if 'G' not in fgORf:\n",
    "        oscillator_strength = f\n",
    "    else:\n",
    "        oscillator_strength = fg\n",
    "    return oscillator_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oscillator strength for ExoMol database\n",
    "def exomol_oscillator_strength(states_df, all_trans_df, ncolumn):\n",
    "    \n",
    "    print('Calculate oscillator strengths.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    \n",
    "    gp, gpp, A, v, trans_s_df = linelist_oscillator_strength(states_df, all_trans_df, ncolumn)\n",
    "\n",
    "    if Ncolumns == 4:\n",
    "        oscillator_strength_df = trans_s_df[['u', 'l', 'A']]\n",
    "        os_format = \"%12d %12d %10.4E %20.8E\"\n",
    "    else:\n",
    "        oscillator_strength_df = trans_s_df[['u', 'l']]\n",
    "        os_format = \"%12d %12d %20.8E\"\n",
    "    oscillator_strength_df['os'] = cal_oscillator_strength(gp, gpp, A, v)\n",
    "\n",
    "    os_folder = save_path + '/oscillator_strength/'\n",
    "    if os.path.exists(os_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(os_folder, exist_ok=True)  \n",
    "    os_path = os_folder + isotopologue + '__' + dataset + '.os' \n",
    "    np.savetxt(os_path, oscillator_strength_df, fmt=os_format)\n",
    "    \n",
    "    t.end()\n",
    "    print('Oscillator strengths have been saved!\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oscillator strength for HITRAN database\n",
    "def hitran_oscillator_strength(hitran_df):\n",
    "    \n",
    "    print('Calculate oscillator strengths.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "\n",
    "    A = hitran_df['A'].values\n",
    "    v = hitran_df['v'].values\n",
    "    gp = hitran_df['gp'].values\n",
    "    gpp = hitran_df['gp'].values\n",
    "    \n",
    "    if Ncolumns == 4:\n",
    "        oscillator_strength_df = hitran_df[['gp', 'gpp', 'A']]\n",
    "        os_format = \"%7.1f %7.1f %10.3E %20.8E\"\n",
    "    else:\n",
    "        oscillator_strength_df = hitran_df[['gp', 'gpp']]\n",
    "        os_format = \"%7.1f %7.1f %20.8E\"\n",
    "    oscillator_strength_df['os'] = cal_oscillator_strength(gp, gpp, A, v)\n",
    "\n",
    "    os_folder = save_path + '/oscillator_strength/'\n",
    "    if os.path.exists(os_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(os_folder, exist_ok=True)  \n",
    "    os_path = os_folder + isotopologue + '__' + dataset + '.os' \n",
    "    np.savetxt(os_path, oscillator_strength_df, fmt=os_format)\n",
    "    \n",
    "    t.end()\n",
    "    print('Oscillator strengths have been saved!\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states_df = read_all_states(read_path)\n",
    "# (all_trans_df, ncolumn) = read_all_trans(read_path)\n",
    "# exomol_oscillator_strength(states_df, all_trans_df, ncolumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parfile_df = read_parfile(read_path)\n",
    "# hitran_df = read_hitran_parfile(read_path,parfile_df,min_wn,max_wn,'None','None').reset_index().drop(columns='index')\n",
    "# hitran_oscillator_strength(hitran_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_part_states(states_df):\n",
    "    if UncFilter != 'None' :\n",
    "        states_part_df = states_df[states_df['unc'].astype(float) <= UncFilter]\n",
    "        states_part_df['id'] = pd.to_numeric(states_part_df['id'])\n",
    "        states_part_df.set_index(['id'], inplace=True, drop=False)\n",
    "    else:\n",
    "        states_part_df = states_df\n",
    "        states_part_df['id'] = pd.to_numeric(states_part_df['id'])\n",
    "        states_part_df.set_index(['id'], inplace=True, drop=False)\n",
    "    if check_uncertainty == 1:\n",
    "        col_unc = ['unc']\n",
    "    else:\n",
    "        col_unc = []\n",
    "    if check_lifetime == 1:\n",
    "        col_lifetime = ['tau']\n",
    "    else:\n",
    "        col_lifetime = []\n",
    "    if check_gfactor == 1:\n",
    "        col_gfac = ['gfac']\n",
    "    else:\n",
    "        col_gfac = []\n",
    "    colname = ['id','E','g','J'] + col_unc + col_lifetime + col_gfac + QNslabel_list\n",
    "    states_part_df.drop(states_part_df.columns[len(colname):], axis=1, inplace=True)\n",
    "    states_part_df.columns = colname\n",
    "    QNcolumns = ['id','E','g','J'] + col_unc + col_lifetime + col_gfac + QNs_label\n",
    "    states_part_df = states_part_df[QNcolumns]\n",
    "    if QNsFilter !=[]:    \n",
    "        for i in range(len(QNs_label)):\n",
    "            if QNs_value[i] != ['']:\n",
    "                list_QN_value = str(QNs_value[i]).replace(\"', '\",\",\").replace(\"'\",\"\").replace('[','').replace(']','').split(',')\n",
    "                if '' in list_QN_value:\n",
    "                    states_part_df = states_part_df\n",
    "                else:\n",
    "                    states_part_df = states_part_df[states_part_df[QNs_label[i]].isin(list_QN_value)]\n",
    "    return(states_part_df)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_transfiles(read_path):\n",
    "    # Get all the transitions files from the folder including the older version files which are named by vn(version number).\n",
    "    trans_filepaths_all = glob.glob(read_path + molecule + '/' + isotopologue + '/' + dataset + '/' + '*trans.bz2')\n",
    "    num_transfiles_all = len(trans_filepaths_all)    # The number of all transitions files including the older version files.\n",
    "    trans_filepaths = []    # The list of the lastest transitions files.\n",
    "    for i in range(num_transfiles_all):\n",
    "        split_version = trans_filepaths_all[i].split('__')[-1].split('.')[0].split('_')    # Split the filenames.\n",
    "        num = len(split_version)\n",
    "        # There are four format filenames.\n",
    "        # The lastest transitions files named in two formats:\n",
    "        # 1. Filenames are named with the name of isotopologue and dataset. \n",
    "        #    End with .trans.bz2.\n",
    "        #    e.g. 14N-16O__XABC.trans.bz2'\n",
    "        # 2. Filenames are named with the name of isotopologue and dataset. \n",
    "        #    Also have the range of wavenumbers xxxxx-yyyyy.\n",
    "        #    End with .trans.bz2.\n",
    "        #    e.g. 1H2-16O__POKAZATEL__00000-00100.trans.bz2\n",
    "        # 3. The older version transitions files are named with vn(version number) based on the first format of the lastest files.\n",
    "        #    e.g. 14N-16O__XABC_v2.trans.bz2\n",
    "        # 4. The older version transitions files are named with updated date (yyyymmdd).\n",
    "        #    e.g. 1H3_p__MiZATeP__20170330.trans.bz2\n",
    "        # After split the filenames:\n",
    "        # The first format filenames only leave the dataset name, e.g. XABC.\n",
    "        # The second format filenames only leave the range of the wavenumber, e.g. 00000-00100.\n",
    "        # The third format filenames leave two parts(dataset name and version number), e.g. XABC and v2.\n",
    "        # The fourth format filenames only leave the updated date, e.g. 20170330.\n",
    "        # This program only process the lastest data, so extract the filenames named by the first two format.\n",
    "        if num == 1:     \n",
    "            if split_version[0] == dataset:        \n",
    "                trans_filepaths.append(trans_filepaths_all[i])\n",
    "            elif len(split_version[0].split('-')) == 2:\n",
    "                trans_filepaths.append(trans_filepaths_all[i])\n",
    "        \n",
    "    if len(trans_filepaths) == 1:\n",
    "        filenames = trans_filepaths\n",
    "    else:\n",
    "        filenames = []\n",
    "        for trans_filename in tqdm(trans_filepaths, position=0, leave=True, desc='Finding trans files'):\n",
    "            lower = int(trans_filename.split('__')[2].split('.')[0].split('-')[0])\n",
    "            upper = int(trans_filename.split('__')[2].split('.')[0].split('-')[1]) \n",
    "            if (lower <= int(min_wn) < upper):\n",
    "                filenames.append(trans_filename)\n",
    "            if (lower >= int(min_wn) and upper <= int(max_wn)):\n",
    "                filenames.append(trans_filename)\n",
    "            if (lower <= int(max_wn) < upper):\n",
    "                filenames.append(trans_filename)    \n",
    "    return(list(set(filenames)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_part_trans(read_path):\n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    print('Reading part transitions ...')\n",
    "    trans_filenames = get_part_transfiles(read_path)\n",
    "    t_df = dict()\n",
    "    trans_part_df = pd.DataFrame()\n",
    "    # Initialise the iterator object.\n",
    "    for trans_filename in tqdm(trans_filenames, position=0, leave=True, desc='Reading transitions'):\n",
    "        t_df[trans_filename] = pd.read_csv(trans_filename, compression='bz2', sep='\\s+', header=None, \n",
    "                                           chunksize=1_000_000, iterator=True, encoding='utf-8')\n",
    "        for chunk in t_df[trans_filename]:\n",
    "            trans_part_df = pd.concat([trans_part_df, chunk])\n",
    "    ncolumn = len(trans_part_df.columns)\n",
    "    if ncolumn == 3: \n",
    "        trans_col_name={0:'u', 1:'l', 2:'A'}\n",
    "    else:\n",
    "        trans_col_name={0:'u', 1:'l', 2:'A', 3:'v'}\n",
    "    trans_part_df = trans_part_df.rename(columns=trans_col_name)\n",
    "    t.end()                \n",
    "    print('Finished reading part transitions!\\n') \n",
    "    return(trans_part_df, ncolumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_broad(broad_df, states_l_df):\n",
    "    J_df = pd.DataFrame()\n",
    "    max_broad_J = max(broad_df['Jpp'])\n",
    "    J_df['Jpp'] = states_l_df['J'].values.astype('float')\n",
    "    J_df['Jpp'][J_df.Jpp > max_broad_J] = max_broad_J\n",
    "    id_broad = (J_df['Jpp']-0.1).round(0).astype(int)\n",
    "    gamma_L = broad_df['gamma_L'][id_broad].values\n",
    "    n_air = broad_df['n_air'][id_broad].values\n",
    "    return(gamma_L, n_air)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absorption coefficient\n",
    "def cal_abscoefs(T, v, gp, A, Epp, Q, abundance):\n",
    "    # abscoef = gp * A * np.exp(- c2 * Epp / T) * (1 - np.exp(- c2 * v / T)) / (8 * np.pi * c * v**2 * Q) * abundance  \n",
    "    abscoef = ne.evaluate('gp * A * exp(- c2 * Epp / T) * (1 - exp(- c2 * v / T)) * Inv8Pic / (v ** 2 * Q) * abundance')  \n",
    "    return abscoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate emission coefficient\n",
    "def cal_emicoefs(T, v, gp, A, Ep, Q, abundance):\n",
    "    # emicoef = gp * A * v * np.exp(- c2 * Ep / T) / (4 * np.pi) / Q * abundance   \n",
    "    emicoef = ne.evaluate('gp * A * v * exp(- c2 * Ep / T) * Inv4Pi / Q * abundance')\n",
    "    return emicoef"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate uncertainty\n",
    "def cal_uncertainty(unc_u, unc_l):\n",
    "    unc = ne.evaluate('sqrt(unc_u ** 2 + unc_l ** 2)')\n",
    "    return unc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion\n",
    "\n",
    "## ExoMol to HITRAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_QNValues_exomol2hitran(states_unc_df, GlobalQNLabel_list, LocalQNLabel_list):\n",
    "    QNLabel_list = GlobalQNLabel_list+LocalQNLabel_list\n",
    "    if 'Gtot' in QNLabel_list:\n",
    "        states_unc_df[\"Gtot\"] = (states_unc_df[\"Gtot\"].replace('1',\"A1'\").replace('2',\"A2'\").replace('3',\"E'\")\n",
    "                                 .replace('4','A1\"').replace('5','A2\"').replace('6','E\"'))\n",
    "    if 'Gvib' in QNLabel_list:\n",
    "        states_unc_df[\"Gvib\"] = (states_unc_df[\"Gvib\"].replace('1',\"A1'\").replace('2',\"A2'\").replace('3',\"E'\")\n",
    "                                 .replace('4','A1\"').replace('5','A2\"').replace('6','E\"'))   \n",
    "    if 'Grot' in QNLabel_list:\n",
    "        states_unc_df[\"Grot\"] = (states_unc_df[\"Grot\"].replace('1',\"A1'\").replace('2',\"A2'\").replace('3',\"E'\")\n",
    "                                 .replace('4','A1\"').replace('5','A2\"').replace('6','E\"'))                   \n",
    "    if 'taui' in QNLabel_list:\n",
    "        states_unc_df[\"taui\"] = states_unc_df[\"taui\"].replace('0','s').replace('1','a')\n",
    "    return(states_unc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_unc_states(states_df):\n",
    "    if Conversion != 0:\n",
    "        if ConversionUnc != 'None':\n",
    "            states_unc_df = states_df[states_df['unc'].astype(float) <= ConversionUnc]\n",
    "        else:\n",
    "            states_unc_df = states_df\n",
    "        states_unc_df['id'] = pd.to_numeric(states_unc_df['id'])\n",
    "        states_unc_df.set_index(['id'], inplace=True, drop=False)\n",
    "    else:\n",
    "        states_unc_df = states_df\n",
    "        states_unc_df['id'] = pd.to_numeric(states_unc_df['id'])\n",
    "        states_unc_df.set_index(['id'], inplace=True, drop=False)\n",
    "    if check_uncertainty == 1:\n",
    "        col_unc = ['unc']\n",
    "    else:\n",
    "        col_unc = []\n",
    "    if check_lifetime == 1:\n",
    "        col_lifetime = ['tau']\n",
    "    else:\n",
    "        col_lifetime = []\n",
    "    if check_gfactor == 1:\n",
    "        col_gfac = ['gfac']\n",
    "    else:\n",
    "        col_gfac = []\n",
    "    fullcolname = ['id','E','g','J'] + col_unc + col_lifetime + col_gfac + QNslabel_list\n",
    "    states_unc_df = states_unc_df.iloc[:, : len(fullcolname)]\n",
    "    states_unc_df.columns = fullcolname  \n",
    "    colnames = ['id','E','g'] + col_unc + GlobalQNLabel_list + LocalQNLabel_list\n",
    "    states_unc_df = states_unc_df[colnames] \n",
    "    states_unc_df = convert_QNValues_exomol2hitran(states_unc_df, GlobalQNLabel_list, LocalQNLabel_list)\n",
    "    # pd.set_option(\"display.max_columns\",30) \n",
    "    return(states_unc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_QNFormat_exomol2hitran(states_u_df, states_l_df, GlobalQNLabel_list, GlobalQNFormat_list, \n",
    "                                   LocalQNLabel_list, LocalQNFormat_list):\n",
    "    from pandarallel import pandarallel\n",
    "    pandarallel.initialize(nb_workers=8,progress_bar=False)    # Initialize.\n",
    "\n",
    "    gQNp = pd.DataFrame()\n",
    "    gQNpp = pd.DataFrame()\n",
    "    n_gQN = len(GlobalQNLabel_list)\n",
    "    for i in range(n_gQN):\n",
    "        gQN_format = GlobalQNFormat_list[i].replace(\"%\",'{: >')+'}'\n",
    "        gQN_label = GlobalQNLabel_list[i]\n",
    "        try:\n",
    "            if 'd' in gQN_format or 'f' in gQN_format: \n",
    "                gQNp[gQN_label+\"'\"] = pd.Series(pd.to_numeric(states_u_df[gQN_label]\n",
    "                                                              .values)).parallel_map(gQN_format.format)\n",
    "                gQNpp[gQN_label+'\"'] = pd.Series(pd.to_numeric(states_l_df[gQN_label]\n",
    "                                                               .values)).parallel_map(gQN_format.format)\n",
    "            elif 's' in gQN_format or 'a' in gQN_format: \n",
    "                gQNp[gQN_label+\"'\"] = pd.Series(states_u_df[gQN_label].str.replace('(','',regex=True)\n",
    "                                                .str.replace(')','',regex=True).values).parallel_map(gQN_format.format)\n",
    "                gQNpp[gQN_label+'\"'] = pd.Series(states_l_df[gQN_label].str.replace('(','',regex=True)\n",
    "                                                .str.replace(')','',regex=True).values).parallel_map(gQN_format.format)\n",
    "        except:\n",
    "            if 'd' in gQN_format or 'f' in gQN_format: \n",
    "                gQNp[gQN_label+\"'\"] = pd.Series(pd.to_numeric(states_u_df[gQN_label].values)).map(gQN_format.format)\n",
    "                gQNpp[gQN_label+'\"'] = pd.Series(pd.to_numeric(states_l_df[gQN_label].values)).map(gQN_format.format)\n",
    "            elif 's' in gQN_format or 'a' in gQN_format:       \n",
    "                gQNp[gQN_label+\"'\"] = pd.Series(states_u_df[gQN_label].str.replace('(','',regex=True)\n",
    "                                                .str.replace(')','',regex=True).values).map(gQN_format.format)\n",
    "                gQNpp[gQN_label+'\"'] = pd.Series(states_l_df[gQN_label].str.replace('(','',regex=True)\n",
    "                                                .str.replace(')','',regex=True).values).map(gQN_format.format)\n",
    "    globalQNp = pd.DataFrame(gQNp).sum(axis=1).map('{: >15}'.format) \n",
    "    globalQNpp = pd.DataFrame(gQNpp).sum(axis=1).map('{: >15}'.format)  \n",
    "\n",
    "    lQNp = pd.DataFrame()\n",
    "    lQNpp = pd.DataFrame()\n",
    "    n_lQN = len(LocalQNLabel_list)\n",
    "    for i in range(n_lQN):\n",
    "        lQN_format = LocalQNFormat_list[i].replace(\"%\",'{: >')+'}'\n",
    "        lQN_label = LocalQNLabel_list[i]\n",
    "        try:\n",
    "            if 'd' in lQN_format or 'f' in lQN_format: \n",
    "                lQNp[lQN_label+\"'\"] = pd.Series(pd.to_numeric(states_u_df[lQN_label].values)).parallel_map(lQN_format.format)\n",
    "                lQNpp[lQN_label+'\"'] = pd.Series(pd.to_numeric(states_l_df[lQN_label].values)).parallel_map(lQN_format.format)\n",
    "            elif 's' in lQN_format or 'a' in lQN_format: \n",
    "                lQNp[lQN_label+\"'\"] = pd.Series(states_u_df[lQN_label].str.replace('(','',regex=True)\n",
    "                                                .str.replace(')','',regex=True).values).parallel_map(lQN_format.format)\n",
    "                lQNpp[lQN_label+'\"'] = pd.Series(states_l_df[lQN_label].str.replace('(','',regex=True)\n",
    "                                                .str.replace(')','',regex=True).values).parallel_map(lQN_format.format)\n",
    "        except:\n",
    "            if 'd' in lQN_format or 'f' in lQN_format: \n",
    "                lQNp[lQN_label+\"'\"] = pd.Series(pd.to_numeric(states_u_df[lQN_label].values)).map(lQN_format.format)\n",
    "                lQNpp[lQN_label+'\"'] = pd.Series(pd.to_numeric(states_l_df[lQN_label].values)).map(lQN_format.format)\n",
    "            elif 's' in lQN_format or 'a' in lQN_format: \n",
    "                lQNp[lQN_label+\"'\"] = pd.Series(states_u_df[lQN_label].str.replace('(','',regex=True)\n",
    "                                                .str.replace(')','',regex=True).values).map(lQN_format.format)\n",
    "                lQNpp[lQN_label+'\"'] = pd.Series(states_l_df[lQN_label].str.replace('(','',regex=True)\n",
    "                                                .str.replace(')','',regex=True).values).map(lQN_format.format)\n",
    "            \n",
    "    localQNp = pd.DataFrame(lQNp).sum(axis=1).map('{: >15}'.format) \n",
    "    localQNpp = pd.DataFrame(lQNpp).sum(axis=1).map('{: >15}'.format)  \n",
    "\n",
    "    QN_df = pd.concat([globalQNp,globalQNpp,localQNp,localQNpp],axis='columns')\n",
    "    QN_df.columns = [\"V'\", 'V\"', \"Q'\", 'Q\"']\n",
    "    return(QN_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linelist_ExoMol2HITRAN(states_unc_df,trans_part_df, ncolumn):\n",
    "    \n",
    "    if ncolumn == 4:\n",
    "        trans_part_df = trans_part_df[trans_part_df['v'].between(ConversionMinFreq,ConversionMaxFreq)] \n",
    "        id_u = trans_part_df['u'].values\n",
    "        id_s = states_unc_df['id'].values\n",
    "\n",
    "        trans_part_df.set_index(['u'], inplace=True, drop=False)\n",
    "        id_us = list(set(id_u).intersection(set(id_s)))\n",
    "        trans_us_df = trans_part_df.loc[id_us]\n",
    "\n",
    "        id_l = trans_us_df['l'].values\n",
    "        id_ls = list(set(id_l).intersection(set(id_s)))\n",
    "        trans_us_df.set_index(['l'], inplace=True, drop=False)\n",
    "        trans_s_df = trans_us_df.loc[id_ls]\n",
    "        trans_s_df.sort_values(by=['v'], inplace=True)\n",
    "    else:\n",
    "        id_u = trans_part_df['u'].values\n",
    "        id_s = pd.to_numeric(states_unc_df['id']).values\n",
    "\n",
    "        trans_part_df.set_index(['u'], inplace=True, drop=False)\n",
    "        id_us = list(set(id_u).intersection(set(id_s)))\n",
    "        trans_us_df = trans_part_df.loc[id_us]\n",
    "\n",
    "        id_l = trans_us_df['l'].values\n",
    "        id_ls = list(set(id_l).intersection(set(id_s)))\n",
    "        trans_us_df.set_index(['l'], inplace=True, drop=False)\n",
    "        trans_s_df = trans_us_df.loc[id_ls]\n",
    "\n",
    "        id_su = trans_s_df['u'].values\n",
    "        id_sl = trans_s_df['l'].values\n",
    "        states_u_df = states_unc_df.loc[id_su]\n",
    "        states_l_df = states_unc_df.loc[id_sl]\n",
    "\n",
    "        Ep = states_u_df['E'].values.astype('float')\n",
    "        Epp = states_l_df['E'].values.astype('float')\n",
    "        trans_s_df['v'] = cal_v(Ep, Epp)\n",
    "        trans_s_df = trans_s_df[trans_s_df['v'].between(ConversionMinFreq,ConversionMaxFreq)] \n",
    "        trans_s_df.sort_values(by=['v'], inplace=True)\n",
    "        \n",
    "    id_su = trans_s_df['u'].values\n",
    "    id_sl = trans_s_df['l'].values\n",
    "    states_u_df = states_unc_df.loc[id_su]\n",
    "    states_l_df = states_unc_df.loc[id_sl]\n",
    "\n",
    "    Ep = states_u_df['E'].values.astype('float')\n",
    "    Epp = states_l_df['E'].values.astype('float')\n",
    "    gp = states_u_df['g'].values.astype('int')\n",
    "    gpp = states_l_df['g'].values.astype('int')\n",
    "    A = trans_s_df['A'].values.astype('float')\n",
    "    v = trans_s_df['v'].values.astype('float')\n",
    "    unc_u = states_u_df['unc'].values.astype('float')\n",
    "    unc_l = states_l_df['unc'].values.astype('float')\n",
    "    unc = cal_uncertainty(unc_u, unc_l)\n",
    "    \n",
    "    broad_col_name = ['code', 'gamma_L', 'n_air', 'Jpp']\n",
    "    default_broad_df = pd.DataFrame(columns=broad_col_name)\n",
    "    default_gamma_L = 0.07\n",
    "    default_n_air = 0.5\n",
    "    default_broad_df = pd.DataFrame([['code', default_gamma_L, default_n_air,'Jpp']],columns=broad_col_name)\n",
    "    air_broad_df = pd.DataFrame(columns=broad_col_name)\n",
    "    rows = len(id_sl)\n",
    "    pattern_air = read_path + molecule + '/**/*air.broad'\n",
    "    if glob.glob(pattern_air, recursive=True) != []:\n",
    "        for fname_air in glob.glob(pattern_air, recursive=True):\n",
    "            air_broad_df = pd.read_csv(fname_air, sep='\\s+', names=broad_col_name, header=None, engine='python')\n",
    "            gamma_air = extract_broad(air_broad_df,states_l_df)[0]\n",
    "            n_air = extract_broad(air_broad_df,states_l_df)[1]\n",
    "    else:\n",
    "        gamma_air= np.full((1,rows),default_broad_df['gamma_L'][0])[0]\n",
    "        n_air = np.full((1,rows),default_broad_df['n_air'][0])[0]\n",
    "    pattern_self = read_path + molecule + '/**/*self.broad'\n",
    "    if glob.glob(pattern_self, recursive=True) != []:\n",
    "        for fname_self in glob.glob(pattern_self, recursive=True):\n",
    "            self_broad_df = pd.read_csv(fname_self, sep='\\s+', names=broad_col_name, header=None, engine='python')\n",
    "            gamma_self = extract_broad(self_broad_df,states_l_df)[0]\n",
    "    else:\n",
    "        gamma_self= np.full((1,rows),default_broad_df['gamma_L'][0])[0]  \n",
    "    \n",
    "    QN_df = convert_QNFormat_exomol2hitran(states_u_df, states_l_df, GlobalQNLabel_list, GlobalQNFormat_list, \n",
    "                                           LocalQNLabel_list, LocalQNFormat_list)\n",
    "\n",
    "    return (A, v, Ep, Epp, gp, gpp, unc, gamma_air, gamma_self, n_air, QN_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_code(unc):\n",
    "    unc[(1<=unc)] = '000000'\n",
    "    unc[(0.1<=unc) & (unc<1)] = '140000'\n",
    "    unc[(0.01<=unc) & (unc<0.1)] = '240000'\n",
    "    unc[(0.001<=unc) & (unc<0.01)] = '340000'\n",
    "    unc[(0.0001<=unc) & (unc<0.001)] = '440000'\n",
    "    unc[(0.00001<=unc) & (unc<0.0001)] = '540000'\n",
    "    unc[(0.000001<=unc) & (unc<0.00001)] = '640000'\n",
    "    unc[(0.0000001<=unc) & (unc<0.000001)] = '740000'\n",
    "    unc[(0.00000001<=unc) & (unc<0.0000001)] = '840000'\n",
    "    unc[(unc<0.00000001)] = '940000'\n",
    "    unc = unc.astype(int)\n",
    "    return(unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_exomol2hitran(read_path, states_df, trans_part_df, ncolumn):\n",
    "    states_unc_df = read_unc_states(states_df)\n",
    "    A, v, Ep, Epp, gp, gpp, unc, gamma_air, gamma_self, n_air, QN_df = linelist_ExoMol2HITRAN(states_unc_df,trans_part_df, ncolumn)\n",
    "    Q = read_exomol_pf(read_path, Tref)\n",
    "    I = cal_abscoefs(Tref, v, gp, A, Epp, Q, abundance)\n",
    "    unc = error_code(unc)\n",
    "    nrows = len(A)\n",
    "    delta_air = ['']*nrows \n",
    "    iref = ['']*nrows \n",
    "    flag = ['']*nrows \n",
    "    '''\n",
    "    hitran_column_name = ['M','I','v','S','A','gamma_air','gamma_self',\n",
    "                          'E\"','n_air','delta_air','Vp','Vpp','Qp','Qpp',\n",
    "                          'Ierr','Iref','flag','gp','gpp']\n",
    "    '''\n",
    "    hitran_begin_dic = {'M':molecule_id, 'I':isotopologue_id, 'v':v, 'S':I, 'A':A, \n",
    "                        'gamma_air':gamma_air,'gamma_self':gamma_self,'E\"':Epp,'n_air':n_air,'delta_air':delta_air}\n",
    "    hitran_begin_df = pd.DataFrame(hitran_begin_dic)\n",
    "    hitran_end_dic = {'Error':unc,'Iref':iref,'*':flag,\"g'\":gp, 'g\"':gpp}\n",
    "    hitran_end_df = pd.DataFrame(hitran_end_dic)\n",
    "\n",
    "    hitran_res_df = pd.concat([hitran_begin_df, QN_df, hitran_end_df], axis='columns')\n",
    "    if ConversionThreshold != 'None':\n",
    "        hitran_res_df = hitran_res_df[hitran_res_df['I'] >= ConversionThreshold]\n",
    "    hitran_res_df = hitran_res_df.sort_values('v')\n",
    "    return(hitran_res_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion_exomol2hitran(read_path, states_df, trans_part_df, ncolumn):\n",
    "    \n",
    "    print('Convert data from the ExoMol format to the HITRAN format.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    \n",
    "    hitran_res_df = convert_exomol2hitran(read_path, states_df, trans_part_df, ncolumn)\n",
    "        \n",
    "    conversion_folder = save_path + '/conversion/ExoMol2HITRAN/'\n",
    "    if os.path.exists(conversion_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(conversion_folder, exist_ok=True)  \n",
    "    conversion_path = conversion_folder + isotopologue + '__' + dataset + '.par'\n",
    "    hitran_format = \"%2s%1s%12.6f%10.3E%10.3E%5.3f%5.3f%10.4f%4.2f%8s%15s%15s%15s%15s%6s%12s%1s%7.1f%7.1f\"\n",
    "    np.savetxt(conversion_path, hitran_res_df, fmt=hitran_format)\n",
    "\n",
    "    t.end()\n",
    "    print('Converted par file has been saved!\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states_df = read_all_states(read_path)\n",
    "# (trans_part_df, ncolumn) = read_part_trans(read_path)\n",
    "# conversion_exomol2hitran(read_path, states_df, trans_part_df, ncolumn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HITRAN to ExoMol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_QNValues_hitran2exomol(hitran2exomol_states_df, GlobalQNLabel_list, LocalQNLabel_list):\n",
    "    QNLabel_list = GlobalQNLabel_list+LocalQNLabel_list\n",
    "    if 'Gtot' in QNLabel_list:\n",
    "        hitran2exomol_states_df[\"Gtot\"] = (hitran2exomol_states_df[\"Gtot\"]\n",
    "                                           .str.replace('A1',\"1'\").str.replace('A2',\"2'\").str.replace(\"E'\",'3')\n",
    "                                           .str.replace('A1\"','4').str.replace('A2\"','5').str.replace('E\"','6'))    \n",
    "    if 'Gvib' in QNLabel_list:\n",
    "        hitran2exomol_states_df[\"Gvib\"] = (hitran2exomol_states_df[\"Gvib\"]\n",
    "                                           .str.replace('A1',\"1'\").str.replace('A2',\"2'\").str.replace(\"E'\",'3')\n",
    "                                           .str.replace('A1\"','4').str.replace('A2\"','5').str.replace('E\"','6'))    \n",
    "    if 'Grot' in QNLabel_list:\n",
    "        hitran2exomol_states_df[\"Grot\"] = (hitran2exomol_states_df[\"Grot\"]\n",
    "                                           .str.replace('A1',\"1'\").str.replace('A2',\"2'\").str.replace(\"E'\",'3')\n",
    "                                           .str.replace('A1\"','4').str.replace('A2\"','5').str.replace('E\"','6'))                  \n",
    "    if 'taui' in QNLabel_list:\n",
    "        hitran2exomol_states_df[\"taui\"] = hitran2exomol_states_df[\"taui\"].str.replace('s','0').str.replace('a','1')\n",
    "    return(hitran2exomol_states_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hitran2StatesTrans(hitran_df, QNu_df, QNl_df):\n",
    "    hitran2exomol_upper_df = pd.concat([hitran_df[['A','v','gp','Unc','Ep']],QNu_df], axis=1, join='inner')\n",
    "    hitran2exomol_lower_df = pd.concat([hitran_df[['A','v','gpp','Unc','Epp']],QNl_df], axis=1, join='inner')\n",
    "    Jpp_df = hitran2exomol_lower_df['J']\n",
    "    # hitran2exomol_upper_df['F'] = cal_F(hitran2exomol_upper_df['gp']).astype(str) \n",
    "    # hitran2exomol_lower_df['F'] = cal_F(hitran2exomol_lower_df['gpp']).astype(str) \n",
    "\n",
    "    hitran2exomol_upper_df.columns = list(map(lambda x: x.replace('gp','g').replace('Ep','E'), list(hitran2exomol_upper_df.columns)))\n",
    "    hitran2exomol_lower_df.columns = list(map(lambda x: x.replace('gpp','g').replace('Epp','E'), list(hitran2exomol_lower_df.columns)))\n",
    "\n",
    "    # hitran2exomol_lower_df = hitran2exomol_lower_df.reset_index(drop=True)\n",
    "    # hitran2exomol_upper_df = hitran2exomol_upper_df.reset_index(drop=True)\n",
    "    # if ('Sym\"' in QNl_col) and (\"Sym'\" not in QNu_col):\n",
    "    #     hitran2exomol_upper_df['Sym'] = hitran2exomol_lower_df['Sym']\n",
    "    #     index_change_sym = np.where(np.array(hitran2exomol_lower_df['J']-hitran2exomol_upper_df['J'])==0.0)[0]\n",
    "    #     hitran2exomol_upper_df['Sym'][index_change_sym] = (hitran2exomol_upper_df['Sym'][index_change_sym]\n",
    "    #                                                        .replace('e','e2f').replace('f','e').replace('e2f','f'))\n",
    "        \n",
    "    hitran2exomol_ul_df = pd.concat([hitran2exomol_upper_df, hitran2exomol_lower_df], axis=0)\n",
    "\n",
    "    hitranQNlabels = [x for x in list(hitran2exomol_ul_df.columns)[5:] if x != 'M' and x != 'm']\n",
    "    hitran2exomol_states_noid = (hitran2exomol_ul_df.loc[hitran2exomol_ul_df.groupby(['g']+hitranQNlabels)['Unc'].idxmax()][['E','g','Unc']+hitranQNlabels]\n",
    "                                .drop_duplicates().sort_values('E').groupby(['E','g']+hitranQNlabels)['Unc'].max().reset_index())\n",
    "    hitran2exomol_states_id = hitran2exomol_states_noid\n",
    "    hitran2exomol_states_id['id'] = hitran2exomol_states_noid.index+1\n",
    "\n",
    "    states_columns_order = ['id','E','g','F','Unc']+[x for x in hitranQNlabels if x != 'F']\n",
    "    hitran2exomol_states_id = hitran2exomol_states_id[states_columns_order]\n",
    "\n",
    "    # Transitions\n",
    "    upper_QNlabel = list(hitran2exomol_upper_df.columns[5:])\n",
    "    upper_idAv = hitran2exomol_upper_df.merge(hitran2exomol_states_id, on=['g']+upper_QNlabel, how='inner').drop(columns=upper_QNlabel)\n",
    "    upper_idAv['diffE'] = np.abs(upper_idAv['E_x']-upper_idAv['E_y'])\n",
    "    upper_AvEid = upper_idAv.loc[upper_idAv.groupby(['v'])['diffE'].idxmin()][['id','A','v','E_y']].rename(columns={'id':'u','E_y':'Ep'})\n",
    "\n",
    "    lower_QNlabel = list(hitran2exomol_lower_df.columns[5:])\n",
    "    lower_idAv = hitran2exomol_lower_df.merge(hitran2exomol_states_id, on=['g']+lower_QNlabel, how='inner').drop(columns=lower_QNlabel)\n",
    "    lower_idAv['diffE'] = np.abs(lower_idAv['E_x']-lower_idAv['E_y'])\n",
    "    lower_AvEid = lower_idAv.loc[lower_idAv.groupby(['v'])['diffE'].idxmin()][['id','A','v','E_y',]].rename(columns={'id':'l','E_y':'Epp'})\n",
    "\n",
    "    hitran2exomol_idAv = upper_AvEid.merge(lower_AvEid, on=['v'], how='inner').drop(columns=['A_y']).rename(columns={'A_x':'A','v':'v_x'})\n",
    "    hitran2exomol_idAv['v'] = hitran2exomol_idAv['Ep']-hitran2exomol_idAv['Epp']\n",
    "    diff = hitran2exomol_idAv[['u','l','A','v','v_x']]\n",
    "    diff['diffv'] = np.abs(diff['v_x'] - diff['v'])\n",
    "    hitran2exomol_trans_df = diff.loc[diff.groupby(['v'])['diffv'].idxmin()][['u','l','A','v']].sort_values('v')\n",
    "    \n",
    "    # States\n",
    "    hitran2exomol_states_df = convert_QNValues_hitran2exomol(hitran2exomol_states_id, GlobalQNLabel_list, LocalQNLabel_list)\n",
    "    hitran2exomol_states_df['Unc'] = (hitran2exomol_states_df['Unc'].replace(10,1e-09).replace(9,1e-08)\n",
    "                                      .replace(8,1e-07).replace(7,1e-06).replace(6,1e-05).replace(5,1e-04)\n",
    "                                      .replace(4,0.001).replace(3,0.01).replace(2,0.1).replace(0,10))\n",
    "    \n",
    "    return(Jpp_df, hitran2exomol_states_df, hitran2exomol_trans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hitran2broad(hitran_df, Jpp_df):\n",
    "    broad_code_df = pd.DataFrame(np.full_like(Jpp_df.astype(str),'a0'), columns=['code'])\n",
    "    hitran2exomol_air_df = pd.concat([broad_code_df, hitran_df[['gamma_air','n_air']], Jpp_df], axis=1).drop_duplicates().dropna()\n",
    "    hitran2exomol_self_df = pd.concat([broad_code_df, hitran_df[['gamma_self','n_air']], Jpp_df], axis=1).drop_duplicates().dropna()\n",
    "    return(hitran2exomol_air_df, hitran2exomol_self_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion_states(hitran2exomol_states_df, conversion_folder):\n",
    "    print('Convert data from the HITRAN format to the ExoMol format states.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    \n",
    "    conversion_states_path = conversion_folder + isotopologue + '__' + dataset + '.states.bz2'\n",
    "    hitranQNlabels_noF = hitran2exomol_states_df.columns[5:].tolist()\n",
    "    hitranQNformats = [QNsformat_list[j] for j in [QNslabel_list.index(i) for i in hitranQNlabels_noF]]\n",
    "\n",
    "    # states_format = (\"%12s %12.6f %6s %7s %12.6f \" \n",
    "    #                  + str(QNsformat_list).replace(\"['\",\"\").replace(\"']\",\"\")\n",
    "    #                  .replace(\"'\",\"\").replace(\",\",\"\").replace(\"d\",\"s\").replace(\"i\",\"s\"))\n",
    "    states_format = (\"%12s %12.6f %6s %7s %12.6f \" \n",
    "                    + str(hitranQNformats).replace(\"['\",\"\").replace(\"']\",\"\")\n",
    "                    .replace(\"'\",\"\").replace(\",\",\"\").replace(\"d\",\"s\").replace(\"i\",\"s\"))\n",
    "    np.savetxt(conversion_states_path, hitran2exomol_states_df, fmt=states_format)\n",
    "    \n",
    "    t.end()\n",
    "    print('Converted states file has been saved!\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion_trans(hitran2exomol_trans_df, conversion_folder): \n",
    "    print('Convert data from the HITRAN format to the ExoMol format transitions.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "\n",
    "    conversion_trans_path = conversion_folder + isotopologue + '__' + dataset + '.trans.bz2'\n",
    "    trans_format = \"%12d %12d %10.4e %15.6f\"\n",
    "    np.savetxt(conversion_trans_path, hitran2exomol_trans_df, fmt=trans_format)\n",
    "\n",
    "    t.end()\n",
    "    print('Converted transition file has been saved!\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion_broad(hitran2exomol_air_df, hitran2exomol_self_df, conversion_folder):\n",
    "    print('Convert data from the HITRAN format to the ExoMol format broadening.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "\n",
    "    nair = len(hitran2exomol_air_df)\n",
    "    nself = len(hitran2exomol_self_df)\n",
    "    nbroad = nair + nself\n",
    "    broad_format = \"%2s %6.4f %6.3f %7s\"    \n",
    "    if nair != 0:\n",
    "        conversion_airbroad_path = conversion_folder + isotopologue + '__air.broad'\n",
    "        np.savetxt(conversion_airbroad_path, hitran2exomol_air_df, fmt=broad_format)\n",
    "    else:\n",
    "        print('No air broadening file.')\n",
    "    if nself != 0:\n",
    "        conversion_selfbroad_path = conversion_folder + isotopologue + '__self.broad'\n",
    "        np.savetxt(conversion_selfbroad_path, hitran2exomol_self_df, fmt=broad_format)\n",
    "    else:\n",
    "        print('No self broadening file.')\n",
    "        \n",
    "    t.end()\n",
    "    if nbroad != 0:\n",
    "        print('Convert broadening files have been saved!\\n')  \n",
    "    else:\n",
    "        print('No broadening files need to be saved!\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion_hitran2exomol(hitran_df):\n",
    "\n",
    "    hitran_df = hitran_df[~hitran_df['Vp'].isin([' '*15])]\n",
    "    hitran_df = hitran_df[~hitran_df['Vpp'].isin([' '*15])]\n",
    "    hitran_df = hitran_df[~hitran_df['Qp'].isin([' '*15])]\n",
    "    hitran_df = hitran_df[~hitran_df['Qpp'].isin([' '*15])]\n",
    "    GlobalQNLabels,GlobalQNFormats = globalQNclasses(molecule,isotopologue)\n",
    "    LocalQNupperLabels, LocalQNlowerLabels, LocalQNupperFormats, LocalQNlowerFormats = localQNgroups(molecule,isotopologue)\n",
    "    QNu_df, QNl_df, QNu_col, QNl_col = separate_QN_hitran(hitran_df,GlobalQNLabels,LocalQNupperLabels,LocalQNlowerLabels,\n",
    "                                                          GlobalQNFormats,LocalQNupperFormats,LocalQNlowerFormats)\n",
    "    hitran_df['Ep'] = cal_Ep(hitran_df['Epp'].values,hitran_df['v'].values)\n",
    "    \n",
    "    Jpp_df, hitran2exomol_states_df, hitran2exomol_trans_df = convert_hitran2StatesTrans(hitran_df, QNu_df, QNl_df)\n",
    "    \n",
    "    hitran2exomol_air_df, hitran2exomol_self_df = convert_hitran2broad(hitran_df, Jpp_df)\n",
    "    \n",
    "    conversion_folder = save_path+'/conversion/HITRAN2ExoMol/'+molecule+'/'+isotopologue+'/'+dataset+'/' \n",
    "    if os.path.exists(conversion_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(conversion_folder, exist_ok=True)      \n",
    "    \n",
    "    conversion_states(hitran2exomol_states_df, conversion_folder)\n",
    "    conversion_trans(hitran2exomol_trans_df, conversion_folder)\n",
    "    conversion_broad(hitran2exomol_air_df, hitran2exomol_self_df, conversion_folder)\n",
    "    print('Finished conversion!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parfile_df = read_parfile(read_path)\n",
    "# hitran_df = read_hitran_parfile(read_path,parfile_df,ConversionMinFreq,ConversionMaxFreq,\n",
    "#                                 ConversionUnc,ConversionThreshold).reset_index().drop(columns='index')\n",
    "# conversion_hitran2exomol(hitran_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stick Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linelist_StickSpectra(states_part_df,trans_part_df, ncolumn):\n",
    "    if ncolumn == 4:\n",
    "        trans_part_df = trans_part_df[trans_part_df['v'].between(min_wn, max_wn)] \n",
    "        id_u = trans_part_df['u'].values\n",
    "        id_s = states_part_df['id'].values\n",
    "        trans_part_df.set_index(['u'], inplace=True, drop=False)\n",
    "        id_us = list(set(id_u).intersection(set(id_s)))\n",
    "        trans_us_df = trans_part_df.loc[id_us]\n",
    "        id_l = trans_us_df['l'].values\n",
    "        id_ls = list(set(id_l).intersection(set(id_s)))\n",
    "        if id_ls != []:\n",
    "            trans_us_df.set_index(['l'], inplace=True, drop=False)\n",
    "            trans_s_df = trans_us_df.loc[id_ls]\n",
    "            trans_s_df.sort_values(by=['v'], inplace=True)\n",
    "            id_su = trans_s_df['u'].values\n",
    "            id_sl = trans_s_df['l'].values\n",
    "            states_u_df = states_part_df.loc[id_su]\n",
    "            states_l_df = states_part_df.loc[id_sl]\n",
    "            Ep = states_u_df['E'].values.astype('float')\n",
    "            Epp = states_l_df['E'].values.astype('float')\n",
    "            gp = states_u_df['g'].values.astype('int')\n",
    "            Jp = pd.to_numeric(states_u_df['J']).values\n",
    "            Jpp = pd.to_numeric(states_l_df['J']).values\n",
    "            A = trans_s_df['A'].values.astype('float')\n",
    "            v = trans_s_df['v'].values.astype('float')\n",
    "            QNp = pd.DataFrame()\n",
    "            QNpp = pd.DataFrame()\n",
    "            for i in range(len(QNs_label)):\n",
    "                QNp[QNs_label[i]+\"'\"] = states_u_df[QNs_label[i]].values\n",
    "                QNpp[QNs_label[i]+'\"'] = states_l_df[QNs_label[i]].values\n",
    "            stick_qn_df = pd.concat([QNp,QNpp],axis='columns')\n",
    "        else:\n",
    "            raise ImportError(\"Empty result with the input filter values. Please type new filter values in the input file.\")  \n",
    "    else:\n",
    "        id_u = trans_part_df['u'].values\n",
    "        id_s = states_part_df['id'].values\n",
    "        trans_part_df.set_index(['u'], inplace=True, drop=False)\n",
    "        id_us = list(set(id_u).intersection(set(id_s)))\n",
    "        trans_us_df = trans_part_df.loc[id_us]\n",
    "        id_l = trans_us_df['l'].values\n",
    "        id_ls = list(set(id_l).intersection(set(id_s)))\n",
    "        trans_us_df.set_index(['l'], inplace=True, drop=False)\n",
    "        trans_s_df = trans_us_df.loc[id_ls]\n",
    "        id_su = trans_s_df['u'].values\n",
    "        id_sl = trans_s_df['l'].values\n",
    "        states_u_df = states_part_df.loc[id_su]\n",
    "        states_l_df = states_part_df.loc[id_sl]\n",
    "        trans_s_df['Ep'] = states_u_df['E'].values.astype('float')\n",
    "        trans_s_df['Epp'] = states_l_df['E'].values.astype('float')\n",
    "        trans_s_df['gp'] = states_u_df['g'].values.astype('int')\n",
    "        trans_s_df['Jp'] = pd.to_numeric(states_u_df['J']).values\n",
    "        trans_s_df['Jpp'] = pd.to_numeric(states_l_df['J']).values\n",
    "        trans_s_df['A'] = trans_s_df['A'].values.astype('float')\n",
    "        trans_s_df['v'] = cal_v(trans_s_df['Ep'].values, trans_s_df['Epp'].values)\n",
    "        trans_s_df = trans_s_df[trans_s_df['v'].between(min_wn, max_wn)]\n",
    "        if len(trans_s_df) != 0:\n",
    "            trans_s_df.sort_values(by=['v'], inplace=True) \n",
    "            Ep = trans_s_df['Ep'].values\n",
    "            Epp = trans_s_df['Epp'].values\n",
    "            gp = trans_s_df['gp'].values\n",
    "            Jp = trans_s_df['Jp'].values\n",
    "            Jpp = trans_s_df['Jpp'].values\n",
    "            A = trans_s_df['A'].values\n",
    "            v = trans_s_df['v'].values\n",
    "            id_su = trans_s_df['u'].values\n",
    "            states_u_df = states_part_df.loc[id_su]        \n",
    "            id_sl = trans_s_df['l'].values\n",
    "            states_l_df = states_part_df.loc[id_sl]\n",
    "            QNp = pd.DataFrame()\n",
    "            QNpp = pd.DataFrame()\n",
    "            for i in range(len(QNs_label)):\n",
    "                QNp[QNs_label[i]+\"'\"] = states_u_df[QNs_label[i]].values\n",
    "                QNpp[QNs_label[i]+'\"'] = states_l_df[QNs_label[i]].values\n",
    "            stick_qn_df = pd.concat([QNp,QNpp],axis='columns')\n",
    "        else:\n",
    "            raise ImportError(\"Empty result with the input filter values. Please type new filter values in the input file.\")  \n",
    "    return (A, v, Ep, Epp, gp, Jp, Jpp, stick_qn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exomol_stick_spectra(read_path, states_part_df, trans_part_df, ncolumn, T):\n",
    "    print('Calculate stick spectra.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    A, v, Ep, Epp, gp, Jp, Jpp, stick_qn_df = linelist_StickSpectra(states_part_df,trans_part_df, ncolumn)\n",
    "    Q = read_exomol_pf(read_path, T)\n",
    "    # Absorption or emission stick spectra.\n",
    "    if abs_emi == 'Ab':\n",
    "        print('Absorption stick spectra')\n",
    "        I = cal_abscoefs(T, v, gp, A, Epp, Q, abundance)\n",
    "    if abs_emi == 'Em':\n",
    "        print('Emission stick spectra')\n",
    "        I = cal_emicoefs(T, v, gp, A, Ep, Q, abundance)\n",
    "    stick_st_dic = {'v':v, 'S':I, \"J'\":Jp, \"E'\":Ep, 'J\"':Jpp, 'E\"':Epp}\n",
    "    stick_st_df = pd.DataFrame(stick_st_dic)\n",
    "    stick_spectra_df = pd.concat([stick_st_df, stick_qn_df], axis='columns')\n",
    "    if threshold != 'None':\n",
    "        stick_spectra_df = stick_spectra_df[stick_spectra_df['I'] >= threshold]\n",
    "    stick_spectra_df = QNfilter_linelist(stick_spectra_df, QNs_value, QNs_label)\n",
    "    stick_spectra_df = stick_spectra_df.sort_values('v')   \n",
    "       \n",
    "    QNsfmf = (str(QNs_format).replace(\"'\",\"\").replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "              .replace('d','s').replace('i','s').replace('.1f','s'))\n",
    "    ss_folder = save_path + '/stick_spectra/stick/'+molecule+'/'+database+'/'\n",
    "    if os.path.exists(ss_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(ss_folder, exist_ok=True)\n",
    "    ss_path = ss_folder + isotopologue + '__' + dataset + '.stick'\n",
    "    ss_colname = stick_spectra_df.columns\n",
    "    fmt = '%12.8E %12.8E %7s %12.4f %7s %12.4f '+QNsfmf+' '+QNsfmf\n",
    "    np.savetxt(ss_path, stick_spectra_df, fmt=fmt, header='')\n",
    "    \n",
    "    # Plot stick spectra and save it as .png.\n",
    "    if PlotStickSpectraYN == 'Y':\n",
    "        from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "        parameters = {'axes.labelsize': 14, \n",
    "                    'legend.fontsize': 14,\n",
    "                    'xtick.labelsize': 12,\n",
    "                    'ytick.labelsize': 12}\n",
    "        plt.rcParams.update(parameters)\n",
    "        ss_plot_folder = save_path + '/stick_spectra/plots/'+molecule+'/'+database+'/'\n",
    "        if os.path.exists(ss_plot_folder):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(ss_plot_folder, exist_ok=True)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.xlim([min_wn, max_wn])\n",
    "        plt.ylim([limitYaxisStick, 10*max(I)])\n",
    "        plt.vlines(stick_spectra_df['v'], 0, stick_spectra_df['S'], label='T = '+str(T)+' K', linewidth=0.4)\n",
    "        plt.semilogy()\n",
    "        #plt.title(database+' '+molecule+' intensity') \n",
    "        plt.xlabel('Wavenumber, cm$^{-1}$')\n",
    "        plt.ylabel('Intensity, cm/molecule')\n",
    "        plt.legend()\n",
    "        leg = plt.legend()                  # Get the legend object.\n",
    "        for line in leg.get_lines():\n",
    "            line.set_linewidth(1.0)         # Change the line width for the legend.\n",
    "        plt.savefig(ss_plot_folder+molecule+'__T'+str(T)+'__'+str(min_wn)\n",
    "                    +'-'+str(max_wn)+'__'+database+'__'+abs_emi+'.png', dpi=500)\n",
    "        plt.show()\n",
    "        print('Stick spectra plot saved.')\n",
    "    t.end()\n",
    "    print('Stick spectra have been saved!\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitran_stick_spectra(hitran_linelist_df, QNs_col, T):\n",
    "    print('Calculate stick spectra.')  \n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    A, v, Ep, Epp, gp, n_air, gamma_air, gamma_self, delta_air = linelist_hitran(hitran_linelist_df)\n",
    "    Q = read_hitran_pf(T)\n",
    "    # Absorption or emission stick spectra.\n",
    "    if abs_emi == 'Ab': \n",
    "        print('Absorption stick spectra') \n",
    "        I = cal_abscoefs(T, v, gp, A, Epp, Q, abundance)\n",
    "    elif abs_emi == 'Em': \n",
    "        print('Emission stick spectra')\n",
    "        I = cal_emicoefs(T, v, gp, A, Ep, Q, abundance)\n",
    "    else:\n",
    "        raise ImportError(\"Please choose one from: 'Absoption' or 'Emission'.\")\n",
    "    \n",
    "    ss_colname = ['v','S',\"J'\",\"E'\",'J\"','E\"'] + QNs_col\n",
    "    stick_spectra_df = hitran_linelist_df[ss_colname]\n",
    "    stick_spectra_df = stick_spectra_df.sort_values('v')  \n",
    "    # QNs_col_ul = [qn.replace(\"'\",\"\").replace('\"','') for qn in QNs_col]\n",
    "\n",
    "    QN_format_noJ = [QNsformat_list[i] for i in [QNslabel_list.index(j) for j in QNs_label]]\n",
    "    QNsfmf = (str(QN_format_noJ).replace(\"'\",\"\").replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "              .replace('d','s').replace('i','s').replace('.1f','s'))\n",
    "    ss_folder = save_path + '/stick_spectra/stick/'+molecule+'/'+database+'/'\n",
    "    if os.path.exists(ss_folder):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(ss_folder, exist_ok=True)\n",
    "    ss_path = ss_folder + isotopologue + '__' + dataset + '.stick'\n",
    "    fmt = '%12.8E %12.8E %7s %12.4f %7s %12.4f ' + QNsfmf + ' ' + QNsfmf\n",
    "    np.savetxt(ss_path, stick_spectra_df, fmt=fmt, header='')\n",
    "    \n",
    "    # Plot stick spectra and save it as .png.\n",
    "    if PlotStickSpectraYN == 'Y':\n",
    "        from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "        parameters = {'axes.labelsize': 14, \n",
    "                    'legend.fontsize': 14,\n",
    "                    'xtick.labelsize': 12,\n",
    "                    'ytick.labelsize': 12}\n",
    "        plt.rcParams.update(parameters)\n",
    "        ss_plot_folder = save_path + '/stick_spectra/plots/'+molecule+'/'+database+'/'\n",
    "        if os.path.exists(ss_plot_folder):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(ss_plot_folder, exist_ok=True)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.xlim([min_wn, max_wn])\n",
    "        plt.ylim([limitYaxisStick, 10*max(I)])\n",
    "        plt.vlines(stick_spectra_df['v'], 0, stick_spectra_df['S'], label='T = '+str(T)+' K', linewidth=0.4)\n",
    "        plt.semilogy()\n",
    "        #plt.title(database+' '+molecule+' intensity') \n",
    "        plt.xlabel('Wavenumber, cm$^{-1}$')\n",
    "        plt.ylabel('Intensity, cm/molecule')\n",
    "        plt.legend()\n",
    "        leg = plt.legend()                  # Get the legend object.\n",
    "        for line in leg.get_lines():\n",
    "            line.set_linewidth(1.0)         # Change the line width for the legend.\n",
    "        plt.savefig(ss_plot_folder+molecule+'__T'+str(T)+'__'+str(min_wn)\n",
    "                    +'-'+str(max_wn)+'__'+database+'__'+abs_emi+'.png', dpi=500)\n",
    "        plt.show()\n",
    "        print('Stick spectra plot saved.')\n",
    "    t.end()\n",
    "    print('Stick spectra have been saved!\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = 300\n",
    "# states_df = read_all_states(read_path)\n",
    "# states_part_df = read_part_states(states_df)\n",
    "# (trans_part_df, ncolumn) = read_part_trans(read_path)\n",
    "# exomol_stick_spectra(read_path, states_part_df, trans_part_df, ncolumn, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = 300\n",
    "# abs_emi = 'Ab'\n",
    "# PlotStickSpectraYN = 'Y'\n",
    "# parfile_df = read_parfile(read_path)\n",
    "# hitran_df = read_hitran_parfile (read_path,parfile_df,min_wn,max_wn,UncFilter,threshold).reset_index().drop(columns='index')\n",
    "# (hitran_linelist_df, QNs_col) = hitran_linelist_QN(hitran_df)\n",
    "# hitran_stick_spectra(hitran_linelist_df, QNs_col, T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linelist_exomol_abs(cutoff,broad,ratio,nbroad,broad_dfs,states_part_df,trans_part_df, ncolumn): \n",
    "    if ncolumn == 4:\n",
    "        if cutoff == 'None':\n",
    "            trans_part_df = trans_part_df[trans_part_df['v'].between(min_wn, max_wn)] \n",
    "        else:\n",
    "            trans_part_df = trans_part_df[trans_part_df['v'].between(min_wn - cutoff, max_wn + cutoff)]      \n",
    "        id_u = trans_part_df['u'].values\n",
    "        id_s = states_part_df['id'].values\n",
    "        trans_part_df.set_index(['u'], inplace=True, drop=False)\n",
    "        id_us = list(set(id_u).intersection(set(id_s)))\n",
    "        trans_us_df = trans_part_df.loc[id_us]\n",
    "        id_l = trans_us_df['l'].values\n",
    "        id_ls = list(set(id_l).intersection(set(id_s)))\n",
    "        if id_ls != []:\n",
    "            trans_us_df.set_index(['l'], inplace=True, drop=False)\n",
    "            trans_s_df = trans_us_df.loc[id_ls]\n",
    "            trans_s_df.sort_values(by=['v'], inplace=True)\n",
    "            id_su = trans_s_df['u'].values\n",
    "            id_sl = trans_s_df['l'].values\n",
    "            states_u_df = states_part_df.loc[id_su]\n",
    "            states_l_df = states_part_df.loc[id_sl]\n",
    "            Epp = states_l_df['E'].values.astype('float')\n",
    "            gp = states_u_df['g'].values.astype('int')\n",
    "            A = trans_s_df['A'].values.astype('float')\n",
    "            v = trans_s_df['v'].values.astype('float')\n",
    "            if 'tau' in states_part_df.columns.to_list():\n",
    "                tau = states_u_df['tau'].values.astype('float')\n",
    "            else:\n",
    "                tau = np.zeros(len(gp))\n",
    "        else:\n",
    "            raise ImportError(\"Empty result with the input filter values. Please type new filter values in the input file.\")  \n",
    "    else:\n",
    "        id_u = trans_part_df['u'].values\n",
    "        id_s = states_part_df['id'].values\n",
    "        trans_part_df.set_index(['u'], inplace=True, drop=False)\n",
    "        id_us = list(set(id_u).intersection(set(id_s)))\n",
    "        trans_us_df = trans_part_df.loc[id_us]\n",
    "        id_l = trans_us_df['l'].values\n",
    "        id_ls = list(set(id_l).intersection(set(id_s)))\n",
    "        trans_us_df.set_index(['l'], inplace=True, drop=False)\n",
    "        trans_s_df = trans_us_df.loc[id_ls]\n",
    "        id_su = trans_s_df['u'].values\n",
    "        id_sl = trans_s_df['l'].values\n",
    "        states_u_df = states_part_df.loc[id_su]\n",
    "        states_l_df = states_part_df.loc[id_sl]\n",
    "        trans_s_df['Ep'] = states_u_df['E'].values.astype('float')\n",
    "        trans_s_df['Epp'] = states_l_df['E'].values.astype('float')\n",
    "        trans_s_df['gp'] = states_u_df['g'].values.astype('int')\n",
    "        trans_s_df['v'] = cal_v(trans_s_df['Ep'].values, trans_s_df['Epp'].values)\n",
    "        if 'tau' in states_part_df.columns.to_list():\n",
    "            trans_s_df['tau'] = states_u_df['tau'].values.astype('float')\n",
    "        else:\n",
    "            trans_s_df['tau'] = np.zeros(len(gp))\n",
    "        if cutoff == 'None':\n",
    "            trans_s_df = trans_s_df[trans_s_df['v'].between(min_wn, max_wn)]\n",
    "        else:\n",
    "            trans_s_df = trans_s_df[trans_s_df['v'].between(min_wn - cutoff, max_wn + cutoff)]\n",
    "        if len(trans_s_df) != 0:\n",
    "            trans_s_df.sort_values(by=['v'], inplace=True)\n",
    "            Epp = trans_s_df['Epp'].values\n",
    "            gp = trans_s_df['gp'].values\n",
    "            A = trans_s_df['A'].values\n",
    "            v = trans_s_df['v'].values\n",
    "            tau = trans_s_df['tau'].values\n",
    "            id_sl = trans_s_df['l'].values\n",
    "            states_l_df = states_part_df.loc[id_sl]\n",
    "        else:\n",
    "            raise ImportError(\"Empty result with the input filter values. Please type new filter values in the input file.\")  \n",
    "    gamma_L = pd.DataFrame()\n",
    "    n_air = pd.DataFrame()\n",
    "    rows = len(id_sl)\n",
    "    for i in range(nbroad):\n",
    "        if broad[i] == 'Default':\n",
    "            gamma_L[i] = np.full((1,rows),broad_dfs[i]['gamma_L'][0])[0] * ratio[i]\n",
    "            n_air[i] = np.full((1,rows),broad_dfs[i]['n_air'][0])[0] * ratio[i]\n",
    "        else:\n",
    "            gamma_L[i] = extract_broad(broad_dfs[i],states_l_df)[0] * ratio[i]\n",
    "            n_air[i] = extract_broad(broad_dfs[i],states_l_df)[1] * ratio[i]\n",
    "    return (A, v, Epp, gp, tau, gamma_L, n_air)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linelist_exomol_emi(cutoff,broad,ratio,nbroad,broad_dfs,states_part_df,trans_part_df, ncolumn):\n",
    "    if ncolumn == 4:\n",
    "        if cutoff == 'None':\n",
    "            trans_part_df = trans_part_df[trans_part_df['v'].between(min_wn, max_wn)] \n",
    "        else:\n",
    "            trans_part_df = trans_part_df[trans_part_df['v'].between(min_wn - cutoff, max_wn + cutoff)] \n",
    "        id_u = trans_part_df['u'].values\n",
    "        id_s = states_part_df['id'].values\n",
    "        trans_part_df.set_index(['u'], inplace=True, drop=False)\n",
    "        id_us = list(set(id_u).intersection(set(id_s)))\n",
    "        trans_us_df = trans_part_df.loc[id_us]\n",
    "        id_l = trans_us_df['l'].values\n",
    "        id_ls = list(set(id_l).intersection(set(id_s)))\n",
    "        if id_ls != []:\n",
    "            trans_us_df.set_index(['l'], inplace=True, drop=False)\n",
    "            trans_s_df = trans_us_df.loc[id_ls]\n",
    "            trans_s_df.sort_values(by=['v'], inplace=True)\n",
    "            id_su = trans_s_df['u'].values\n",
    "            id_sl = trans_s_df['l'].values\n",
    "            states_u_df = states_part_df.loc[id_su]\n",
    "            states_l_df = states_part_df.loc[id_sl]\n",
    "            Ep = states_u_df['E'].values.astype('float')\n",
    "            gp = states_u_df['g'].values.astype('int')\n",
    "            A = trans_s_df['A'].values.astype('float')\n",
    "            v = trans_s_df['v'].values.astype('float')\n",
    "            if 'tau' in states_part_df.columns.to_list():\n",
    "                tau = states_u_df['tau'].values.astype('float')\n",
    "            else:\n",
    "                tau = np.zeros(len(gp))\n",
    "        else:\n",
    "            raise ImportError(\"Empty result with the input filter values. Please type new filter values in the input file.\")  \n",
    "    else:\n",
    "        id_u = trans_part_df['u'].values\n",
    "        id_s = states_part_df['id'].values\n",
    "        trans_part_df.set_index(['u'], inplace=True, drop=False)\n",
    "        id_us = list(set(id_u).intersection(set(id_s)))\n",
    "        trans_us_df = trans_part_df.loc[id_us]\n",
    "        id_l = trans_us_df['l'].values\n",
    "        id_ls = list(set(id_l).intersection(set(id_s)))\n",
    "        trans_us_df.set_index(['l'], inplace=True, drop=False)\n",
    "        trans_s_df = trans_us_df.loc[id_ls]\n",
    "        id_su = trans_s_df['u'].values\n",
    "        id_sl = trans_s_df['l'].values\n",
    "        states_u_df = states_part_df.loc[id_su]\n",
    "        states_l_df = states_part_df.loc[id_sl]\n",
    "        trans_s_df['Ep'] = states_u_df['E'].values.astype('float')\n",
    "        trans_s_df['Epp'] = states_l_df['E'].values.astype('float')\n",
    "        trans_s_df['gp'] = states_u_df['g'].values.astype('int')\n",
    "        trans_s_df['v'] = cal_v(trans_s_df['Ep'].values, trans_s_df['Epp'].values)\n",
    "        if 'tau' in states_part_df.columns.to_list():\n",
    "            trans_s_df['tau'] = states_u_df['tau'].values.astype('float')\n",
    "        else:\n",
    "            trans_s_df['tau'] = np.zeros(len(gp))\n",
    "        if cutoff == 'None':\n",
    "            trans_s_df = trans_s_df[trans_s_df['v'].between(min_wn, max_wn)]\n",
    "        else:\n",
    "            trans_s_df = trans_s_df[trans_s_df['v'].between(min_wn - cutoff, max_wn + cutoff)]\n",
    "        if len(trans_s_df) != 0:\n",
    "            trans_s_df.sort_values(by=['v'], inplace=True)\n",
    "            Ep = trans_s_df['Ep'].values\n",
    "            gp = trans_s_df['gp'].values\n",
    "            A = trans_s_df['A'].values\n",
    "            v = trans_s_df['v'].values\n",
    "            tau = trans_s_df['tau'].values\n",
    "            id_sl = trans_s_df['l'].values\n",
    "            states_l_df = states_part_df.loc[id_sl]\n",
    "        else:\n",
    "            raise ImportError(\"Empty result with the input filter values. Please type new filter values in the input file.\")  \n",
    "    gamma_L = pd.DataFrame()\n",
    "    n_air = pd.DataFrame()\n",
    "    rows = len(id_sl)\n",
    "    for i in range(nbroad):\n",
    "        if broad[i] == 'Default':\n",
    "            gamma_L[i] = np.full((1,rows),broad_dfs[i]['gamma_L'][0])[0] * ratio[i]\n",
    "            n_air[i] = np.full((1,rows),broad_dfs[i]['n_air'][0])[0] * ratio[i]\n",
    "        else:\n",
    "            gamma_L[i] = extract_broad(broad_dfs[i],states_l_df)[0] * ratio[i]\n",
    "            n_air[i] = extract_broad(broad_dfs[i],states_l_df)[1] * ratio[i]\n",
    "    return (A, v, Ep, gp, tau, gamma_L, n_air)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Doppler_HWHM(v,T):\n",
    "    '''Return the Doppler half-width at half-maximum (HWHM) -- alpha.'''\n",
    "    # alpha = np.sqrt(2 * N_A * kB * T * np.log(2) / mass) * v / c\n",
    "    alpha = ne.evaluate('Sqrt2NAkBln2mInvc * sqrt(T) * v')\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_standard_deviation(alpha):\n",
    "    '''Return the Gaussian standard deviation -- sigma.'''\n",
    "    # sigma = alpha / np.sqrt(2 * np.log(2))\n",
    "    sigma = ne.evaluate('alpha * InvSqrt2ln2')\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lorentzian_HWHM(gamma_L, n_air,T,P):\n",
    "    '''Return the Lorentzian half-width at half-maximum (HWHM) -- gamma.'''\n",
    "    gamma = ne.evaluate('gamma_L * (Tref / T)**n_air * (P / Pref)')\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifetime_broadening(tau):\n",
    "    '''Return the lifetime broadening -- gamma_tau.'''\n",
    "    gamma_tau = ne.evaluate('1 / (PI4c) * tau')\n",
    "    return gamma_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DopplerHWHM_alpha(num_v, alpha_HWHM, v, T):\n",
    "    if alpha_HWHM != 'None':\n",
    "        alpha = np.full(num_v, alpha_HWHM)\n",
    "    else:\n",
    "        alpha = Doppler_HWHM(v,T)\n",
    "    return(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LorentzianHWHM_gamma(num_v, gamma_HWHM, broad, gamma_L, n_air, gamma_air, gamma_self, tau, T, P):\n",
    "    if gamma_HWHM != 'None':\n",
    "        gamma = np.full(num_v, gamma_HWHM)\n",
    "    else:\n",
    "        if database == 'ExoMol':\n",
    "            gamma_p = pd.DataFrame()\n",
    "            for i in range(len(broad)):\n",
    "                gamma_p[i] = Lorentzian_HWHM (gamma_L[i].values, n_air[i].values,T,P)\n",
    "            gamma_p = gamma_p.sum(axis=1).values  \n",
    "            if check_predissoc == 0 and 'LOR' not in profile:\n",
    "                gamma_tau = lifetime_broadening(tau)\n",
    "                gamma = gamma_p + gamma_tau\n",
    "            else:\n",
    "                gamma = gamma_p\n",
    "        elif database == 'HITRAN':   \n",
    "            gamma_L = gamma_air*0.7 + gamma_self*0.3\n",
    "            gamma = Lorentzian_HWHM(gamma_L, n_air,T,P) \n",
    "    return(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def FWHM(alpha, gamma):\n",
    "#     '''Return the Gaussian full-width at half-maximum (FWHM)   -- fG.\n",
    "#        Return the Lorentzian full-width at half-maximum (FWHM) -- fL.\n",
    "#     '''\n",
    "#     # fG = 2 * sigma * np.sqrt(2 * np.log(2)) = 2 * alpha\n",
    "#     # fG = ne.evaluate('sigma * TwoSqrt2ln2')\n",
    "#     fG = ne.evaluate('2 * alpha')\n",
    "#     fL = ne.evaluate('2 * gamma')\n",
    "#     return (fG, fL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Doppler_profile(dv, alpha):\n",
    "    '''Return Doppler line profile at dv with HWHM alpha.'''\n",
    "    # Doppler_profile = np.sqrt(np.log(2) / np.pi) / alpha * np.exp(-np.log(2) * (dv / alpha)**2) \n",
    "    DopplerProfile = ne.evaluate('Sqrtln2InvPi / alpha * exp(Negln2 * (dv / alpha)**2)')\n",
    "    return DopplerProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lorentzian_profile(dv, gamma):\n",
    "    '''Return Lorentzian line profile at dv with HWHM gamma.'''\n",
    "    LorentzianProfile = ne.evaluate('gamma / PI / (dv**2 + gamma**2)')\n",
    "    return LorentzianProfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SciPyVoigt_profile(dv, sigma, gamma):\n",
    "    '''Return the Voigt line profile with Doppler component HWHM alpha and Lorentzian component HWHM gamma.'''\n",
    "    SciPyVoigtProfile = voigt_profile(dv, sigma, gamma)\n",
    "    return SciPyVoigtProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SciPyWofzVoigt_profile(dv, sigma, gamma):\n",
    "    '''Return the Voigt line profile with Doppler component HWHM alpha and Lorentzian component HWHM gamma.'''\n",
    "    # scipy_wofz_Voigt_profile = np.real(wofz((dv + 1j*gamma)/sigma/np.sqrt(2))) / sigma / np.sqrt(2*np.pi)\n",
    "    z = ne.evaluate('(dv + 1j*gamma)/sigma*InvSqrt2')\n",
    "    wz = wofz(z)\n",
    "    SciPyWofzVoigtProfile = ne.evaluate('real(wz) / sigma * InvSqrt2Pi')\n",
    "    return SciPyWofzVoigtProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Humlicek1(t):\n",
    "    w = ne.evaluate('t * InvSqrtPi / (0.5 + t**2)')\n",
    "    return(w)\n",
    "\n",
    "def Humlicek2(t, u):\n",
    "    w = ne.evaluate('(t*(1.4104739589+u*InvSqrtPi))/(0.75+u*(3+u))')\n",
    "    return(w)\n",
    "\n",
    "def Humlicek3(t):\n",
    "    w = ne.evaluate('(16.4955+t*(20.20933+t*(11.96482+t*(3.778987+0.5642236*t))))/(16.4955+t*(38.82363+t*(39.27121+t*(21.69274+t*(6.699398+t)))))')\n",
    "    return(w)\n",
    "\n",
    "def Humlicek4(t, u):\n",
    "    nom = ne.evaluate('t*(36183.31-u*(3321.99-u*(1540.787-u*(219.031-u*(35.7668-u*(1.320522-u*0.56419))))))')\n",
    "    den = ne.evaluate('32066.6-u*(24322.8-u*(9022.23-u*(2186.18-u*(364.219-u*(61.5704-u*(1.84144-u))))))')\n",
    "    w = ne.evaluate('exp(u)-nom/den')   \n",
    "    return(w)\n",
    "\n",
    "def HumlicekVoigt_profile(dv, alpha, gamma):\n",
    "    x = ne.evaluate('dv * Sqrtln2 / alpha')\n",
    "    y = ne.evaluate('gamma * Sqrtln2 / alpha')\n",
    "    t = ne.evaluate('y-1j*x')\n",
    "    s = ne.evaluate('abs(x)+y')\n",
    "    u = ne.evaluate('t**2')\n",
    "    w = np.zeros_like(alpha)\n",
    "    ybound = ne.evaluate('0.195*abs(x)-0.176')\n",
    "    # Region 1\n",
    "    humfilter1 = s >= 15\n",
    "    t1 = t[humfilter1]\n",
    "    w[humfilter1] = Humlicek1(t1)\n",
    "    # Region 2\n",
    "    humfilter2 = (5.5 <= s) & (s < 15)\n",
    "    t2 = t[humfilter2]\n",
    "    u2 = u[humfilter2]\n",
    "    w[humfilter2] = Humlicek2(t2, u2)\n",
    "    # Region 3\n",
    "    humfilter3 = (s < 5.5) & (y >= ybound)\n",
    "    t3 = t[humfilter3]\n",
    "    w[humfilter3] = Humlicek3(t3)\n",
    "    # Region 4\n",
    "    humfilter4 = (s < 5.5) & (y < ybound)\n",
    "    t4 = t[humfilter4]\n",
    "    u4 = u[humfilter4]\n",
    "    w[humfilter4] = Humlicek4(t4, u4)  \n",
    "    HumlicekVoigtProfile = ne.evaluate('real(w) / alpha * Sqrtln2InvPi')\n",
    "    return HumlicekVoigtProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PseudoVoigt_profile(dv, alpha, gamma, eta, hV):\n",
    "    '''\n",
    "    hv is the Voigt half-width at half-maximum (HWHM), \n",
    "    which can be found from the HWHM of the associated Doppler and Lorentzian profile.\n",
    "    eta is a function of Voigt profile half width at half maximum (HWHM) parameters.\n",
    "    '''\n",
    "    GaussianProfile = Doppler_profile(dv, hV)\n",
    "    LorentzianProfile = Lorentzian_profile(dv, hV)\n",
    "    PseudoVoigtProfile = ne.evaluate('eta * LorentzianProfile + (1 - eta) * GaussianProfile')\n",
    "    return PseudoVoigtProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PseudoThompsonVoigt(alpha, gamma):\n",
    "    '''\n",
    "    hv is the Voigt half-width at half-maximum (HWHM), \n",
    "    which can be found from the HWHM of the associated Doppler and Lorentzian profile.\n",
    "    eta is a function of Voigt profile half width at half maximum (HWHM) parameters.\n",
    "    ''' \n",
    "    hV = ne.evaluate('(alpha**5+2.69269*alpha**4*gamma+2.42843*alpha**3*gamma**2+4.47163*alpha**2*gamma**3+0.07842*alpha*gamma**4+gamma**5)**0.2')\n",
    "    eta = ne.evaluate('1.36603*(gamma/hV) - 0.47719*(gamma/hV)**2 + 0.11116*(gamma/hV)**3')\n",
    "    return (eta, hV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PseudoKielkopfVoigt(alpha, gamma):\n",
    "    '''\n",
    "    hv is the Voigt half-width at half-maximum (HWHM), \n",
    "    which can be found from the HWHM of the associated Doppler and Lorentzian profile.\n",
    "    eta is a function of Voigt profile half width at half maximum (HWHM) parameters.\n",
    "    '''\n",
    "    hV = ne.evaluate('0.5346 * gamma + sqrt(0.2166 * gamma**2 + alpha**2)')\n",
    "    eta = ne.evaluate('1.36603*(gamma/hV) - 0.47719*(gamma/hV)**2 + 0.11116*(gamma/hV)**3')\n",
    "    return (eta, hV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PseudoOliveroVoigt(alpha, gamma):\n",
    "    '''\n",
    "    hv is the Voigt half-width at half-maximum (HWHM), \n",
    "    which can be found from the HWHM of the associated Doppler and Lorentzian profile.\n",
    "    eta is a function of Voigt profile half width at half maximum (HWHM) parameters.\n",
    "    '''\n",
    "    d = ne.evaluate('(gamma-alpha)/(gamma+alpha)')\n",
    "    hV = ne.evaluate('(1-0.18121*(1-d**2)-(0.023665*exp(0.6*d)+0.00418*exp(-1.9*d))*sinPI*d)*(alpha+gamma)')\n",
    "    eta = ne.evaluate('1.36603*(gamma/hV) - 0.47719*(gamma/hV)**2 + 0.11116*(gamma/hV)**3')\n",
    "    return (eta, hV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PseudoLiuLinVoigt(alpha, gamma):\n",
    "    '''\n",
    "    hv is the Voigt half-width at half-maximum (HWHM), \n",
    "    which can be found from the HWHM of the associated Doppler and Lorentzian profile.\n",
    "    eta is a function of Voigt profile half width at half maximum (HWHM) parameters.\n",
    "    '''\n",
    "    d = ne.evaluate('(gamma-alpha)/(gamma+alpha)')\n",
    "    hV = ne.evaluate('(1-0.18121*(1-d**2)-(0.023665*exp(0.6*d)+0.00418*exp(-1.9*d))*sinPI*d)*(alpha+gamma)')\n",
    "    eta = ne.evaluate('0.68188+0.61293*d-0.18384*d**2-0.11568*d**3')\n",
    "    return (eta, hV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PseudoRoccoVoigt(alpha, gamma):\n",
    "    '''\n",
    "    hv is the Voigt half-width at half-maximum (HWHM), \n",
    "    which can be found from the HWHM of the associated Doppler and Lorentzian profile.\n",
    "    eta is a function of Voigt profile half width at half maximum (HWHM) parameters.\n",
    "    '''\n",
    "    y = gamma*Sqrtln2/alpha\n",
    "    erfy = erf(y)\n",
    "    bhalfy = ne.evaluate('y+Sqrtln2*exp(-0.6055*y+0.0718*y**2-0.0049*y**3+0.000136*y**4)')\n",
    "    Vy = ne.evaluate('bhalfy*exp(y**2)*(1-erfy)')\n",
    "    hV = ne.evaluate('alpha / Sqrtln2 * bhalfy')\n",
    "    eta = ne.evaluate('(Vy-Sqrtln2)/(Vy*OneminSqrtPIln2)')\n",
    "    return (eta, hV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinnedGaussian_profile(dv, alpha):\n",
    "    '''Return binned Gaussian line profile at dv with HWHM alpha.'''\n",
    "    erfxpos = erf(ne.evaluate('Sqrtln2*(dv+binSizeHalf)/alpha'))\n",
    "    erfxneg = erf(ne.evaluate('Sqrtln2*(dv-binSizeHalf)/alpha'))\n",
    "    BinnedGaussianProfile = ne.evaluate('erfxpos-erfxneg')\n",
    "    return BinnedGaussianProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinnedLorentzian_profile(dv, gamma, bnormBinsize):\n",
    "    '''Return binned Lorentzian line profile at dv with HWHM gamma.'''\n",
    "    BinnedLorentzianProfile = ne.evaluate('(arctan((dv+binSizeHalf)/gamma)-arctan((dv-binSizeHalf)/gamma))*bnormBinsize')\n",
    "    return BinnedLorentzianProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinnedVoigt_bnormq(wngrid_start, wngrid_end, v, sigma, gamma, x):\n",
    "    '''Return binned Voigt line profile at dv with HWHM gamma.'''\n",
    "    vxsigma = ne.evaluate('v+x*sigma')\n",
    "    bnormq = ne.evaluate('1/(arctan((wngrid_end-vxsigma)/gamma)-arctan((wngrid_start-vxsigma)/gamma))')\n",
    "    return bnormq\n",
    "\n",
    "def BinnedVoigt_lorenz(dv, sigma, gamma, x):\n",
    "    '''Return binned Voigt line profile at dv with HWHM gamma.'''\n",
    "    dvxsigma = ne.evaluate('dv-x*sigma')\n",
    "    lorenz = ne.evaluate('arctan((dvxsigma+binSizeHalf)/gamma)-arctan((dvxsigma-binSizeHalf)/gamma)')\n",
    "    return lorenz\n",
    "\n",
    "def BinnedVoigt_profile(w, bnormq, lorenz):\n",
    "    '''Return binned Voigt line profile at dv with HWHM gamma.'''\n",
    "    BinnedVoigtProfile = ne.evaluate('sum(w*bnormq*lorenz)')\n",
    "    return BinnedVoigtProfile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Cross Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_Doppler(wn_grid, v, alpha, coef, cutoff, threshold):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with Doppler profile.\n",
    "    \n",
    "    '''  \n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if (cutoff == 'None') & (threshold == 'None'):   \n",
    "        start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            Doppler = Doppler_profile(dv, alpha)\n",
    "            _xsec[idx] = ne.evaluate('sum(coef * Doppler)')\n",
    "    elif (cutoff == 'None') & (threshold != 'None'):\n",
    "        filter = coef >= threshold\n",
    "        _alpha = alpha[filter]\n",
    "        if _alpha.size > 0:\n",
    "            _coef = coef[filter]\n",
    "            start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "            end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "            _xsec = np.zeros(shape=(end-start))\n",
    "            for i in range(start,end):\n",
    "                idx = i-start\n",
    "                wn_grid_i = wn_grid[i]\n",
    "                _dv = ne.evaluate('wn_grid_i - v')[filter]\n",
    "                Doppler = Doppler_profile(_dv, _alpha)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * Doppler)')\n",
    "    elif (cutoff != 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _alpha = alpha[filter]\n",
    "                _coef = coef[filter]\n",
    "                Doppler = Doppler_profile(_dv, _alpha)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * Doppler)')\n",
    "    else: \n",
    "        filter_threshold = coef >= threshold\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter_cutoff = np.abs(dv) <= cutoff\n",
    "            filter = filter_cutoff & filter_threshold\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _alpha = alpha[filter]\n",
    "                _coef = coef[filter]\n",
    "                Doppler = Doppler_profile(_dv, _alpha)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * Doppler)')      \n",
    "            \n",
    "    xsec[start:end] += _xsec\n",
    "    return (xsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_Lorentzian(wn_grid, v, gamma, coef, cutoff, threshold):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with Lorentzian profile.\n",
    "    \n",
    "    '''\n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if (cutoff == 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            Lorentzian = Lorentzian_profile(dv, gamma)\n",
    "            _xsec[idx] = ne.evaluate('sum(coef * Lorentzian)')       \n",
    "    elif (cutoff == 'None') & (threshold != 'None'):\n",
    "        filter = coef >= threshold\n",
    "        _gamma = gamma[filter]\n",
    "        if _gamma.size > 0:\n",
    "            _coef = coef[filter]\n",
    "            start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "            end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "            _xsec = np.zeros(shape=(end-start))\n",
    "            for i in range(start,end):\n",
    "                idx = i-start\n",
    "                wn_grid_i = wn_grid[i]\n",
    "                _dv = ne.evaluate('wn_grid_i - v')[filter]\n",
    "                Lorentzian = Lorentzian_profile(_dv, _gamma)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * Lorentzian)')       \n",
    "    elif (cutoff != 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                Lorentzian = Lorentzian_profile(_dv, _gamma)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * Lorentzian)')        \n",
    "    else: \n",
    "        filter_threshold = coef >= threshold\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter_cutoff = np.abs(dv) <= cutoff\n",
    "            filter = filter_cutoff & filter_threshold\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                Lorentzian = Lorentzian_profile(_dv, _gamma)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * Lorentzian)')        \n",
    "            \n",
    "    xsec[start:end] += _xsec\n",
    "    return (xsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_SciPyVoigt(wn_grid, v, sigma, gamma, coef, cutoff, threshold):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with SciPy Voigt profile.\n",
    "    \n",
    "    '''\n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if (cutoff == 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            SciPyVoigt = SciPyVoigt_profile(dv, sigma, gamma)\n",
    "            _xsec[idx] = ne.evaluate('sum(coef * SciPyVoigt)') \n",
    "    elif (cutoff == 'None') & (threshold != 'None'):\n",
    "        filter = coef >= threshold\n",
    "        _sigma = sigma[filter]\n",
    "        if _sigma.size > 0:\n",
    "            _gamma = gamma[filter]\n",
    "            _coef = coef[filter]\n",
    "            start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "            end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "            _xsec = np.zeros(shape=(end-start))\n",
    "            for i in range(start,end):\n",
    "                idx = i-start\n",
    "                wn_grid_i = wn_grid[i]\n",
    "                _dv = ne.evaluate('wn_grid_i - v')[filter]\n",
    "                SciPyVoigt = SciPyVoigt_profile(_dv, _sigma, _gamma)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * SciPyVoigt)') \n",
    "    elif (cutoff != 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _sigma = sigma[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                SciPyVoigt = SciPyVoigt_profile(_dv, _sigma, _gamma)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * SciPyVoigt)') \n",
    "    else: \n",
    "        filter_threshold = coef >= threshold\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter_cutoff = np.abs(dv) <= cutoff\n",
    "            filter = filter_cutoff & filter_threshold\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _sigma = sigma[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                SciPyVoigt = SciPyVoigt_profile(_dv, _sigma, _gamma)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * SciPyVoigt)') \n",
    "            \n",
    "    xsec[start:end] += _xsec\n",
    "    return (xsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_SciPyWofzVoigt(wn_grid, v, sigma, gamma, coef, cutoff, threshold):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with SciPy Wofz Voigt profile.\n",
    "\n",
    "    '''\n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if (cutoff == 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            SciPyWofzVoigt = SciPyWofzVoigt_profile(dv, sigma, gamma)\n",
    "            _xsec[idx] = ne.evaluate('sum(coef * SciPyWofzVoigt)')\n",
    "    elif (cutoff == 'None') & (threshold != 'None'):            \n",
    "        filter = coef >= threshold\n",
    "        _sigma = sigma[filter]\n",
    "        if _sigma.size > 0:\n",
    "            _gamma = gamma[filter]\n",
    "            _coef = coef[filter]\n",
    "            start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "            end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "            _xsec = np.zeros(shape=(end-start))\n",
    "            for i in range(start,end):\n",
    "                idx = i-start\n",
    "                wn_grid_i = wn_grid[i]\n",
    "                _dv = ne.evaluate('wn_grid_i - v')[filter]\n",
    "                SciPyWofzVoigt = SciPyWofzVoigt_profile(_dv, _sigma, _gamma)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * SciPyWofzVoigt)') \n",
    "    elif (cutoff != 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _sigma = sigma[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                SciPyWofzVoigt = SciPyWofzVoigt_profile(_dv, _sigma, _gamma)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * SciPyWofzVoigt)')\n",
    "    else: \n",
    "        filter_threshold = coef >= threshold\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter_cutoff = np.abs(dv) <= cutoff\n",
    "            filter = filter_cutoff & filter_threshold\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _sigma = sigma[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                SciPyWofzVoigt = SciPyWofzVoigt_profile(_dv, _sigma, _gamma)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * SciPyWofzVoigt)')\n",
    "            \n",
    "    xsec[start:end] += _xsec\n",
    "    return (xsec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_HumlicekVoigt(wn_grid, v, alpha, gamma, coef, cutoff, threshold):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with Humlicek Voigt profile.\n",
    "    \n",
    "    '''\n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if (cutoff == 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            HumlicekVoigt = HumlicekVoigt_profile(dv, alpha, gamma)\n",
    "            _xsec[idx] = ne.evaluate('sum(coef * HumlicekVoigt)') \n",
    "    elif (cutoff == 'None') & (threshold != 'None'):\n",
    "        filter = coef >= threshold\n",
    "        _alpha = alpha[filter]\n",
    "        if _alpha.size > 0:\n",
    "            _gamma = gamma[filter]\n",
    "            _coef = coef[filter]\n",
    "            start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "            end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "            _xsec = np.zeros(shape=(end-start))\n",
    "            for i in range(start,end):\n",
    "                idx = i-start\n",
    "                wn_grid_i = wn_grid[i]\n",
    "                _dv = ne.evaluate('wn_grid_i - v')[filter]\n",
    "                HumlicekVoigt = HumlicekVoigt_profile(_dv, _alpha, _gamma)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * HumlicekVoigt)') \n",
    "    elif (cutoff != 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _alpha = alpha[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                HumlicekVoigt = HumlicekVoigt_profile(_dv, _alpha, _gamma)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * HumlicekVoigt)') \n",
    "    else: \n",
    "        filter_threshold = coef >= threshold\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter_cutoff = np.abs(dv) <= cutoff\n",
    "            filter = filter_cutoff & filter_threshold\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _alpha = alpha[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                HumlicekVoigt = HumlicekVoigt_profile(_dv, _alpha, _gamma)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * HumlicekVoigt)') \n",
    "            \n",
    "    xsec[start:end] += _xsec\n",
    "    return (xsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, hV, coef, cutoff, threshold):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with Pseudo Voigt profile.\n",
    "\n",
    "    '''\n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if (cutoff == 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            PseudoVoigt = PseudoVoigt_profile(dv, alpha, gamma, eta, hV)\n",
    "            _xsec[idx] = ne.evaluate('sum(coef * PseudoVoigt)')\n",
    "    elif (cutoff == 'None') & (threshold != 'None'):\n",
    "        filter = coef >= threshold\n",
    "        _alpha = alpha[filter]\n",
    "        if _alpha.size > 0:\n",
    "            _gamma = gamma[filter]\n",
    "            _eta = eta[filter]\n",
    "            _hV = hV[filter]\n",
    "            _coef = coef[filter]\n",
    "            start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "            end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "            _xsec = np.zeros(shape=(end-start))\n",
    "            for i in range(start,end):\n",
    "                idx = i-start\n",
    "                wn_grid_i = wn_grid[i]\n",
    "                _dv = ne.evaluate('wn_grid_i - v')[filter]\n",
    "                PseudoVoigt = PseudoVoigt_profile(_dv, _alpha, _gamma, _eta, _hV)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * PseudoVoigt)')\n",
    "    elif (cutoff != 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _alpha = alpha[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _eta = eta[filter]\n",
    "                _hV = hV[filter]\n",
    "                _coef = coef[filter]\n",
    "                PseudoVoigt = PseudoVoigt_profile(_dv, _alpha, _gamma, _eta, _hV)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * PseudoVoigt)')\n",
    "    else: \n",
    "        filter_threshold = coef >= threshold\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter_cutoff = np.abs(dv) <= cutoff\n",
    "            filter = filter_cutoff & filter_threshold\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _alpha = alpha[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _eta = eta[filter]\n",
    "                _hV = hV[filter]\n",
    "                _coef = coef[filter]\n",
    "                PseudoVoigt = PseudoVoigt_profile(_dv, _alpha, _gamma, _eta, _hV)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * PseudoVoigt)')\n",
    "            \n",
    "    xsec[start:end] += _xsec\n",
    "    return (xsec)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_BinnedGaussian(wn_grid, v, alpha, coef, cutoff, threshold):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with binned Gaussian profile.\n",
    "    \n",
    "    '''  \n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if (cutoff == 'None') & (threshold == 'None'):   \n",
    "        start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            BinnedGaussian = BinnedGaussian_profile(dv, alpha)\n",
    "            _xsec[idx] = ne.evaluate('sum(coef * BinnedGaussian)')\n",
    "    elif (cutoff == 'None') & (threshold != 'None'):            \n",
    "        filter = coef >= threshold\n",
    "        _alpha = alpha[filter]\n",
    "        if _alpha.size > 0:\n",
    "            _coef = coef[filter]\n",
    "            start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "            end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "            _xsec = np.zeros(shape=(end-start))\n",
    "            for i in range(start,end):\n",
    "                idx = i-start\n",
    "                wn_grid_i = wn_grid[i]\n",
    "                _dv = ne.evaluate('wn_grid_i - v')[filter]\n",
    "                BinnedGaussian = BinnedGaussian_profile(_dv, _alpha)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * BinnedGaussian)')\n",
    "    elif (cutoff != 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _alpha = alpha[filter]\n",
    "                _coef = coef[filter]\n",
    "                BinnedGaussian = BinnedGaussian_profile(_dv, _alpha)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * BinnedGaussian)')\n",
    "    else: \n",
    "        filter_threshold = coef >= threshold\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter_cutoff = np.abs(dv) <= cutoff\n",
    "            filter = filter_cutoff & filter_threshold\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _alpha = alpha[filter]\n",
    "                _coef = coef[filter]\n",
    "                BinnedGaussian = BinnedGaussian_profile(_dv, _alpha)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * BinnedGaussian)')     \n",
    "            \n",
    "    xsec[start:end] += _xsec\n",
    "    xsec = ne.evaluate('xsec/binSize2')\n",
    "    return (xsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_BinnedLorentzian(wn_grid, v, gamma, coef, cutoff, threshold):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with binned Lorentzian profile.\n",
    "    \n",
    "    '''\n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if (cutoff == 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "        wngrid_start = wn_grid[start]\n",
    "        wngrid_end = wn_grid[end-1]\n",
    "        bnormBinsize = ne.evaluate('1/(arctan((wngrid_end-v)/gamma)-arctan((wngrid_start-v)/gamma))/bin_size')\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            BinnedLorentzian = BinnedLorentzian_profile(dv, gamma, bnormBinsize)\n",
    "            _xsec[idx] = ne.evaluate('sum(coef * BinnedLorentzian)')        \n",
    "    elif (cutoff == 'None') & (threshold != 'None'):\n",
    "        filter = coef >= threshold\n",
    "        _gamma = gamma[filter]\n",
    "        if _gamma.size > 0:\n",
    "            _coef = coef[filter] \n",
    "            start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "            end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "            wngrid_start = wn_grid[start]\n",
    "            wngrid_end = wn_grid[end-1]\n",
    "            bnormBinsize = ne.evaluate('1/(arctan((wngrid_end-v)/_gamma)-arctan((wngrid_start-v)/_gamma))/bin_size')\n",
    "            _xsec = np.zeros(shape=(end-start))\n",
    "            for i in range(start,end):\n",
    "                idx = i-start\n",
    "                wn_grid_i = wn_grid[i]\n",
    "                _dv = ne.evaluate('wn_grid_i - v')[filter]\n",
    "                BinnedLorentzian = BinnedLorentzian_profile(_dv, _gamma, bnormBinsize)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * BinnedLorentzian)')    \n",
    "    elif (cutoff != 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        wngrid_start = wn_grid[start]\n",
    "        wngrid_end = wn_grid[end-1]\n",
    "        bnormBinsize = ne.evaluate('1/(arctan((wngrid_end-v)/gamma)-arctan((wngrid_start-v)/gamma))/bin_size')\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                _bnormBinsize = bnormBinsize[filter]\n",
    "                BinnedLorentzian = BinnedLorentzian_profile(_dv, _gamma, _bnormBinsize)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * BinnedLorentzian)')        \n",
    "    else: \n",
    "        filter_threshold = coef >= threshold\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        wngrid_start = wn_grid[start]\n",
    "        wngrid_end = wn_grid[end-1]\n",
    "        bnormBinsize = ne.evaluate('1/(arctan((wngrid_end-v)/gamma)-arctan((wngrid_start-v)/gamma))/bin_size')\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter_cutoff = np.abs(dv) <= cutoff\n",
    "            filter = filter_cutoff & filter_threshold\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                _bnormBinsize = bnormBinsize[filter]\n",
    "                BinnedLorentzian = BinnedLorentzian_profile(_dv, _gamma, _bnormBinsize)\n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * BinnedLorentzian)')          \n",
    "            \n",
    "    xsec[start:end] += _xsec\n",
    "    return (xsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_BinnedVoigt(wn_grid, v, sigma, gamma, coef, cutoff, threshold):\n",
    "    '''\n",
    "    Read ExoMol .states, .trans, .pf and .broad files as the input files.\n",
    "    Return the wavennumbers and cross sections with binned Voigt profile.\n",
    "\n",
    "    '''\n",
    "    nquad = 20\n",
    "    roots, weights= roots_hermite(nquad, mu=False)\n",
    "    bnormq = []\n",
    "    xsec = np.zeros_like(wn_grid)\n",
    "    if (cutoff == 'None') & (threshold == 'None'):   \n",
    "        start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "        wngrid_start = wn_grid[start]\n",
    "        wngrid_end = wn_grid[end-1]\n",
    "        for iquad in range(nquad):\n",
    "            xi = roots[iquad]   \n",
    "            bnormq.append(BinnedVoigt_bnormq(wngrid_start, wngrid_end, v, sigma, gamma, xi))\n",
    "        bnormqT =np.transpose(np.array(bnormq))\n",
    "        _xsec = np.zeros(shape=(end-start))                \n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            lorenz = []\n",
    "            for iquad in range(nquad):\n",
    "                xi = roots[iquad] \n",
    "                lorenz.append(BinnedVoigt_lorenz(dv, sigma, gamma, xi))\n",
    "            lorenzT = np.transpose(np.array(lorenz))  \n",
    "            BinnedVoigtProfile = ne.evaluate('sum(weights*bnormqT*lorenzT)')   \n",
    "            _xsec[idx] = ne.evaluate('sum(coef * BinnedVoigtProfile)')                       \n",
    "    elif (cutoff == 'None') & (threshold != 'None'):\n",
    "        filter = coef >= threshold\n",
    "        _sigma = sigma[filter]\n",
    "        if _sigma.size > 0:\n",
    "            _gamma = gamma[filter]\n",
    "            _coef = coef[filter]\n",
    "            start = max(0,wn_grid.searchsorted(v.min())-1)\n",
    "            end = min(wn_grid.searchsorted(v.max()),len(wn_grid))\n",
    "            wngrid_start = wn_grid[start]\n",
    "            wngrid_end = wn_grid[end-1]\n",
    "            for iquad in range(nquad):\n",
    "                xi = roots[iquad]   \n",
    "                bnormq.append(BinnedVoigt_bnormq(wngrid_start, wngrid_end, v, _sigma, _gamma, xi))\n",
    "            bnormqT =np.transpose(np.array(bnormq))\n",
    "            _xsec = np.zeros(shape=(end-start))\n",
    "            for i in range(start,end):\n",
    "                idx = i-start\n",
    "                wn_grid_i = wn_grid[i]\n",
    "                _dv = ne.evaluate('wn_grid_i - v')[filter]\n",
    "                lorenz = []\n",
    "                for iquad in range(nquad):\n",
    "                    xi = roots[iquad]  \n",
    "                    lorenz.append(BinnedVoigt_lorenz(_dv, _sigma, _gamma, xi))\n",
    "                lorenzT = np.transpose(np.array(lorenz))  \n",
    "                BinnedVoigtProfile = ne.evaluate('sum(weights*bnormqT*lorenzT)')   \n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * BinnedVoigtProfile)')                    \n",
    "    elif (cutoff != 'None') & (threshold == 'None'):\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        wngrid_start = wn_grid[start]\n",
    "        wngrid_end = wn_grid[end-1]\n",
    "        for iquad in range(nquad):\n",
    "            xi = roots[iquad]   \n",
    "            bnormq.append(BinnedVoigt_bnormq(wngrid_start, wngrid_end, v, sigma, gamma, xi))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter = np.abs(dv) <= cutoff\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _sigma = sigma[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                lorenz = []\n",
    "                for iquad in range(nquad):\n",
    "                    xi = roots[iquad]  \n",
    "                    lorenz.append(BinnedVoigt_lorenz(_dv, _sigma, _gamma, xi))\n",
    "                bnormqT =np.transpose(np.array([bnormq[i][filter] for i in range(nquad)]))\n",
    "                lorenzT = np.transpose(np.array(lorenz))  \n",
    "                BinnedVoigtProfile = ne.evaluate('sum(weights*bnormqT*lorenzT)')   \n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * BinnedVoigtProfile)')    \n",
    "    else: \n",
    "        filter_threshold = coef >= threshold\n",
    "        start = max(0,wn_grid.searchsorted(v.min()-cutoff)-1)\n",
    "        end = min(wn_grid.searchsorted(v.max()+cutoff),len(wn_grid))\n",
    "        wngrid_start = wn_grid[start]\n",
    "        wngrid_end = wn_grid[end-1]\n",
    "        for iquad in range(nquad):\n",
    "            xi = roots[iquad]   \n",
    "            bnormq.append(BinnedVoigt_bnormq(wngrid_start, wngrid_end, v, sigma, gamma, xi))\n",
    "        _xsec = np.zeros(shape=(end-start))\n",
    "        for i in range(start,end):\n",
    "            idx = i-start\n",
    "            wn_grid_i = wn_grid[i]\n",
    "            dv = ne.evaluate('wn_grid_i - v')\n",
    "            filter_cutoff = np.abs(dv) <= cutoff\n",
    "            filter = filter_cutoff & filter_threshold\n",
    "            _dv = dv[filter]\n",
    "            if _dv.size > 0:\n",
    "                _sigma = sigma[filter]\n",
    "                _gamma = gamma[filter]\n",
    "                _coef = coef[filter]\n",
    "                lorenz = []   \n",
    "                for iquad in range(nquad):\n",
    "                    xi = roots[iquad]  \n",
    "                    lorenz.append(BinnedVoigt_lorenz(_dv, _sigma, _gamma, xi))\n",
    "                bnormqT =np.transpose(np.array([bnormq[i][filter] for i in range(nquad)]))\n",
    "                lorenzT = np.transpose(np.array(lorenz))  \n",
    "                BinnedVoigtProfile = ne.evaluate('sum(weights*bnormqT*lorenzT)')   \n",
    "                _xsec[idx] = ne.evaluate('sum(_coef * BinnedVoigtProfile)')              \n",
    "    xsec[start:end] += _xsec\n",
    "    xsec = ne.evaluate('xsec*InvbinSizePIhalf')\n",
    "    return (xsec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_xsec(wn, xsec, database, profile_label):             \n",
    "    xsecs_foldername = save_path+'/xsecs/files/'+molecule+'/'+database+'/'\n",
    "    if os.path.exists(xsecs_foldername):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(xsecs_foldername, exist_ok=True)\n",
    "    print('{:25s} : {:<6}'.format('Temperature selected', T), 'K')\n",
    "    print('{:25s} : {:<6}'.format('Pressure selected', P), 'bar')\n",
    "    \n",
    "    if 'L' not in wn_wl:\n",
    "        print('{:25s} : {:<6}'.format('Cutoff is', cutoff), u'cm\\u207B\\u00B9')\n",
    "        print('{:25s} : {:<6}'.format('Threshold is', threshold), u'cm\\u207B\\u00B9/(molecule cm\\u207B\\u00B2)')\n",
    "        print('{:25s} : {} {} {} {}'.format('Wavenumber range selected', min_wn, u'cm\\u207B\\u00B9 -', max_wn, 'cm\\u207B\\u00B9'))\n",
    "        # Save cross sections into .xsec file.\n",
    "        xsec_df = pd.DataFrame()\n",
    "        xsec_df['wavenumber'] = wn\n",
    "        xsec_df['cross-section'] = xsec\n",
    "        xsec_filename = (xsecs_foldername+molecule+'__T'+str(T)+'__'+wn_wl+str(min_wn)+'-'+str(max_wn)+'__'\n",
    "                        +database+'__'+abs_emi+'__'+profile_label.replace(' ','')+'.xsec')\n",
    "        np.savetxt(xsec_filename, xsec_df, fmt=\"%12.6f %14.8E\")\n",
    "        print('Cross sections file saved.')\n",
    "        if PlotCrossSectionYN == 'Y':\n",
    "            plots_foldername = save_path+'/xsecs/plots/'+molecule+'/'+database+'/'\n",
    "            if os.path.exists(plots_foldername):\n",
    "                pass\n",
    "            else:\n",
    "                os.makedirs(plots_foldername, exist_ok=True)  \n",
    "            #plt.legend(fancybox=True, framealpha=0.0)\n",
    "            parameters = {'axes.labelsize': 14,\n",
    "                        'legend.fontsize': 14,\n",
    "                        'xtick.labelsize': 12,\n",
    "                        'ytick.labelsize': 12}\n",
    "            plt.rcParams.update(parameters)\n",
    "            # Plot cross sections and save it as .png.\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.xlim([min_wn, max_wn])\n",
    "            plt.ylim([limitYaxisXsec, 10*max(xsec)])\n",
    "            plt.plot(wn, xsec, label='T = '+str(T)+' K, '+profile_label, linewidth=0.4)   \n",
    "            plt.semilogy()\n",
    "            #plt.title(database+' '+molecule+' '+abs_emi+' Cross-Section with '+ profile_label) \n",
    "            plt.xlabel('Wavenumber, cm$^{-1}$')\n",
    "            plt.ylabel('Cross-section, cm$^{2}$/molecule')\n",
    "            plt.legend()\n",
    "            leg = plt.legend()                  # Get the legend object.\n",
    "            for line in leg.get_lines():\n",
    "                line.set_linewidth(1.0)         # Change the line width for the legend.\n",
    "            plt.savefig(plots_foldername+molecule+'__T'+str(T)+'__'+wn_wl+str(min_wn)+'-'+str(max_wn)+'__'\n",
    "                        +database+'__'+abs_emi+'__'+profile_label.replace(' ','')+'.png', dpi=500)\n",
    "            plt.show()\n",
    "            print('Cross sections plot saved.')\n",
    "    elif 'L' in wn_wl:\n",
    "        wl = 10000 / wn\n",
    "        min_wl = '%.02f' % (10000 / max_wn)\n",
    "        max_wl = '%.02f' % (10000 / min_wn)\n",
    "        print('{:25s} : {:<6}'.format('Cutoff is', 10000/cutoff),u'\\xb5m')\n",
    "        print('{:25s} : {:<6}'.format('Threshold is',10000/threshold),u'\\xb5m/(moleculeu \\xb5m\\u00B2)')\n",
    "        print('{:25s} : {} {} {} {}'.format('Wavelength range selected',min_wl,u'\\xb5m -',max_wl,u'\\xb5m'))\n",
    "        # Save cross sections into .xsec file.\n",
    "        xsec_df = pd.DataFrame()\n",
    "        xsec_df['wavelength'] = wl\n",
    "        xsec_df['cross-section'] = xsec\n",
    "        xsec_filename = (xsecs_foldername+molecule+'__T'+str(T)+'__'+wn_wl+str(min_wl)+'-'+str(max_wl)+'__'\n",
    "                         +database+'__'+abs_emi+'__'+profile_label.replace(' ','')+'.xsec')\n",
    "        np.savetxt(xsec_filename, xsec_df, fmt=\"%12.6f %14.8E\")\n",
    "        print('Cross sections file saved.')       \n",
    "        if PlotCrossSectionYN == 'Y':\n",
    "            plots_foldername = save_path+'/xsecs/plots/'+molecule+'/'+database+'/'\n",
    "            if os.path.exists(plots_foldername):\n",
    "                pass\n",
    "            else:\n",
    "                os.makedirs(plots_foldername, exist_ok=True)  \n",
    "            #plt.legend(fancybox=True, framealpha=0.0)\n",
    "            parameters = {'axes.labelsize': 14,\n",
    "                        'legend.fontsize': 14,\n",
    "                        'xtick.labelsize': 12,\n",
    "                        'ytick.labelsize': 12}\n",
    "            plt.rcParams.update(parameters)\n",
    "            # Plot cross sections and save it as .png.\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.xlim([min_wl, max_wl])\n",
    "            plt.ylim([limitYaxisXsec, 10*max(xsec)])\n",
    "            plt.plot(wl, xsec, label='T = '+str(T)+' K, '+profile_label, linewidth=0.4)             \n",
    "            plt.semilogy()\n",
    "            #plt.title(database+' '+molecule+' '+abs_emi+' Cross-Section with '+ profile_label) \n",
    "            plt.xlabel(u'Wavelength, \\xb5m')\n",
    "            plt.ylabel(u'Cross-section, \\xb5m\\u207B\\u00B2/molecule')\n",
    "            plt.legend()\n",
    "            leg = plt.legend()                  # Get the legend object.\n",
    "            for line in leg.get_lines():\n",
    "                line.set_linewidth(1.0)         # Change the line width for the legend.\n",
    "            plt.savefig(plots_foldername+molecule+'__T'+str(T)+'__'+wn_wl+str(min_wl)+'-'+str(max_wl)+'__'\n",
    "                        +database+'__'+abs_emi+'__'+profile_label.replace(' ','')+'.png', dpi=500)\n",
    "            plt.show()\n",
    "            print('Cross sections plot saved.')\n",
    "    else:\n",
    "        print('Please choose wavenumber or wavelength and type in correct format: wn or wl.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crosssection(read_path, states_part_df, trans_part_df, hitran_df, ncolumn):\n",
    "    print('Calculate cross-sections.')\n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    # ExoMol or HITRAN\n",
    "    if database == 'ExoMol':\n",
    "        gamma_air = gamma_self = []\n",
    "        broad, ratio, nbroad, broad_dfs = read_broad(read_path)\n",
    "        Q = read_exomolweb_pf(T)              # Read partition function from the ExoMol website.\n",
    "        # Q = read_exomol_pf(read_path, T)    # Read partition function from local partition function file.\n",
    "        # Absorption or emission cross section\n",
    "        if abs_emi == 'Ab': \n",
    "            print('Absorption cross section')\n",
    "            A, v, Epp, gp, tau, gamma_L, n_air = linelist_exomol_abs(cutoff,broad,ratio,nbroad,broad_dfs,states_part_df,trans_part_df, ncolumn)\n",
    "            coef = cal_abscoefs(T, v, gp, A, Epp, Q, abundance)\n",
    "        elif abs_emi == 'Em': \n",
    "            print('Emission cross section')\n",
    "            A, v, Ep, gp, tau, gamma_L, n_air = linelist_exomol_emi(cutoff,broad,ratio,nbroad,broad_dfs,states_part_df,trans_part_df, ncolumn)\n",
    "            coef = cal_emicoefs(T, v, gp, A, Ep, Q, abundance)\n",
    "        else:\n",
    "            raise ImportError(\"Please choose one from: 'Absoption' or 'Emission'.\")         \n",
    "    elif database == 'HITRAN':\n",
    "        gamma_L = []\n",
    "        broad = []\n",
    "        tau = []\n",
    "        A, v, Ep, Epp, gp, n_air, gamma_air, gamma_self, delta_air = linelist_hitran(hitran_df)\n",
    "        Q = read_hitran_pf(T)\n",
    "        # Absorption or emission cross section\n",
    "        if abs_emi == 'Ab': \n",
    "            print('Absorption cross section') \n",
    "            coef = cal_abscoefs(T, v, gp, A, Epp, Q, abundance)\n",
    "        elif abs_emi == 'Em': \n",
    "            print('Emission cross section')\n",
    "            Ep = cal_Ep(Epp, v)\n",
    "            coef = cal_emicoefs(T, v, gp, A, Ep, Q, abundance)\n",
    "        else:\n",
    "            raise ImportError(\"Please choose one from: 'Absoption' or 'Emission'.\") \n",
    "    else:\n",
    "        raise ImportError(\"Please add the name of the database 'ExoMol' or 'HITRAN' into the input file.\")\n",
    "    # Line profile: Gaussion, Lorentzian or Voigt\n",
    "    num_v = len(v)\n",
    "    if 'LOR' not in profile:\n",
    "        alpha = DopplerHWHM_alpha(num_v, alpha_HWHM, v, T)\n",
    "    if 'DOP' not in profile and 'GAU' not in profile:\n",
    "        gamma = LorentzianHWHM_gamma(num_v, gamma_HWHM, broad, gamma_L, n_air, gamma_air, gamma_self, tau, T, P)\n",
    "    if 'SCI' in profile or 'W' in profile:\n",
    "        sigma = Gaussian_standard_deviation(alpha)     \n",
    "    if 'VOI' in profile and 'BIN' in profile:\n",
    "        sigma = Gaussian_standard_deviation(alpha)  \n",
    "    # Line profiles\n",
    "    if profile[0:3] == 'DOP':\n",
    "        print('Doppler profile')\n",
    "        profile_label = 'Doppler'\n",
    "        xsec = cross_section_Doppler(wn_grid, v, alpha, coef, cutoff, threshold)\n",
    "    elif profile[0:3] == 'GAU':\n",
    "        print('Gaussion profile')\n",
    "        profile_label = 'Gaussion'\n",
    "        xsec = cross_section_Doppler(wn_grid, v, alpha, coef, cutoff, threshold)\n",
    "    elif profile[0:3] == 'LOR':\n",
    "        print('Lorentzian profile')\n",
    "        profile_label = 'Lorentzian'\n",
    "        xsec = cross_section_Lorentzian(wn_grid, v, gamma, coef, cutoff, threshold)\n",
    "    elif 'SCI' in profile and 'W' not in profile:\n",
    "        print('SciPy Voigt profile')\n",
    "        profile_label = 'SciPy Voigt'\n",
    "        xsec = cross_section_SciPyVoigt(wn_grid, v, sigma, gamma, coef, cutoff, threshold)\n",
    "    elif 'W' in profile:\n",
    "        print('SciPy wofz Voigt profile')\n",
    "        profile_label = 'SciPy wofz Voigt'\n",
    "        xsec = cross_section_SciPyWofzVoigt(wn_grid, v, sigma, gamma, coef, cutoff, threshold)\n",
    "    elif 'H' in profile:\n",
    "        print('Humlicek Voigt profile')\n",
    "        profile_label = 'Humlicek Voigt'\n",
    "        xsec = cross_section_HumlicekVoigt(wn_grid, v, alpha, gamma, coef, cutoff, threshold)  \n",
    "    elif 'TH' in profile:\n",
    "        print('Thompson pseudo-Voigt profile')\n",
    "        profile_label = 'Thompson pseudo-Voigt'\n",
    "        eta, hV = PseudoThompsonVoigt(alpha, gamma)\n",
    "        xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, hV, coef, cutoff, threshold)       \n",
    "    elif 'K' in profile and 'H' not in profile:\n",
    "        print('Kielkopf pseudo-Voigt profile')\n",
    "        profile_label = 'Kielkopf pseudo-Voigt'\n",
    "        eta, hV = PseudoKielkopfVoigt(alpha, gamma)\n",
    "        xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, hV, coef, cutoff, threshold)       \n",
    "    elif 'OL' in profile:\n",
    "        print('Olivero pseudo-Voigt profile')\n",
    "        profile_label = 'Olivero pseudo-Voigt'\n",
    "        eta, hV = PseudoOliveroVoigt(alpha, gamma)\n",
    "        xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, hV, coef, cutoff, threshold)       \n",
    "    elif 'LI' in profile or 'LL' in profile:\n",
    "        print('Liu-Lin pseudo-Voigt profile')\n",
    "        profile_label = 'Liu-Lin pseudo-Voigt'\n",
    "        eta, hV = PseudoLiuLinVoigt(alpha, gamma)\n",
    "        xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, hV, coef, cutoff, threshold)       \n",
    "    elif 'RO' in profile:\n",
    "        print('Rocco pseudo-Voigt profile')\n",
    "        profile_label = 'Rocco pseudo-Voigt'\n",
    "        eta, hV = PseudoRoccoVoigt(alpha, gamma)\n",
    "        xsec = cross_section_PseudoVoigt(wn_grid, v, alpha, gamma, eta, hV, coef, cutoff, threshold) \n",
    "    elif 'BIN' in profile and 'DOP' in profile:\n",
    "        print('Binned Doppler profile')\n",
    "        profile_label = 'Binned Doppler'\n",
    "        xsec = cross_section_BinnedGaussian(wn_grid, v, alpha, coef, cutoff, threshold)        \n",
    "    elif 'BIN' in profile and 'GAU' in profile:\n",
    "        print('Binned Gaussion profile')\n",
    "        profile_label = 'Binned Gaussion'\n",
    "        xsec = cross_section_BinnedGaussian(wn_grid, v, alpha, coef, cutoff, threshold)\n",
    "    elif 'BIN' in profile and 'LOR' in profile:\n",
    "        print('Binned Lorentzian profile')\n",
    "        profile_label = 'Binned Lorentzian'\n",
    "        xsec = cross_section_BinnedLorentzian(wn_grid, v, gamma, coef, cutoff, threshold)\n",
    "    elif 'BIN' in profile and 'VOI' in profile:\n",
    "        print('Binned Voigt profile')\n",
    "        profile_label = 'Binned Voigt'\n",
    "        xsec = cross_section_BinnedVoigt(wn_grid, v, sigma, gamma, coef, cutoff, threshold)           \n",
    "    else:\n",
    "        raise ImportError('Please choose line profile from the list.')\n",
    "    save_xsec(wn_grid, xsec, database, profile_label)    \n",
    "    t.end()\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get Results\n",
    "def get_results(read_path): \n",
    "    t_tot = Timer()\n",
    "    t_tot.start()  \n",
    "    states_part_df = pd.DataFrame()\n",
    "    trans_part_df = pd.DataFrame() \n",
    "    hitran_df = pd.DataFrame()\n",
    "    # ExoMol or HITRAN\n",
    "    if database == 'ExoMol':\n",
    "        print('ExoMol database')\n",
    "        print('Molecule\\t:', molecule, '\\nIsotopologue\\t:', isotopologue, '\\nDataset\\t\\t:', dataset)\n",
    "        # All functions need whole states.\n",
    "        states_df = read_all_states(read_path)\n",
    "        # Only calculating lifetimes, cooling functions and oscillator stengths need whole transitions.\n",
    "        NeedAllTrans = Lifetimes + CoolingFunctions + OscillatorStrengths\n",
    "        if NeedAllTrans != 0: \n",
    "            (all_trans_df, ncolumn) = read_all_trans(read_path)  \n",
    "        if CrossSections == 1 and check_predissoc + check_lifetime == 0 and 'VOI' in profile:\n",
    "            if NeedAllTrans == 0:\n",
    "                (all_trans_df, ncolumn) = read_all_trans(read_path)  \n",
    "            lifetime_df = cal_lifetime(states_df, all_trans_df)\n",
    "            states_df['tau'] = lifetime_df['tau'].values  \n",
    "        # Only calculating stick spectra and cross sections need part of states.\n",
    "        NeedPartStates = StickSpectra + CrossSections\n",
    "        if NeedPartStates != 0:\n",
    "            states_part_df = read_part_states(states_df) \n",
    "        # Conversion and calculating stick spectra and cross sections need part of transitions.\n",
    "        NeedPartTrans = Conversion + StickSpectra + CrossSections\n",
    "        if NeedPartTrans != 0:\n",
    "            (trans_part_df, ncolumn) = read_part_trans(read_path) \n",
    "\n",
    "        # Functions\n",
    "        Nfunctions = (PartitionFunctions + SpecificHeats + Lifetimes + CoolingFunctions \n",
    "                      + OscillatorStrengths + Conversion + StickSpectra  + CrossSections)\n",
    "        if Nfunctions > 0:\n",
    "            if PartitionFunctions == 1:\n",
    "                exomol_partition_func(states_df, Ntemp, Tmax)\n",
    "            if SpecificHeats == 1:\n",
    "                exomol_specificheat(states_df, Ntemp, Tmax)\n",
    "            if Lifetimes == 1:\n",
    "                exomol_lifetime(read_path, states_df, all_trans_df)\n",
    "            if OscillatorStrengths == 1:\n",
    "                exomol_oscillator_strength(states_df, all_trans_df, ncolumn)\n",
    "            if CoolingFunctions == 1:\n",
    "                exomol_cooling_func(read_path, states_df, all_trans_df, Ntemp, Tmax, ncolumn)\n",
    "            if (Conversion == 1 and ConversionFormat == 1):\n",
    "                conversion_exomol2hitran(read_path, states_df, trans_part_df, ncolumn)\n",
    "            if StickSpectra == 1:\n",
    "                exomol_stick_spectra(read_path, states_part_df, trans_part_df, ncolumn, T)\n",
    "            if CrossSections == 1:\n",
    "                get_crosssection(read_path, states_part_df, trans_part_df, hitran_df, ncolumn) \n",
    "        else:   \n",
    "            raise ImportError(\"Please choose functions which you want to calculate.\")\n",
    "    elif database == 'HITRAN':\n",
    "        print('HITRAN database')\n",
    "        print('Molecule\\t:', molecule, '\\t\\t\\tMolecule ID\\t:', molecule_id, \n",
    "              '\\nIsotopologue\\t:', isotopologue, '\\t\\tIsotopologue ID\\t:', isotopologue_id,\n",
    "              '\\nDataset\\t\\t:', dataset)\n",
    "        parfile_df = read_parfile(read_path)\n",
    "        if (Conversion == 1 and ConversionFormat == 2):\n",
    "            hitran_df = read_hitran_parfile(read_path,parfile_df,ConversionMinFreq,ConversionMaxFreq,\n",
    "                                            ConversionUnc,ConversionThreshold).reset_index().drop(columns='index')\n",
    "            conversion_hitran2exomol(hitran_df)\n",
    "        # Use ExoMol functions\n",
    "        NuseExoMolFunc = PartitionFunctions+SpecificHeats+Lifetimes\n",
    "        if NuseExoMolFunc > 0:\n",
    "            read_hitran2exomol_path = save_path + 'conversion/HITRAN2ExoMol/'\n",
    "            if ConversionFormat != 2:\n",
    "                conversion_foldername = read_hitran2exomol_path+molecule+'/'+isotopologue+'/'+dataset+'/'\n",
    "                if os.path.exists(conversion_foldername):\n",
    "                    states_list = glob.glob(conversion_foldername + isotopologue + '__' + dataset + '.states.bz2')\n",
    "                    trans_list = glob.glob(conversion_foldername + isotopologue + '__' + dataset + '*.trans.bz2')\n",
    "                    if (states_list == [] and trans_list == []):\n",
    "                        hitran_df = read_hitran_parfile(read_path,parfile_df,ConversionMinFreq,ConversionMaxFreq,\n",
    "                                                        'None','None').reset_index().drop(columns='index')\n",
    "                        conversion_hitran2exomol(hitran_df)\n",
    "            # All functions need whole states.\n",
    "            states_df = read_all_states(read_hitran2exomol_path)\n",
    "            if PartitionFunctions == 1:\n",
    "                exomol_partition_func(states_df, Ntemp, Tmax)\n",
    "            if SpecificHeats == 1:\n",
    "                exomol_specificheat(states_df, Ntemp, Tmax)\n",
    "            if Lifetimes == 1:\n",
    "                (all_trans_df, ncolumn) = read_all_trans(read_hitran2exomol_path)\n",
    "                # hitran_lifetime(states_df, all_trans_df)\n",
    "                exomol_lifetime(read_hitran2exomol_path, states_df, all_trans_df)\n",
    "        # Use HITRAN functions\n",
    "        # Calculate cooling functions or oscillatore strengths.\n",
    "        NeedWholeHITRAN = CoolingFunctions+OscillatorStrengths\n",
    "        if NeedWholeHITRAN != 0: \n",
    "            hitran_df = read_hitran_parfile(read_path,parfile_df,min_wn,max_wn,'None','None').reset_index().drop(columns='index')\n",
    "        if CoolingFunctions == 1:\n",
    "            hitran_cooling_func(hitran_df, Ntemp, Tmax)\n",
    "        if OscillatorStrengths == 1:\n",
    "            hitran_oscillator_strength(hitran_df)\n",
    "        if (StickSpectra == 1 or CrossSections == 1):\n",
    "            hitran_df = read_hitran_parfile(read_path,parfile_df,min_wn,max_wn,\n",
    "                                             UncFilter,threshold).reset_index().drop(columns='index')\n",
    "            (hitran_linelist_df, QNs_col) = hitran_linelist_QN(hitran_df)\n",
    "        if StickSpectra == 1:\n",
    "            hitran_stick_spectra(hitran_linelist_df, QNs_col, T)\n",
    "        if CrossSections == 1:\n",
    "            ncolumn = 0\n",
    "            get_crosssection(read_path, states_part_df, trans_part_df, hitran_linelist_df, ncolumn)\n",
    "    else:\n",
    "        raise ImportError(\"Please add the name of the database 'ExoMol' or 'HITRAN' into the input file.\")     \n",
    "    print('\\nThe program total running time:')    \n",
    "    t_tot.end()\n",
    "    print('\\nFinished!')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(read_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('exomol')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "722cf535e3d158a42a418157a233af576121476252bfbc7c5af47c8831a1fc54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
